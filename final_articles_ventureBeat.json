[{"topic_title":"Artificial Intelligence","article_title":"Q-CTRL raises $59M for quantum computing software","article_url":"https:\/\/venturebeat.com\/ai\/q-ctrl-raises-59m-for-quantum-computing-software\/","article_date":"October 8, 2024 5:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreQ-CTRL has raised $59 million in additional funding for its latest round for building quantum computing infrastructure software.Sydney, Australia-based Q-CTRL has now raised $113 million in funding in its Series B funding round, and it has raised a total of $133 million to date.The Series B-2 all-equity funding round was led by global late-stage venture firm GP Bullhound and saw a dramatic increase in valuation from earlier financing.\u201cWe\u2019re very excited that GP Bullhound has led this round,\u201d said Q-CTRL CEO Michael Biercuk, in a statement. \u201cTheir experience and international presence will support us as we continue our expansion, and we look forward to working closely with them as a shareholder and board member.\u201dThe investor syndicate includes venture capital leaders and strategic global defense technology giants reflecting the financial opportunity presented by Q-CTRL\u2019s strong commercials and the critical value of its technology in shaping the quantum industry.New investors include Alpha Edison, Lockheed Martin Ventures, NTT Finance, Salus Group, and TISI; they are joined by repeat contributions from existing investors Alumni Ventures, DCVC, John Eales, ICM Allectus, Main Sequence Ventures, and Salesforce Ventures.\u201cWe are thrilled to support Q-CTRL in unlocking the full potential of quantum technology,\u201d said Per Roman, GP Bullhound Founder and Managing Partner. \u201cAt GP Bullhound, we believe that quantum computing and sensing will be central to the next wave of technological transformation, reshaping industries such as finance, transport, and pharmaceuticals. Our investment reflects our commitment to backing visionary companies capable of bringing this revolution from the lab to real-world applications.\u201d\u201cOur focused view that software can be the key enabler of quantum hardware across all applications has become a key driver of new capabilities in the field, and underpins our major commercial partnerships with leading quantum platform vendors, Biercuk added. \u201cThis new investment, coupled with our growing portfolio of technical demonstrations, has positioned us for ubiquity and permanence in the industry.\u201dWith this funding, Q-CTRL will expand its investment in quantum control R&D and product engineering to deliver on a growing portfolio of customer engagements among Fortune 500 clients, government departments and agencies, and quantum platform providers. The company has already achieved extraordinary commercial success, indicating tremendous opportunity for value capture and growth with new investment.A big opportunityWorld\u2019s smallest strapdown 3-axis quantum inertial measurement unit can act as an advanced navigation system when GPS is blocked or spoofed.The company believes quantum technology could revolutionize industries like pharmaceuticals, finance, and resources, representing a $1.2 trillion opportunity, according to McKinsey. With BCC Research projecting the global sensing market to surpass $300 billion by 2029, quantum sensing is poised to capture a significant market share from existing classical technologies due to its enhanced performance and ability to enable critical new missions for defense.Q-CTRL uniquely spans both quantum computing and quantum sensing through its focus on how quantum control infrastructure software can enable useful field-deployed quantum solutions.The current funding round highlights the commercial and technological success of Q-CTRL in the emerging quantum industry. GP Bullhound\u2019s role in leading the equity financing round highlights the way Q-CTRL has become a key accelerant of quantum technology development and uptake through its quantum infrastructure software business.Investment from NTT Finance and TISI highlights a growing quantum opportunity in the Japanese market, coinciding with Japan\u2019s emerging role as a partner to the AUKUS technology sharing agreement. Q-CTRL also adds national-resilience financial investor Salus Group and strategic investor Lockheed Martin Ventures to previous support from Airbus Ventures, showcasing the strategic value of Q-CTRL\u2019s \u201csoftware-ruggedized\u201d quantum sensing technology for defense.Recent major commercial outcomes include deploying its unique performance-management infrastructure software into major quantum cloud platforms, pioneering a trend of quantum-industrydeverticalization.Q-CTRL recently deployed application-focused Qiskit Functions with IBM, building on its world-first native integration of third-party software into IBM Quantum Services. Its technology has powered the record-setting performance achieved by industry customers such as Softbank and Mitsubishi Chemical. This expansion followed the recent announcement that its performance-management software for quantumcomputing was being deployed into three new major platforms: Diraq, Oxford Quantum Circuits, and Rigetti.Behind the techQ-CTRL\u2019s UAV-mountable optical magnetometer used for measuring Earth\u2019s magnetic field to identify subsurface features like metal objects, geological structures and archaeological artifacts.The company said it is building the quantum workforce of the future. Q-CTRL has directly tackled thechallenge of quantum workforce development, rolling out its Black Opal quantum education software at national and state levels in the UK, Tamil Nadu in India, and among major corporate clients.And it is leading the field-deployment of quantum sensors for defense and commercial applications. Q-CTRL has built commercial engagements with major defense primes, the Australian Department of Defence, and the UK Navy\u2019s Office of the Chief Technology Officer focused on quantum-assured navigation in GPS-denied environments. This year, Q-CTRL performed a world-first deployment of \u201csoftware-ruggedized\u201d quantum gravimeters on maritime vessels, showing its software stabilizationtechnology made the difference between complete loss-of-signal and useful performance.And the company announced a partnership with Airbus building on these field trials. These quantum-navigation capabilities underpin new commercial opportunities providing a GPS backup in maritime and airborne vehicles.Founded by Michael J. Biercuk in November 2017, Q-CTRL has assembled a team of 135 people. Q-CTRL is using control to solve the hardest problems facing quantum technology, improving hardware  performance and accelerating pathways to useful quantum computers and other technologies. Through our professionally developed tools, we put our deep expertise in quantum control into your hands. Q-CTRL\u2019s mission is to enhance the stability of quantum technology to unlock its full potential.Quantum computers are highly sensitive to errors due to their fragile nature. Q-CTRL provides software that reduces errors and noise in quantum algorithms. Their products use techniques such as quantum error suppression and error correction, which help quantum processors operate more reliably. This enables researchers and companies to use quantum computers to solve complex problems more effectively.For example, Q-CTRL\u2019s Fire Opal software is designed to automatically reduce errors and boost algorithmic success on cloud-accessible quantum computers. The software works by providing performance management solutions that can be integrated with existing quantum hardware, making it easier for end-users to execute algorithms successfully. It has even been adopted by companies like IBM, where Q-CTRL\u2019s software runs natively on IBM\u2019s cloud quantum computing systems.And it is also focused on quantum sensing, which leverages quantum systems to detect and measure physical quantities with high precision. Q-CTRL\u2019s quantum sensing technology involves using quantum control to enhance the performance of sensors. Their software \u201cruggedizes\u201d quantum sensors, improving their resilience to environmental noise and increasing their sensitivity.Q-CTRL recently expanded its efforts in quantum sensing by developing ultrasensitive sensors that can measure gravity, motion, and magnetic fields. These advancements have practical applications in areas such as defense, Earth observation, and space exploration. For example, their technology has been adopted by the Australian Defence Force for GPS-denied navigation, showcasing how quantum sensors can function effectively in challenging environments\u200b.Q-CTRL\u2019s technology revolves around quantum control, which is crucial in stabilizing and improving the performance of quantum systems.Black Opal is Q-CTRL\u2019s educational platform for quantum computing. It provides an interactive learning experience for users to understand and work with quantum computing concepts. Black Opal offers a series of intuitive lessons that include visualizations, animations, and hands-on exercises, helping users from beginners to experts understand complex quantum mechanics in a digestible way. The goal of Black Opal is to reduce the barriers to entry in the field of quantum computing, equipping users with the knowledge they need to engage with quantum technologies effectively.Q-CTRL was founded to address the significant challenges facing quantum technology, particularly the instability and errors in quantum hardware. Biercuk, a professor of quantum physics and quantum tech, founded the company to use quantum control techniques to make quantum computing and sensing more practical and reliable. After noticing an increased industry interest in quantum technology, he saw an opportunity to use his decade of research to address quantum hardware issues, which were hindering progress across various applications.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Artificial Intelligence","article_title":"SAP adds more open source LLM support, turns Joule into a collaborative agent","article_url":"https:\/\/venturebeat.com\/ai\/saps-joule-collaborative-ai-agent-driving-innovation-with-open-source-llms\/","article_date":"October 8, 2024 12:31 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreSAPannounced today the expansion of its generative AI copilot Joule\u2019s capabilities to support up to 80% of its customers\u2019 most common business tasks. This will enable it to be a collaborative agent that can accomplish complex workflows.For customers to get the most value from Joule and future innovations, they must be on SAP cloud platforms and systems. SAP is accelerating product innovation on the cloud in the hopes of attracting more customers to itsRISE with SAPinitiative.Launched in Jan. 2021, RISE withSAP aims to guide customers\u2019 transition from on-premises SAP ERP systems to the cloud, modernizing processes along the way. SAP reported that its cloud revenue increased by 25% inQ2 2024, with the Cloud ERP Suite growing by 33% as a result, demonstrating that RISE is effective.Joule\u2019s infusion of new features signals how serious SAP is about moving its customers to the cloud. SAP says on-premises customers can still use Joule. However, they will need to use theSAP Integration Suiteto connect their existing infrastructure to SAP\u2019s cloud services, enabling Joule to access and process data while extending AI capabilities to their on-premise environments.Additional announcements atTechEd 2024introduced additional open source large language model (LLM) support as part of the SAP Generative AI Hub, introduced the SAP Knowledge Graph, showcasing developer enhancements in SAP Build, highlighted specific use cases and reaffirmed the company\u2019s commitment to upskill 2 million people by 2025.SAP placing a strategic bet on Joule\u2019s new agentic AI strengthsDesigned as a cloud-native AI assistant that is core to SAP\u2019sBusiness Technology Platform (BTP), Joules\u2019 ability to integrate and scale with all current and future apps, modules and platform environments to accelerate customers to the cloud further.SAP made astrategic bet with BTP, believing their customers would see the value of a unified cloud platform over the legacy on-premises ERP systems, which earned a reputation for being challenging to integrate real-time data and third-party applications with. SAP doubling down on Joule shows they\u2019re working to reverse their proprietary ways of the past and go after a more open cloud-based architecture that can deliver the accuracy, speed and scale their customers need.Embedded across SAP\u2019s ecosystem, Joule can already understand business contexts, deliver data-driven insights and enable customers to get more work done using its advanced natural language processing and machine learning (ML) capabilities. With 80% of the most common business tasks now part of Joule, SAP is betting their latest gen AI copilot will be compelling enough for more customers to join RISE and move to the cloud.Source: Presentation at VB Transform 2024 by Yaad Oren.SAP highlighted two use cases at TechEd 2024 to demonstrate the power of these agents:Dispute Management: AI agents autonomously resolve disputes related to invoices, credits and payments. This will significantly reduce manual intervention.Financial Accounting: Specialized agents streamline billing, ledger updates and invoice processing. This will ensure accuracy and efficiency.\u201cCollaborative AI agents from SAP represent a new era in enterprise productivity,\u201d said Philipp Herzig, Chief Artificial Intelligence Officer at SAP. \u201cOur ability to integrate multiple specialized AI agents into Joule allows businesses to automate intricate workflows and focus on tasks that truly require human ingenuity.\u201dThree new open-source models added to the SAP gen AI HubThe open-source LLM announcements at TechEd 2024 show that SAP is continuing to develop its generative AI hub strategy. Open-source models now available on the SAP Generative AI Hub include Meta\u2019s Llama 3.1 70B model, Mistral Large 2 (available by the end of 2024), and Mistral Codestral. SAP\u2019s announcements this week at TechEd 2024 show that it is committed to keeping up with the quicker pace of innovation in open-source LLMs.The SAP Generative AI Hub, positioned as a central node within SAP\u2019s Business Technology Platform (BTP), connects both proprietary and open-source models. Developer tools like the Extensibility Wizard and SAP Build enhancements streamline this integration, reflecting SAP\u2019s push toward a developer-friendly environment.Source: SAPSAP continues to go on the offensive when it comes to providing more significant support for open-source LLMs. Their series of announcements this week at TechEd 2024 show they\u2019re committed to keeping up with the quickening pace of innovation in open source LLMs in general and across all of open source strategically.One of the main goals of going on the offensive with open-source LLMs is to enable enterprise-level standards for customers to adopt while ensuring reliability, scalability, security and performance, all within the SAP AI Core.The following table summarizes the three open-source LLMs SAP announced support for during TechEd 2024:Source: VentureBeat analysisSAP\u2019s new AI era has arrivedLong known for its dominance in the ERP market, SAP shows signs of successfully reinventing itself in a new AI era. Lessons learned on usability, the need for a more open, adaptive system architecture, and the need to provide customers with more flexibility in how they use data, including open-source LLMs, now dominate their product strategies. Their Business Technology Platform with an SAP AI Core reflects a more forward-thinking SAP that realizes their quickest path to value is recognizing customers need the freedom to go open source when they choose.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Artificial Intelligence","article_title":"Foxconn to build Taiwan\u2019s fastest AI supercomputer with Nvidia Blackwell","article_url":"https:\/\/venturebeat.com\/ai\/foxconn-to-build-taiwans-fastest-ai-supercomputer-with-nvidia-blackwell\/","article_date":"October 7, 2024 10:00 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreNvidiaandFoxconnare building Taiwan\u2019s largest supercomputer using Nvidia Blackwell chips.The project, Hon Hai Kaohsiung Super Computing Center, revealed Tuesday at Hon Hai Tech Day, will be built around Nvidia\u2019s Blackwell graphics processing unit (GPU) architecture and feature the GB200 NVL72 platform, which includes a total of 64 racks and 4,608 Tensor Core GPUs.With an expected performance of over 90 exaflops of AI performance, the machine would easily be considered the fastest in Taiwan.Foxconn plans to use the supercomputer, once operational, to power breakthroughs in cancer research, large language model development and smart city innovations, positioning Taiwan as a global leader in AI-driven industries.Foxconn\u2019s \u201cthree-platform strategy\u201d focuses on smart manufacturing, smart cities and electric vehicles. The new supercomputer will play a pivotal role in supporting Foxconn\u2019s ongoing efforts in digital twins, robotic automation and smart urban infrastructure, bringing AI-assisted services to urban areas like Kaohsiung.Construction has started on the new supercomputer housed in Kaohsiung, Taiwan. The first phase is expected to be operational by mid-2025. Full deployment is targeted for 2026.The project will integrate with Nvidia technologies, such as Nvidia Omniverse and Isaac robotics platforms for AI and digital twins technologies to help transform manufacturing processes.Nvidia is providing Blackwell AI chips to Foxconn for a new supercomputer.\u201cPowered by Nvidia\u2019s Blackwell platform, Foxconn\u2019s new AI supercomputer is one of the most powerful in the world, representing a significant leap forward in AI computing and efficiency,\u201d said Foxconn vice president James Wu, in a statement.The GB200 NVL72 is a state-of-the-art data center platform optimized for AI and accelerated computing.Each rack features 36 Nvidia Grace CPUs and 72 Nvidia Blackwell GPUs connected via Nvidia\u2019s NVLink technology, delivering 130TB\/s of bandwidth.Nvidia NVLink Switch allows the 72-GPU system to function as a single, unified GPU. This makes it ideal for training large AI models and executing complex inference tasks in real time on trillion-parameter models.Taiwan-based Foxconn, officially known as Hon Hai Precision Industry Co., is the world\u2019s largest electronics manufacturer, known for producing a wide range of products, from smartphones to servers, for the world\u2019s top technology brands. Foxconn is building digital twins of its factories using Nvidia Omniverse, and Foxconn was also one of the first companies to use Nvidia NIM microservices inthe development of domain-specific large language models, or LLMs, embedded into a variety of internal systems and processes in its AI factories for smart manufacturing, smart electric vehicles and smart cities.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Artificial Intelligence","article_title":"Hugging Face\u2019s new tool lets devs build AI-powered web apps with OpenAI in just minutes","article_url":"https:\/\/venturebeat.com\/ai\/hugging-face-new-tool-developers-build-ai-web-apps-openai-minutes\/","article_date":"October 7, 2024 4:52 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreHugging Facehas released an innovative new Python package that allows developers to create AI-powered web apps with just a few lines of code.The tool, called \u201cOpenAI-Gradio,\u201d simplifies the process of integrating OpenAI\u2019s large language models (LLMs) into web applications, making AI more accessible to developers of all skill levels.The release signals a major shift in how companies can leverage AI, reducing development time while maintaining powerful, scalable applications.How developers can create web apps in minutes with OpenAI-GradioThe OpenAI-Gradio package integratesOpenAI\u2019s APIwithGradio, a popular interface tool for machine learning (ML) applications.In just a few steps, developers can install the package, set theirOpenAI API key, and launch a fully functional web app.The simplicity of this setup allows even smaller teams with limited resources to deploy advanced AI models quickly.For instance, after installing the package with:pip install openai-gradioA developer can write:import gradio as gr\nimport openai_gradio\n\ngr.load(\n    name='gpt-4-turbo',\n    src=openai_gradio.registry,\n).launch()This small amount of code spins up a Gradio interface connected to OpenAI\u2019s GPT-4-turbo model, allowing users to interact with state-of-the-art AI directly from a web app.Developers can also customize the interface further, adding specific input and output configurations or even embedding the app into larger projects.Simplifying AI development for businesses of all sizesHugging Face\u2019sopenai-gradiopackage removes traditional barriers to AI development, such as managing complex backend infrastructure or dealing with model hosting.By abstracting these challenges, the package enables businesses of all sizes to build and deploy AI-powered applications without needing large engineering teams or significant cloud infrastructure.This shift makes AI development more accessible to a much wider range of businesses. Small and mid-sized companies, startups, and online retailers can now quickly experiment with AI-powered tools, like automated customer service systems or personalized product recommendations, without the need for complex infrastructure.With these new tools, companies can create and launch AI projects in days instead of months.With Hugging Face\u2019s new openai-gradio tool, developers can quickly create interactive web apps, like this one powered by the GPT-4-turbo model, allowing users to ask questions and receive AI-generated responses in real-time. (Credit: Hugging Face \/ Gradio)Customizing AI interfaces with just a few lines of codeOne of the standout features ofopenai-gradiois how easily developers can customize the interface for specific applications.By adding a few more lines of code, they can adjust everything from the input fields to the output format, tailoring the app for tasks such as answering customer queries or generating reports.For example, developers can modify the interface to include specific prompts and responses, adjusting everything from the input method to the format of the output.This could involve creating a chatbot that handles customer service questions or a data analysis tool that generates insights based on user inputs.Here\u2019s an example provided by Gradio:gr.load(\n    name='gpt-4-turbo',\n    src=openai_gradio.registry,\n    title='OpenAI-Gradio Integration',\n    description=\"Chat with GPT-4-turbo model.\",\n    examples=[\"Explain quantum gravity to a 5-year-old.\", \"How many R's are in the word Strawberry?\"]\n).launch()The flexibility of the tool allows for seamless integration into broader web-based projects or standalone applications.The package also integrates seamlessly into largerGradio Web UIs, enabling the use of multiple models in a single application.Why this matters: Hugging Face\u2019s growing influence in AI developmentHugging Face\u2019s latest release positions the company as a key player in the AI infrastructure space. By making it easier to integrate OpenAI\u2019s models into real-world applications, Hugging Face is pushing the boundaries of what developers can achieve with minimal resources.This move also signals a broader trend toward AI-first development, where companies can iterate more quickly and deploy cutting-edge technology into production faster than ever before.The openai-gradio package is part of Hugging Face\u2019s broader strategy to empower developers and disrupt the traditional AI model development cycle.As Kevin Weil, OpenAI\u2019s Chief Product Officer,mentionedduring the company\u2019s recent DevDay, lowering the barriers to AI adoption is critical to accelerating its use across industries.Hugging Face\u2019s package directly addresses this need by simplifying the development process while maintaining the power ofOpenAI\u2019s LLMs.Hugging Face puts AI tools within everyone\u2019s reachHugging Face\u2019s openai-gradio package makes AI development as easy as writing a few lines of code. It opens the door for businesses to quickly build and deploy AI-powered web apps, leveling the playing field for startups and enterprises alike.The tool strips away much of the complexity that has traditionally slowed down AI adoption, offering a faster, more approachable way to harness the power of OpenAI\u2019s language models.As more industries dive into AI, the need for scalable, cost-effective tools has never been greater. Hugging Face\u2019s solution meets this need head-on, making it possible for developers to go from prototype to production in a fraction of the time.Whether you\u2019re a small team testing the waters or a larger company scaling up, openai-gradio offers a practical, no-nonsense approach to getting AI into the hands of users. In a landscape where speed and agility are everything, if you\u2019re not building with AI now, you\u2019re already playing catch-up.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Artificial Intelligence","article_title":"Clout Kitchen raises $4.45M for AI gaming pal that mimics content creators","article_url":"https:\/\/venturebeat.com\/games\/clout-kitchen-raises-4-45m-for-ai-gaming-pal-that-mimics-content-creators\/","article_date":"October 7, 2024 3:14 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreClout Kitchen announced today that it has raised $4.45 million in its seed funding round, which it plans to put towards its new creator-powered products and experiences. The first of these is Backseat AI, an AI-powered buddy forLeague of Legendsthat the company created with Tyler \u201cTyler1\u201d Steinkamp \u2014 an AI buddy that can take on the aspect of popular gaming content creators. Clout Kitchen plans to use its funding to expand its team and build out its shared internal tech stack.Backseat AI is a guide, built to be compliant with Riot Games\u2019 in-game API, that gives players helpful tips on how to better play the game, all spoken by an AI voice twin of a content creator. The product launched for early access testing in June, and the full release costs players $4.99 a month for each creator-specific \u201cbuddy\u201d they choose. At present, those buddies include Tyler1 himself, as well as cosplayer and streamer Emily \u201cEmiru\u201d Schunk and professional esports player Marcin \u201cJankos\u201d Jankowski. It\u2019s aimed at entry-to-mid-level players.GamesBeat spoke with Clout Kitchen CEO Justin Gorriceta-Banusing about Backseat AI and what it means for gamers and creators alike. Banusing said, \u201cBackseat is a real time buddy for League; it tells you how to play better and commentates on the action in the voice of your favorite creator. We also have visual overlays that provide tips, tricks and stats. Think of it like that friend who cheered us on and helped us get better at your favorite game growing up.\u201dPlaying games with an AI-powered friendPeak XV\u2019s Surge and a16z Speedrun led the round with AppWorks, Antler, Hustle Fund, Founders Launchpad, Orvel Ventures and several creators participating. Jack Soslow, investment partner at a16z Games, said in a statement, \u201cAt a16z, we believe in the transformative power of technology and games to reshape how we connect, create, and experience the world. Clout Kitchen embodies this vision wholly, and we are thrilled to support their journey as they pioneer new ways for fans to engage with the creators and games they love.\u201dBanusing said part of the inspiration for Backseat AI was Tyler1 becoming a father, as he wanted a way to keep up with his community without taking time away from his child. \u201cUltimately, what we do is help solve for the creator scale and time problem. Most creators monetize through ads, content subscriptions (like Patreon, which have the expectation of content releases), and brand deals. These take up time and may not necessarily always align with a creator\u2019s (and their community\u2019s) own interests and style. With AI buddies, we can give every single one of their community a way to interact with their favorite creator that\u2019s distinctly them \u2014 all without taking up a creator\u2019s precious time.\u201dBanusing also added that Clout Kitchen wishes to expand its products into other titles, and with other creators: \u201cAs we grow, we want to build more apps like this for other games and verticals \u2014 bringing your favorite creators as trusted buddies into everything that you love to do. We\u2019re exploring games like World of Warcraft, as well as non-gaming use cases like fitness. As we say internally \u2018\u201d\u2018all your favorite games, all your favorite people.'\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Security","article_title":"3thix partners with Avalanche on !eb3 gaming ad data","article_url":"https:\/\/venturebeat.com\/games\/3thix-partners-with-avalanche-on-web3-gaming-ad-data\/","article_date":"October 3, 2024 2:24 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreWeb3 finance company 3thix announced today that it\u2019s partnering with Avalanche on a new ad-tech layer, which would offer advertisers a decentralized means of obtaining consumer behavioral data without compromising privacy or protections. This follows3thix\u2019s $8.5 million fundraise, led by Xsolla, earlier this year to help monetize Web3 games.According to 3thix, this new blockchain-based solution would provide advertisers with a decentralized Identity for Advertisers, or IDFA. This would allow for more-targeted advertising, but without compromising users\u2019 privacy provided by Apple\u2019s protections. 3thix champions an ethical approach to game monetization and in-game transactions, and says this would allow users access to better ads while allowing advertisers to remain compliant with privacy laws.This ad-tech solution would be built on Avalanche\u2019s Layer 1 blockchain which allows transaction finality and privacy. Andrew Cooper, Avalanche\u2019s head of games, said in a statement, \u201c3thix\u2019s decentralized ecosystem revolutionizes in-game transactions using blockchain. Users earn rewards, advertisers get precise targeting, and developers monetize better, creating sustainable value for all.\u201dTimothy Tello, 3thix CEO, said in a statement, \u201cThe unified platform, along with Avalanche\u2019s industry-leading smart contracts technologies, is miles ahead of competing projects. Through this partnership, we are creating a blockchain-verifiable web of relations that defines how partnerships work and function in the imminent future.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Security","article_title":"Vera AI launches \u2018AI Gateway\u2019 to help companies safely scale AI without the risks","article_url":"https:\/\/venturebeat.com\/ai\/vera-ai-launches-ai-gateway-to-help-companies-safely-scale-ai-without-the-risks\/","article_date":"October 2, 2024 10:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreVera AI Inc., a startup focused on responsible artificial intelligence deployment, announced today the general availability of itsAI Gatewayplatform. The system aims to help organizations more quickly and safely implement AI technologies by providing customizable guardrails and model routing capabilities.\u201cWe\u2019re really excited to be announcing the general availability of our model routing and guardrails platform,\u201d said Liz O\u2019Sullivan, CEO and co-founder of Vera, in an interview with VentureBeat. \u201cWe\u2019ve been hard at work over the last year building something that could scalably and repeatably accelerate time to production for the kinds of business use cases that actually stand to generate a lot of excitement.\u201dVera AI\u2019s policy configuration interface, showcasing the platform\u2019s granular content moderation tools. The dashboard allows companies to customize AI safeguards, balancing the need for innovation with responsible content management \u2014 a key selling point in Vera\u2019s mission to make AI deployment both efficient and ethical. (Credit: Vera)Bridging the gap: How Vera\u2019s AI gateway tackles last-mile challengesThe launch comes at a time when many companies are eager to adopt generative AI and other advanced AI technologies, but remain hesitant due to potential risks and challenges in implementing safeguards. Vera\u2019s platform sits between users and AI models, enforcing policies and optimizing costs across different types of AI requests.\u201cBusinesses are only ever interested in doing one of three things, whether that\u2019s make more money, save more money, or reducing risk,\u201d O\u2019Sullivan explained. \u201cWe\u2019ve focused ourselves squarely on the last mile problems, which people think, just like regular software engineering, that it\u2019s going to be quick and easy, that these are just afterthoughts that you can apply to optimize costs or to reduce risks associated with things like disinformation and broad and CSAM, but they\u2019re actually quite hard.\u201dJustin Norman, CTO and co-founder of Vera, emphasized the importance of nuance in AI policy implementation: \u201cYou want to be able to set the bar for where your system will respond and where it will not respond and what it will do, without having to rely upon what some other companies made a decision for you on.\u201dVera AI\u2019s interface demonstrates its content moderation capabilities, blocking a user\u2019s input that failed to follow the specified rules \u2014 a key feature in the company\u2019s mission to provide guardrails for responsible AI deployment. (Credit: Vera)From AI safety activism to startup success: The minds behind VeraThe company\u2019s approach appears to be gaining traction. According to O\u2019Sullivan, Vera is already \u201cprocessing tens of thousands of model requests per month across a handful of paying customers.\u201d The startup offers API-based pricing at one cent per call, aligning its incentives with customer success in AI deployment. Additionally, Vera has introduced a 30-day free trial, which can be accessed using the code \u201cFRIENDS30,\u201d allowing potential customers to experience the platform\u2019s capabilities firsthand.Vera\u2019s launch is particularly noteworthy given the founders\u2019 backgrounds. O\u2019Sullivan, who serves on theNational AI Advisory Committee, has a history of AI safety activism, including her work atClarifai. Norman brings experience from government, academia, and industry, including PhD work at UC Berkeley focused onAI robustness and evaluation.Navigating the AI safety landscape: Vera\u2019s role in responsible innovationAs AI adoption accelerates across industries, platforms like Vera\u2019s could play a crucial role in addressing safety and ethical concerns while enabling innovation. The startup\u2019s focus on customizable guardrails and efficient model routing positions it well to serve both enterprise clients managing internal AI use and companies developing consumer-facing AI applications.However, Vera faces a competitive landscape with other AI safety and deployment startups also vying for market share. The company\u2019s success will likely depend on its ability to demonstrate clear value to customers and stay ahead of rapidly evolving AI technologies and associated risks.For organizations looking to responsibly implement AI, Vera\u2019s launch offers a new option to consider. As O\u2019Sullivan put it, \u201cWe\u2019re here to make it as easy as possible to enjoy the benefits of AI while reducing the risks that things do go wrong.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Security","article_title":"OpenAI\u2019s DevDay 2024: 4 major updates that will make AI more accessible and affordable","article_url":"https:\/\/venturebeat.com\/ai\/openai-devday-2024-4-major-updates-that-will-make-ai-more-accessible-and-affordable\/","article_date":"October 1, 2024 10:15 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIn a marked contrast to last year\u2019ssplashy event, OpenAI held a more subduedDevDay conferenceon Tuesday, eschewing major product launches in favor of incremental improvements to its existing suite of AI tools and APIs.The company\u2019s focus this year was on empowering developers and showcasing community stories, signaling a shift in strategy as the AI landscape becomes increasingly competitive.The company unveiled four major innovations at the event: Vision Fine-Tuning, Realtime API, Model Distillation, and Prompt Caching. These new tools highlight OpenAI\u2019s strategic pivot towards empowering its developer ecosystem rather than competing directly in the end-user application space.Prompt caching: A boon for developer budgetsOne of the most significant announcements is the introduction ofPrompt Caching, a feature aimed at reducing costs and latency for developers.This system automatically applies a 50% discount on input tokens that the model has recently processed, potentially leading to substantial savings for applications that frequently reuse context.\u201cWe\u2019ve been pretty busy,\u201d said Olivier Godement, OpenAI\u2019s head of product for the platform, at a small press conference at the company\u2019s San Francisco headquarters kicking off the developer conference. \u201cJust two years ago, GPT-3 was winning. Now, we\u2019ve reduced [those] costs by almost 1000x. I was trying to come up with an example of technologies who reduced their costs by almost 1000x in two years\u2014and I cannot come up with an example.\u201dThis dramatic cost reduction presents a major opportunity for startups and enterprises to explore new applications, which were previously out of reach due to expense.A pricing table from OpenAI\u2019s DevDay 2024 reveals major cost reductions for AI model usage, with cached input tokens offering up to 50% savings compared to uncached tokens across various GPT models. The new o1 model showcases premium pricing, reflecting its advanced capabilities. (Credit: OpenAI)Vision fine-tuning: A new frontier in visual AIAnother major announcement is the introduction of vision fine-tuning forGPT-4o, OpenAI\u2019s latest large language model. This feature allows developers to customize the model\u2019s visual understanding capabilities using both images and text.The implications of this update are far-reaching, potentially impacting fields such as autonomous vehicles, medical imaging, and visual search functionality.Grab, a leading Southeast Asian food delivery and rideshare company, has already leveraged this technology to improve its mapping services, according to OpenAI.Using just 100 examples, Grab reportedly achieved a 20 percent improvement in lane count accuracy and a 13 percent boost in speed limit sign localization.This real-world application demonstrates the possibilities for vision fine-tuning to dramatically enhance AI-powered services across a wide range of industries using small batches of visual training data.Realtime API: Bridging the gap in conversational AIOpenAI also unveiled itsRealtime API, now in public beta. This new offering enables developers to create low-latency, multimodal experiences, particularly in speech-to-speech applications. This means that developers can start adding ChatGPT\u2019s voice controls to apps.To illustrate the API\u2019s potential, OpenAI demonstrated an updated version ofWanderlust, a travel planning app showcased atlast year\u2019s conference.With the Realtime API, users can speak directly to the app, engaging in a natural conversation to plan their trips. The system even allows for mid-sentence interruptions, mimicking human dialogue.While travel planning is just one example, the Realtime API opens up a wide range of possibilities for voice-enabled applications across various industries.From customer service to education and accessibility tools, developers now have a powerful new resource to create more intuitive and responsive AI-driven experiences.\u201cWhenever we design products, we essentially look at like both startups and enterprises,\u201d Godement explained. \u201cAnd so in the alpha, we have a bunch of enterprises using the APIs, the new models of the new products as well.\u201dThe Realtime API essentially streamlines the process of building voice assistants and other conversational AI tools, eliminating the need to stitch together multiple models for transcription, inference, and text-to-speech conversion.Early adopters likeHealthify, a nutrition and fitness coaching app, andSpeak, a language learning platform, have already integrated the Realtime API into their products.These implementations showcase the API\u2019s potential to create more natural and engaging user experiences in fields ranging from healthcare to education.The Realtime API\u2019s pricing structure, while not inexpensive at $0.06 per minute of audio input and $0.24 per minute of audio output, could still represent a significant value proposition for developers looking to create voice-based applications.Model distillation: A step toward more accessible AIPerhaps the most transformative announcement was the introduction of Model Distillation. This integrated workflow allows developers to use outputs from advanced models likeo1-previewandGPT-4oto improve the performance of more efficient models such asGPT-4o mini.The approach could enable smaller companies to harness capabilities similar to those of advanced models without incurring the same computational costs.It addresses a long-standing divide in the AI industry between cutting-edge, resource-intensive systems and their more accessible but less capable counterparts.Consider a small medical technology start-up developing an AI-powered diagnostic tool for rural clinics. Using Model Distillation, the company could train a compact model that captures much of the diagnostic prowess of larger models while running on standard laptops or tablets.This could bring sophisticated AI capabilities to resource-constrained environments, potentially improving healthcare outcomes in underserved areas.OpenAI\u2019s strategic shift: Building a sustainable AI ecosystemOpenAI\u2019s DevDay 2024 marks a strategic pivot for the company, prioritizing ecosystem development over headline-grabbing product launches.This approach, while less exciting for the general public, demonstrates a mature understanding of the AI industry\u2019s current challenges and opportunities.This year\u2019s subdued event contrasts sharply with the 2023 DevDay, which generatediPhone-like excitementwith the launch of the GPT Store and custom GPT creation tools.However, the AI landscape has evolved rapidly since then. Competitors have madesignificant strides, and concerns about data availability for training have intensified. OpenAI\u2019s focus on refining existing tools and empowering developers appears to be a calculated response to these shifts. By improving the efficiency and cost-effectiveness of their models, OpenAI aims to maintain its competitive edge while addressing concerns aboutresource intensityandenvironmental impact.As OpenAI transitions from a disruptor to a platform provider, its success will largely depend on its ability to foster a thriving developer ecosystem.By providing improved tools, reduced costs, and increased support, the company is laying the groundwork for long-term growth and stability in the AI sector.While the immediate impact may be less visible, this strategy could ultimately lead to more sustainable and widespread AI adoption across many industries.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Security","article_title":"Artisan raises $11.5M to deploy AI \u2019employees\u2019 for sales teams","article_url":"https:\/\/venturebeat.com\/business\/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams\/","article_date":"September 30, 2024 6:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreArtisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company\u2019s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails.Founded just last year, Artisan has already reached $1 million in annual recurring revenue, with over 120 companies using its platform. The seed round was led by Oliver Jung, with participation from Y Combinator, HubSpot Ventures, Day One Ventures, and others.\u201cWe create AI employees called artisans, and then we consolidate software tools together to create this unified software ecosystem where AI employees are managing and doing your work for you,\u201d said Jaspar Carmichael-Jack, Artisan\u2019s 23-year-old CEO and co-founder, in an interview with VentureBeat.How Artisan\u2019s AI assistant streamlines sales processesArtisan\u2019s approach aims to streamline the fragmented landscape of sales software. Rather than integrating multiple tools, the company offers a single platform that handles tasks ranging from lead generation to email outreach. At the center is Ava, an AI assistant that can operate autonomously to find prospects, research companies, and craft personalized messages.\u201cAva finds leads for people that match their ICP [ideal customer profile]. We have access to over 300 million different B2B lead profiles,\u201d Carmichael-Jack explained. \u201cAva enriches leads using data sources like CrunchBase, Apollo, Cognism\u2026writes emails to the leads and LinkedIn messages, and automates the entire process.\u201dAI\u2019s impact on sales jobs: A shift in rolesCarmichael-Jack acknowledged that AI will likely replace some roles, but argued this shift is ultimately beneficial: \u201cI think there\u2019s going to be a shift from the manual, repetitive, automatable roles to more human centered roles,\u201d he said. \u201cHumans will be shifted to more human activities.\u201dArtisan plans to expand beyond sales, with AI assistants for marketing and customer success in development. The involvement of HubSpot as an investor signals that even established software providers see potential in AI-first approaches.\u201cHubSpot backing us has been like a really meaningful thing to us, because it\u2019s showing that even the legacy software providers are ready for the next paradigm of software to come,\u201d Carmichael-Jack noted.The Future of AI in Business OperationsAs Artisan pushes forward with its AI sales assistants, the line between human and machine in the workplace continues to blur. The question now isn\u2019t whether AI will transform sales, but how quickly.For businesses, the future of sales may be less about closing deals and more about choosing the right digital companion. In this new landscape, the best salesperson might just be the one you never see.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Security","article_title":"Why Microsoft\u2019s security initiative and Apple\u2019s cloud privacy matter to enterprises now","article_url":"https:\/\/venturebeat.com\/security\/why-microsofts-security-initiative-and-apples-cloud-privacy-matter-to-enterprises-right-now\/","article_date":"September 27, 2024 3:08 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreWith cyber threats growing more automated and malicious, securing enterprise data and privacy has never been more challenging.AppleandMicrosoft\u2018s new security initiatives capitalize on their core cloud security and privacy strengths to close security gaps and reduce risk for every business.Microsoft\u2019sSecure Future Initiative(SFI) and Apple\u2019sPrivate Cloud Compute(PCC) represent the latest enterprise-ready approaches to improving cloud security and privacy. The larger the enterprise, the more diverse its cybersecurity and privacy needs, so SFI and PCC are designed to deliver real-time responses at scale.Microsoft first unveiled the Secure Future Initiative (SFI) in Nov. 2023 to enhance its clients\u2019 enterprise cloud security infrastructure. SFI\u2019s goal is to deliver step-wise improvements in security across the Microsoft ecosystem. The company recently published itsSecure Future Initiative Progress Report.Apple launched itsPrivate Cloud Compute (PCC) platformin June 2024. ThePCCis a cloud intelligence system created specifically forprivate AI processing. Apple\u2019s device-level security and privacy architecture is core to PCC and extended to cloud-based AI operations. One of the PCC\u2019s primary design goals is to keep cloud-processed user data private. This is done with custom silicon, a hardened OS and privacy-preserving methods that manage data requests without storing data.Microsoft\u2019s Secure Future Initiative (SFI) is a multi-layered defense for enterprise securityAt its foundation, SFI is designed to embed security into every layer of Microsoft products and services as part of itssecure-by-designframework and more broadly speaking, a new security philosophy.Microsoft\u2019s Executive Vice President Takeshi Numoto recentlysaid, \u201cAt Microsoft, security is our top priority, and through SFI, we ensure that our products and AI systems are secure, private and safe.\u201d Microsoft reaffirmed itscommitment to TrustWorthy AIwith an announcement this week emphasizing responsible development and deployment of AI technologies.Six engineering pillars form the foundation of Microsoft\u2019s Secure Future Initiative (SFI) strategy. These pillars are designed to protect systems, data and identities while anticipating cybersecurity threats all from a common platform.Three core principles define SFI. These include secure by design, secure by default and secure operations. Microsoft committed to these in theirlatest report, saying all product teams will be using these principles and adopting theMicrosoft Security Development Lifecycle (SDL)as their development methodology.Source: Microsoft.Secure Future Initiative Progress Report, September 2024.Six engineering pillars make up Microsoft SFI:Protect identities and secrets. Securing identities is a critical focus of SFI, especially after the rise in identity-based breaches targeting Active Directory (AD), looking to take control of all identities in a company. Microsoft looks to significantly reduce enterprise identity-related attack surfaces by introducing phishing-resistant credentials and video-based identity verification.Protect tenants and isolate production systems.Microsoft designed SFI to strengthen network security by isolating production environments and improving compliance tracking. Also designed in are more stringent isolation policies across virtual networks and production systems to help prevent lateral movement of threats. Microsoft also vows to provide enhanced monitoring to ensure potential threats are identified and acted on quickly.Protect Networks.Core to SFI is improved monitoring of virtual networks by recording all assets in a central inventory and ensuring isolation between corporate and production networks. The teams who architected SFI are placing a high priority on enforcing micro-segmentation and minimizing the attack surface. A core construct of this area of SFI is that it ensures lateral movement within the network is limited and controlled, limiting the blast radius of a potential attack.Protect Engineering Systems. SFI\u2019s architects chose to rely on the Zero Trust framework to protect Microsoft\u2019s software development environments. Central to this approach is limiting the lifespan of personal access tokens and enforcing stringent checks during code development. Microsoft\u2019s SFI contends that these measures help prevent unauthorized access and protect critical resources during the software development lifecycle.Monitor and Detect Threats. Real-time threat detection is the cornerstone of SFI. Microsoft\u2019s SFI framework aims to enable all production systems to emit standardized security logs, providing timely visibility into network activities. This centralized logging enables faster identification of threats and helps enterprises proactively monitor malicious activities.Accelerate Response and Remediation.SFI also reduces threat identification and action time to address vulnerabilities quickly. Microsoft publishes critical vulnerabilities (CVEs) regardless of customer action, helping the industry adopt mitigation strategies faster. This proactive approach boosts cloud ecosystem security.Apple\u2019s Private Cloud Compute (PCC) has privacy at the coreWhile Microsoft concentrates on closing the gaps it sees across the cloud and entering infrastructure, Apple\u2019s Private Cloud Compute (PCC) capitalizes on the company\u2019s decades of R&D experience in privacy.Apple invested yearsof research and development in PCC, looking to create a stateless architecture that could ensure the privacy of customers\u2019 data at the silicon level, making it impossible for an insider attack inside the company to breach it.Of the many design goals that define the PCC, one of the most important is scaling Apple\u2019s industry-leading device privacy controls into cloud-based AI services. Apple\u2019s central goal is to set a new standard for secure cloud intelligence.Key features of PCC include the following:Stateless computation and enforceable privacy:PCC employs a unique stateless architecture that ensures sensitive data is processed only for its intended purpose and never retained after a process is complete. The stateless architecture is built on hardware-backed secure enclaves and cryptographic protocols to ensure data confidentiality during processing. PCC\u2019s memory is non-persistent, with all data cryptographically erased upon request completion.No privileged access:PCC implemented a zero-trust model that prevents any privileged access that could potentially bypass privacy controls. Apple achieves this by using a combination of hardware-enforced isolation, secure boot processes and code-signing algorithms. PCC is designed with such stringent privileged access that Apple\u2019s site reliability engineers cannot access user data or bypass security measures.Verifiable transparency to the log level.Cryptographically signed transparency logs of all software running on PCC nodes are published to enable third-party audits. The transparency logs are also used to verify that the code matches the reviewed software. Apple also provides a Virtual Research Environment for simulating PCC environments and offers bug bounties for discoveries across the entire PCC stack.Custom silicon and hardened OS.PCC leverages custom Apple silicon with built-in security features like the Secure Enclave and a hardened subset of iOS and macOS. This ensures that user data is processed in isolated environments with hardware-enforced security boundaries.Oblivious HTTP routing:PCC requests go through an independent Oblivious HTTP relay. This hides the request origin, preventing IP address-person correlation.Apple also designed end-to-end encryption, advanced anonymization techniques to protect data throughout its lifecycle, advanced access controls, and support for multi-factor authentication. The PCC also has real-time threat detection and supports regular security audits and penetration testing. For a thorough analysis of the PCC platform, seeVentureBeat\u2019s recent in-depth analysis.Security and privacy comparison: Microsoft SFI vs. Apple PCCIT and security teams are too busy to manage another platform. Microsoft and Apple are embedding security into their architectures to reduce this burden.SFI is how Microsoft is integrating security into Azure and Microsoft 365 at every layer. Hardware-level privacy protections in Apple\u2019s Private Cloud Compute (PCC) boost privacy. Both methods simplify critical security measures to keep teams safe without adding work.The following comparison is a short guide to help IT and security teams gain insights into the differences between each platform:Cloud security and threat modelApple PCC:Designed for secure AI cloud processing, it aims to prevent data leakage, insider threats, and targeted attacks, with robust measures to ensure privacy and security in cloud environments, according toApple\u2019s PCC blog postreleased earlier this year.Microsoft SFI:Focuses on reducing the attack surfaces across all Microsoft tenants and production environments, with a specific aim of preventing lateral movement between environments. SFI aligns with Zero Trust, a framework that assumes a breach has already happened and requires continuous verification of user and device identity, regardless of network location. Azure and Microsoft 365 ecosystems are protected by Zero Trust. For more information on the Zero Trust framework see theNIST standard, Special Publication 800-207, which outlines the key principles of Zero Trust Architecture (ZTA).Cultural IntegrationApple PCC:Prioritizes privacy through technical design rather than cultural changes. Privacy is embedded in both the hardware (Apple silicon) and software (iOS\/macOS), ensuring secure-by-design architecture without needing broad cultural shifts.Microsoft SFI:Security is embedded into all operations, from corporate governance to employee evaluations. TheMicrosoft Cybersecurity Governance Councilplays a key role in ensuring risk management is consistent across the company.Scope and Focus:Apple PCC:Focuses on AI privacy in cloud, multi-cloud and hybrid cloud environments. It is designed specifically for businesses seeking security and privacy assurances in AI applications, offering high levels of security for AI processing and data storage.Microsoft SFI:Microsoft\u2019s product and services-wide initiative to engrain security into the DNA of every product and service they offer. A comprehensive security framework that spans identity management, governance, employee training, and technical safeguards across its ecosystem, including Azure and Microsoft 365. It aims to secure all layers of its platform and user base.Technical Implementation:Apple PCC:Apple secures its framework with custom server hardware and silicon. Stateless computation reduces risks by not storing data between sessions. AI data privacy is a primary design goal by having an integrated hardware and software design. With privacy protections at its core, Apple\u2019s goal is to make PCC-based AI processing secure.Microsoft SFI:Microsoft\u2019s strategy weaves security into every phase of software development through a Secure Development Lifecycle (SDL), ensuring that security measures are incorporated from the design stage to deployment. CodeQL, an automated code analysis tool, meticulously scans for vulnerabilities within the code. Moreover, robust identity protection is guaranteed via MSAL (Microsoft Authentication Library), which oversees secure authentication and token management across various applications and services.Transparency and Governance:Apple PCC:Researchers can audit Apple\u2019s systems and view its AI processing environments in cryptographically signed transparency logs. Accountability allows businesses to evaluate and trust Apple\u2019s AI infrastructure without compromising sensitive data.Microsoft SFI:Microsoft\u2019s Secure Future Initiative (SFI) seeks to improve security transparency and cybersecurity across its products and services. Advanced security features like Azure Active Directory Conditional Access and Microsoft Defender for Cloud use machine learning algorithms to detect and respond to threats in real time. The company also launched Cyber Signals to provide threat intelligence insights and a Customer Security Management Office (CSMO) to improve security incident communication. These initiatives are promising, but Microsoft\u2019s handling of critical system flaws and data breaches shows the ongoing challenges of scaling cybersecurity.Why Microsoft SFI and Apple PCC signal a shift in enterprise securityRealizing that IT and security teams are overstretched already, and no one needs another platform to look after, Microsoft and Apple have taken unique approaches to make security and privacy the core of their DNA.For many IT and security leaders, these two platforms are overdue. SFI is a strong attempt to change the security of Microsoft DNA at its core. As the first generation of an entirely new era of security, SFI is comprehensive and sets the structure so security can become part of its DNA. Starting with the areas that are the most challenging for IT and security to deal with, SFI takes on the challenges of identity management, governance, and technical safeguards.Apple\u2019s continual investments in privacy pay dividends in PCC. Their prioritizing AI cloud privacy, and embedding privacy protections directly into silicon and operating system software make them unlike any other platform vendors offering privacy at scale.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Infrastructure","article_title":"Microsoft just dropped Drasi, and it could change how we handle big data","article_url":"https:\/\/venturebeat.com\/ai\/microsoft-just-dropped-drasi-and-it-could-change-how-we-handle-big-data\/","article_date":"October 3, 2024 8:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMicrosofthas launchedDrasi, a new open-source data processing system designed to simplify the detection and reaction to critical events in complex infrastructures.This release follows last year\u2019s launch ofRadius, an open application platform for the cloud, and further cements Microsoft\u2019s commitment to open-source innovation in cloud computing.Mark Russinovich, CTO and Technical Fellow at Microsoft Azure described Drasi as \u201cthe birth of a new category of data processing system\u201d in an interview with VentureBeat.He explained that Drasi emerged from recognizing the growing complexity in event-driven architectures, particularly in scenarios like IoT edge deployments and smart building management.From complexity to clarity\u201cWe saw massive simplification of the architecture, just incredible developer productivity,\u201d Russinovich said, highlighting Drasi\u2019s potential to reduce the complexity of reactive systems.Drasi works by continuously monitoring data sources, evaluating incoming changes through predefined queries and executing automated reactions when specific conditions are met.This approach eliminates the need for inefficient polling mechanisms or constant data source querying, which can lead to performance bottlenecks in large-scale systems.The system\u2019s key innovation lies in its use of continuous database queries to monitor state changes. \u201cWhat Drasi does is takes that and says, I just have a database query\u2026 and when an event comes in\u2026 Drasi knows, \u2018Hey, part of this query is satisfied,'\u201d Russinovich explained.Open-source synergyMicrosoft\u2019s decision to release Drasi as anopen-source projectaligns with its broader strategy of contributing to the open-source community, particularly in cloud-native computing.This strategy is evident in the recent launch of Radius, which addresses challenges in deploying and managing cloud-native applications across multiple environments.\u201cWe believe in contributing to the open-source community because\u2026 many enterprises are making strategies that are, especially around Cloud Native Computing, centered on open-source software and open governance,\u201d Russinovich said.The Azure Incubations team, responsible for bothDrasiandRadius, has a track record of launching successful open-source projects includingDapr,KEDAandCopacetic. These projects are all available through the Cloud Native Computing Foundation (CNCF).While Radius focuses on application deployment and management, Drasi tackles the complexities of event-driven architectures. Together, these tools represent Microsoft\u2019s holistic approach to addressing the challenges faced by developers and operations teams in modern cloud environments.Drasi\u2019s continuous queries usher in a new era of reactive systemsLooking ahead, Russinovich hinted at the possible integration of Drasi into Microsoft\u2019s data services. \u201cIt looks like it\u2019ll probably slot into our data services, where you have Drasi integrated into Postgres database or Cosmos DB, or as a standalone service that integrates across these,\u201d he said.The introduction of Drasi could have significant implications for businesses grappling with the complexities of cloud-native development and event-driven architectures. By simplifying these processes, Microsoft aims to enable organizations to build more responsive and efficient applications, potentially leading to improved operational efficiency and faster time-to-market for new features.As with Radius, Microsoft is actively seeking feedback from partners and early adopters to refine Drasi and address any scaling, performance, or security concerns that may arise in production environments. The true test for both tools will be their adoption and performance in real-world scenarios across various cloud providers and on-premises environments.As businesses increasingly rely on cloud-native applications and real-time data processing, tools like Drasi and Radius could play a crucial role in managing the growing complexity of modern software systems.Whether Drasi will indeed establish itself as a new category of data processing system, as Russinovich suggests, remains to be seen, but its introduction marks another significant step in Microsoft\u2019s ongoing efforts to shape the future of cloud computing through open-source innovation.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Infrastructure","article_title":"Voyage AI\u2019s multilingual embeddings boost Snowflake\u2019s Cortex AI for improved enterprise RAG","article_url":"https:\/\/venturebeat.com\/ai\/why-snowflake-is-backing-embedding-startup-voyage-ai-to-improve-enterprise-rag\/","article_date":"October 3, 2024 6:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIn the world of Retrieval Augmented Generation (RAG) for enterprise AI, embedding models are critical.It is the embedding model that essentially translates different types of content into vectors, where it can be understood and used by AI and RAG approaches.OpenAIat one point dominated the embeddings space with its ada embeddings model, but some enterprises have come to realize over time that it\u2019s not specific enough for their particular use cases. That\u2019s whereVoyage AIfits into the market.The startup today announced that it has raised a $20 million series A round of funding to advance the development of its embedding and retrieval models for enterprise RAG AI use cases. Among the company\u2019s backers is cloud data vendorSnowflake, which is now also set to integrate the Voyage AI models into itsCortex AI service. Specifically, the Voyage AI will land in the Cortex AI search service which is based on technology from Snowflake\u2019s acquisition ofAI search vendor Neeva.Voyage AI\u2019s mission is all about making enterprise RAG better. The company has a multilingual embedding model that supports 27 languages, with a high degree of accuracy.\u201cBasically, we make RAG better by improving the retrieval quality,\u201d Tengyu Ma, founder and CEO of Voyage AI, told VentureBeat. \u201cWhen you have more relevant documents, the response becomes better, because if you don\u2019t have relevant documents, then the large language model will hallucinate.\u201dHow Voyage AI improves enterprise RAG with better embeddingsEmbedding models are nothing new and are a foundational element of large language model (LLM) training and RAG deployments.Ma explained that Voyage AI is about building embedding and reranker models for improving retrieval quality. Ma said that when it comes to RAG where specific domain or enterprise information is needed, existing approaches, particularly OpenAI\u2019s approach, aren\u2019t enough.\u201cI think people realize that OpenAI\u2019s ada is not good enough now, because when you have higher and higher accuracy requirements, it is not accurate enough,\u201d Ma said. \u201cSo we do embeddings with better accuracy and more understanding of complex concepts.\u201dHe explained that the way Voyage AI improves accuracy is with a number of advanced techniques. Voyage AI optimizes every part of the training pipeline. That includes collecting and filtering the data. Ma also noted that his company trains its models for different specific domains such as coding, finance and legal use cases.\u201cThis allows us to get even better performance for a particular domain,\u201d he said.How a contrastive learning approach improves trainingTraining is often a particularly thorny issue as most data is unlabelled.In order to get value from unlabelled data for an enterprise, Voyage AI uses a technique called contrastive learning to train its models. Ma explained that contrastive learning is a different approach than the typical \u2018next word prediction\u2019 approach that is used for some training operations. In the next-word approach, the model predicts what word or words should follow another word or phrase based on patterns. Contrastive learning takes a different path.\u201cYou create this kind of so called contrastive pairs from unlabeled data, and use that to train the model,\u201d Ma said.Why Snowflake is embracing Voyage AI to improve enterprise RAGFor Snowflake, supporting Voyage AI and integrating it into its Cortex AI services, is all about making AI more useful to enterprise users.\u201cEvery provider is trying to build some kind of a RAG system and very much the angle we take is you point us at the data, you can talk to your data, and whether it\u2019s structured or unstructured, it will just work,\u201d Vivek Raghunathan, SVP of Engineering at Snowflake told VentureBeat.Raghunathan added that Snowflake is excited about Voyage AI\u2019s models because of the improved and advanced capabilities that they will bring to Snowflake\u2019s customers including multilingual capabilities. He also noted that Voyage AI provides longer context windows which will also help to improve enterprise use cases.Snowflake already has its ownArctic embedding modelwhich is currently often the default. The Voyage AI models will provide an optional alternative for users.\u201cThink of the Pareto frontier of efficiency versus quality, our models tend to be focused for a certain size,\u201d Raghunathan said. \u201cVoyage AI \u2018s models are far higher quality for the really hard use cases.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Infrastructure","article_title":"Google Cloud brings tech behind Search and YouTube to enterprise gen AI apps","article_url":"https:\/\/venturebeat.com\/data-infrastructure\/google-cloud-brings-tech-behind-search-and-youtube-to-enterprise-gen-ai-apps\/","article_date":"October 2, 2024 12:15 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreAs the generative AI continues to progress, having a simple chatbot may no longer be enough for many enterprises.Cloud hyperscalers are racing to build up their databases and tools to help enterprises deploy operational data quickly and efficiently, letting them build applications that are both intelligent and contextually aware.Case in point:Google Cloud\u2019srecent barrage of updates for multiple database offerings, starting withAlloyDB.According to a blog post from the company, the fully managed PostgreSQL-compatible database now supportsScaNN (scalable nearest neighbor)vector index in general availability. The technology powers its Search and YouTube services and paves the way for faster index creation and vector queries while consuming far less memory.In addition, the company also announced a partnership with Aiven for the managed deployment of AlloyDB as well as updates forMemorystore for ValkeyandFirebase.Understanding the value of ScaNN for AlloyDBVector databasesare critical to power advanced AI workloads, right from RAG chatbots to recommender systems.At the heart of these systems sit key capabilities like storing and managing vector embeddings (numerical representation of data) and conducting similarity searches needed for the targeted applications.As most developers in the world prefer PostgreSQL as the go-to operational database, its extension for vector search, pgvector, has become highly popular. Google Cloud already supports it on AlloyDB for PostgreSQL, with a state-of-the-art graph-based algorithm called Hierarchical Navigable Small World (HNSW) handling vector jobs.However, on occasions where the vector workload is too large, the performance of the algorithm may decline, leading to application latencies and high memory usage.To address this, Google Cloud is making ScaNN vector index in AlloyDB generally available. This new index uses the same technology that powers Google Search and YouTube to deliver up to four times faster vector queries and up to eight-fold faster index build times, with a 3-4x smaller memory footprint than the HNSW index in standard PostgreSQL.\u201cThe ScaNN index is the firstPostgreSQL-compatible index that can scale to support more than one billion vectors while maintaining state-of-the-art query performance \u2014 enabling high-performance workloads for every enterprise,\u201d Andi Gutmans, the GM and VP of engineering for Databases at Google Cloud, wrote in ablog post.Gutmans also announced a partnership with Aiven to make AlloyDB Omni, the downloadable edition of AlloyDB, available as a managed service that runs anywhere, including on-premises or on the cloud.\u201cYou can now run transactional, analytical, and vector workloads across clouds on a single platform, and easily get started building gen AI applications, also on any cloud. This is the first partnership that adds an administration and management layer for AlloyDB Omni,\u201d he added.What\u2019s new in Memorystore for Valkey and Firebase?In addition to AlloyDB, Google Cloud announced enhancements for Memorystore for Valkey, the fully managed cluster for the Valkey in-memory database, and the Firebase application development platform.For the Valkey offering, the company said it is adding vector search capabilities. Gutmans noted that a single Memorystore for Valkey instance can now perform similarity search at single-digit millisecond latency on over a billion vectors, with more than 99% recall.He also added that the next version of Memorystore for Valkey, 8.0, is now in public preview with 2x faster querying speed as compared to Memorystore for Redist Cluster, a new replication scheme, networking enhancements and detailed visibility into performance and resource usage.As for Firebase, Google Cloud is adding Data Connect, a new backend-as-a-service that will be integrated with a fully managed PostgreSQL database powered byCloud SQL. It will go into public preview later this year.With these developments, Google Cloud hopes developers will have a broader selection of infrastructure and database capabilities \u2014 along with powerful language models \u2013 to build intelligent applications for their organizations. It remains to be seen how these new advancements are deployed to real use cases, but the general trend indicates the volume of gen AI applications is expected to soar significantly.Omdiaestimates that the market for generative AI applications will grow from $6.2 billion in 2023 to $58.5 billion in 2028, marking a CAGR of 56%.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Infrastructure","article_title":"Vera AI launches \u2018AI Gateway\u2019 to help companies safely scale AI without the risks","article_url":"https:\/\/venturebeat.com\/ai\/vera-ai-launches-ai-gateway-to-help-companies-safely-scale-ai-without-the-risks\/","article_date":"October 2, 2024 10:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreVera AI Inc., a startup focused on responsible artificial intelligence deployment, announced today the general availability of itsAI Gatewayplatform. The system aims to help organizations more quickly and safely implement AI technologies by providing customizable guardrails and model routing capabilities.\u201cWe\u2019re really excited to be announcing the general availability of our model routing and guardrails platform,\u201d said Liz O\u2019Sullivan, CEO and co-founder of Vera, in an interview with VentureBeat. \u201cWe\u2019ve been hard at work over the last year building something that could scalably and repeatably accelerate time to production for the kinds of business use cases that actually stand to generate a lot of excitement.\u201dVera AI\u2019s policy configuration interface, showcasing the platform\u2019s granular content moderation tools. The dashboard allows companies to customize AI safeguards, balancing the need for innovation with responsible content management \u2014 a key selling point in Vera\u2019s mission to make AI deployment both efficient and ethical. (Credit: Vera)Bridging the gap: How Vera\u2019s AI gateway tackles last-mile challengesThe launch comes at a time when many companies are eager to adopt generative AI and other advanced AI technologies, but remain hesitant due to potential risks and challenges in implementing safeguards. Vera\u2019s platform sits between users and AI models, enforcing policies and optimizing costs across different types of AI requests.\u201cBusinesses are only ever interested in doing one of three things, whether that\u2019s make more money, save more money, or reducing risk,\u201d O\u2019Sullivan explained. \u201cWe\u2019ve focused ourselves squarely on the last mile problems, which people think, just like regular software engineering, that it\u2019s going to be quick and easy, that these are just afterthoughts that you can apply to optimize costs or to reduce risks associated with things like disinformation and broad and CSAM, but they\u2019re actually quite hard.\u201dJustin Norman, CTO and co-founder of Vera, emphasized the importance of nuance in AI policy implementation: \u201cYou want to be able to set the bar for where your system will respond and where it will not respond and what it will do, without having to rely upon what some other companies made a decision for you on.\u201dVera AI\u2019s interface demonstrates its content moderation capabilities, blocking a user\u2019s input that failed to follow the specified rules \u2014 a key feature in the company\u2019s mission to provide guardrails for responsible AI deployment. (Credit: Vera)From AI safety activism to startup success: The minds behind VeraThe company\u2019s approach appears to be gaining traction. According to O\u2019Sullivan, Vera is already \u201cprocessing tens of thousands of model requests per month across a handful of paying customers.\u201d The startup offers API-based pricing at one cent per call, aligning its incentives with customer success in AI deployment. Additionally, Vera has introduced a 30-day free trial, which can be accessed using the code \u201cFRIENDS30,\u201d allowing potential customers to experience the platform\u2019s capabilities firsthand.Vera\u2019s launch is particularly noteworthy given the founders\u2019 backgrounds. O\u2019Sullivan, who serves on theNational AI Advisory Committee, has a history of AI safety activism, including her work atClarifai. Norman brings experience from government, academia, and industry, including PhD work at UC Berkeley focused onAI robustness and evaluation.Navigating the AI safety landscape: Vera\u2019s role in responsible innovationAs AI adoption accelerates across industries, platforms like Vera\u2019s could play a crucial role in addressing safety and ethical concerns while enabling innovation. The startup\u2019s focus on customizable guardrails and efficient model routing positions it well to serve both enterprise clients managing internal AI use and companies developing consumer-facing AI applications.However, Vera faces a competitive landscape with other AI safety and deployment startups also vying for market share. The company\u2019s success will likely depend on its ability to demonstrate clear value to customers and stay ahead of rapidly evolving AI technologies and associated risks.For organizations looking to responsibly implement AI, Vera\u2019s launch offers a new option to consider. As O\u2019Sullivan put it, \u201cWe\u2019re here to make it as easy as possible to enjoy the benefits of AI while reducing the risks that things do go wrong.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Infrastructure","article_title":"Nvidia just dropped a bombshell: Its new AI model is open, massive, and ready to rival GPT-4","article_url":"https:\/\/venturebeat.com\/ai\/nvidia-just-dropped-a-bombshell-its-new-ai-model-is-open-massive-and-ready-to-rival-gpt-4\/","article_date":"October 1, 2024 3:58 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreNvidiahas released a powerful open-source artificial intelligence model that competes with proprietary systems from industry leaders like OpenAI and Google.The company\u2019s newNVLM 1.0family of large multimodal language models, led by the 72 billion parameterNVLM-D-72B, demonstrates exceptional performance across vision and language tasks while also enhancing text-only capabilities.\u201cWe introduce NVLM 1.0, a family of frontier-class multimodal large language models that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models,\u201d the researchers explain intheir paper.By making the model weightspublicly availableand promising to release thetraining code, Nvidia breaks from the trend of keeping advanced AI systems closed. This decision grants researchers and developers unprecedented access to cutting-edge technology.Benchmark results comparing NVIDIA\u2019s NVLM-D model to AI giants like GPT-4, Claude 3.5, and Llama 3-V, showing NVLM-D\u2019s competitive performance across various visual and language tasks. (Credit: arxiv.org)NVLM-D-72B: A versatile performer in visual and textual tasksThe NVLM-D-72B model shows impressive adaptability in processing complex visual and textual inputs. Researchers provided examples that highlight the model\u2019s ability to interpret memes, analyze images, and solve mathematical problems step-by-step.Notably, NVLM-D-72B improves its performance on text-only tasks after multimodal training. While many similar models see a decline in text performance, NVLM-D-72B increased its accuracy by an average of 4.3 points across key text benchmarks.\u201cOur NVLM-D-1.0-72B demonstrates significant improvements over its text backbone on text-only math and coding benchmarks,\u201d the researchers note, emphasizing a key advantage of their approach.NVIDIA\u2019s new AI model analyzes a meme comparing academic abstracts to full papers, demonstrating its ability to interpret visual humor and scholarly concepts. (Credit: arxiv.org)AI researchers respond to Nvidia\u2019s open-source initiativeThe AI community has reacted positively to the release. One AI researcher commenting on social media, observed, \u201cWow! Nvidia just published a 72B model with is ~on par with llama 3.1 405B in math and coding evals and also has vision ?\u201dNvidia\u2019s decision to make such a powerful model openly available could accelerate AI research and development across the field. By providing access to a model that rivals proprietary systems from well-funded tech companies, Nvidia may enable smaller organizations and independent researchers to contribute more significantly to AI advancements.The NVLM project also introduces innovative architectural designs, including a hybrid approach that combines different multimodal processing techniques. This development could shape the direction of future research in the field.Wow nvidia just published a 72B model with is ~on par with llama 3.1 405B in math and coding evals and also has vision ?pic.twitter.com\/c46DeXql7s\u2014 Phil (@phill__1)October 1, 2024NVLM 1.0: A new chapter in open-source AI developmentNvidia\u2019s release of NVLM 1.0 marks a pivotal moment in AI development. By open-sourcing a model that rivals proprietary giants, Nvidia isn\u2019t just sharing code\u2014it\u2019s challenging the very structure of the AI industry.This move could spark a chain reaction. Other tech leaders may feel pressure to open their research, potentially accelerating AI progress across the board. It also levels the playing field, allowing smaller teams and researchers to innovate with tools once reserved for tech giants.However, NVLM 1.0\u2019s release isn\u2019t without risks. As powerful AI becomes more accessible, concerns about misuse and ethical implications will likely grow. The AI community now faces the complex task of promoting innovation while establishing guardrails for responsible use.Nvidia\u2019s decision also raises questions about the future of AI business models. If state-of-the-art models become freely available, companies may need to rethink how they create value and maintain competitive edges in AI.The true impact of NVLM 1.0 will unfold in the coming months and years. It could usher in an era of unprecedented collaboration and innovation in AI. Or, it might force a reckoning with the unintended consequences of widely available, advanced AI.One thing is certain: Nvidia has fired a shot across the bow of the AI industry. The question now is not if the landscape will change, but how dramatically\u2014and who will adapt fast enough to thrive in this new world of open AI.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Automation","article_title":"Nvidia just dropped a bombshell: Its new AI model is open, massive, and ready to rival GPT-4","article_url":"https:\/\/venturebeat.com\/ai\/nvidia-just-dropped-a-bombshell-its-new-ai-model-is-open-massive-and-ready-to-rival-gpt-4\/","article_date":"October 1, 2024 3:58 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreNvidiahas released a powerful open-source artificial intelligence model that competes with proprietary systems from industry leaders like OpenAI and Google.The company\u2019s newNVLM 1.0family of large multimodal language models, led by the 72 billion parameterNVLM-D-72B, demonstrates exceptional performance across vision and language tasks while also enhancing text-only capabilities.\u201cWe introduce NVLM 1.0, a family of frontier-class multimodal large language models that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models,\u201d the researchers explain intheir paper.By making the model weightspublicly availableand promising to release thetraining code, Nvidia breaks from the trend of keeping advanced AI systems closed. This decision grants researchers and developers unprecedented access to cutting-edge technology.Benchmark results comparing NVIDIA\u2019s NVLM-D model to AI giants like GPT-4, Claude 3.5, and Llama 3-V, showing NVLM-D\u2019s competitive performance across various visual and language tasks. (Credit: arxiv.org)NVLM-D-72B: A versatile performer in visual and textual tasksThe NVLM-D-72B model shows impressive adaptability in processing complex visual and textual inputs. Researchers provided examples that highlight the model\u2019s ability to interpret memes, analyze images, and solve mathematical problems step-by-step.Notably, NVLM-D-72B improves its performance on text-only tasks after multimodal training. While many similar models see a decline in text performance, NVLM-D-72B increased its accuracy by an average of 4.3 points across key text benchmarks.\u201cOur NVLM-D-1.0-72B demonstrates significant improvements over its text backbone on text-only math and coding benchmarks,\u201d the researchers note, emphasizing a key advantage of their approach.NVIDIA\u2019s new AI model analyzes a meme comparing academic abstracts to full papers, demonstrating its ability to interpret visual humor and scholarly concepts. (Credit: arxiv.org)AI researchers respond to Nvidia\u2019s open-source initiativeThe AI community has reacted positively to the release. One AI researcher commenting on social media, observed, \u201cWow! Nvidia just published a 72B model with is ~on par with llama 3.1 405B in math and coding evals and also has vision ?\u201dNvidia\u2019s decision to make such a powerful model openly available could accelerate AI research and development across the field. By providing access to a model that rivals proprietary systems from well-funded tech companies, Nvidia may enable smaller organizations and independent researchers to contribute more significantly to AI advancements.The NVLM project also introduces innovative architectural designs, including a hybrid approach that combines different multimodal processing techniques. This development could shape the direction of future research in the field.Wow nvidia just published a 72B model with is ~on par with llama 3.1 405B in math and coding evals and also has vision ?pic.twitter.com\/c46DeXql7s\u2014 Phil (@phill__1)October 1, 2024NVLM 1.0: A new chapter in open-source AI developmentNvidia\u2019s release of NVLM 1.0 marks a pivotal moment in AI development. By open-sourcing a model that rivals proprietary giants, Nvidia isn\u2019t just sharing code\u2014it\u2019s challenging the very structure of the AI industry.This move could spark a chain reaction. Other tech leaders may feel pressure to open their research, potentially accelerating AI progress across the board. It also levels the playing field, allowing smaller teams and researchers to innovate with tools once reserved for tech giants.However, NVLM 1.0\u2019s release isn\u2019t without risks. As powerful AI becomes more accessible, concerns about misuse and ethical implications will likely grow. The AI community now faces the complex task of promoting innovation while establishing guardrails for responsible use.Nvidia\u2019s decision also raises questions about the future of AI business models. If state-of-the-art models become freely available, companies may need to rethink how they create value and maintain competitive edges in AI.The true impact of NVLM 1.0 will unfold in the coming months and years. It could usher in an era of unprecedented collaboration and innovation in AI. Or, it might force a reckoning with the unintended consequences of widely available, advanced AI.One thing is certain: Nvidia has fired a shot across the bow of the AI industry. The question now is not if the landscape will change, but how dramatically\u2014and who will adapt fast enough to thrive in this new world of open AI.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Automation","article_title":"Meta, Outshift, Intuit and Asana dig into the agentic AI future","article_url":"https:\/\/venturebeat.com\/ai\/meta-outshift-intuit-and-asana-dig-into-the-agentic-ai-future\/","article_date":"October 1, 2024 7:10 AM","article_text":"While business and technology leaders are working to reap value from AI today, the future is barreling toward us, and it\u2019s vital to build a foundation for what\u2019s quickly coming. Tech leaders are betting that the future of AI is agentic. In other words, organizations will adopt intelligent systems\u00a0that not only perform tasks autonomously, but also make decisions with near-human-like\u00a0precision, from writing code to handling ecommerce operations, functioning as automated sales agents and more.Agentic AI was the focus of the VentureBeat AI Impact Tour: \u201cAgentic AI \u2014 the next giant leap forward in the AI revolution,\u201d presented byOutshift by Cisco. Speakers from Outshift, Meta, Asana and Intuit joined VB CEO Matt Marshall to explore how orgs can plan for an agentic AI future and other onrushing advances in AI.Building out an agentic future\u201cIf you think about that future where these agentic systems are going to work together to solve bigger problems, we need distributed agentic systems computing and we need an open, interoperable internet of agents,\u201d Vijoy Pandey, GM and SVP, Outshift by Cisco told Marshall. \u201cInnovation slows down when you\u2019re in a walled garden. Whether you\u2019re an infrastructure vendor, operator, an app developer and most importantly a consumer or customer, an open system provides value for each individual link in the chain.\u201d\u201cInnovation slows down when you\u2019re in a walled garden.\u201dAgentic systems that learn how talk to each other and interconnect have the power to change the way humans work, starting with software and IT and then moving toward knowledge work, services and even physical work as robotics evolve. They\u2019ll also need to be integrated into existing software systems and physical environments, as well as instantiated on those existing software systems, whether it\u2019s cloud, on-prem or embedded in a robotic solution.Tying it all together requires abstraction layers. That will look like open models, open tooling, an orchestration and discovery layer and then a communication layer that is secure, stable and open. Then there\u2019s handling probabilistic outcomes, communication through NLP, the exchange of state information and more.\u201cThese could be pretty massive problems to solve,\u201d Pandey said. \u201cWe\u2019re looking for these problems and what they look like and how to solve them. That\u2019s where the future is.\u201dThe time to start on AI agents is nowToday\u2019s big question is whether the technology is mature enough to realize its full potential \u2014 and it isn\u2019t quite yet. However, that can\u2019t be a barrier, Mano Paluri, VP of generative AI engineering at Meta, said, during a one-on-one conversation with Marshall.\u201cYou can\u2019t wait. Agents clearly feel like the next step in the evolution of these models.\u201d\u201cYou can\u2019t wait,\u201d he added. \u201cIn that sense, I would say that it\u2019s ready. Agents clearly feel like the next step in the evolution of these models. The way we have been thinking about it is moving away from a model to a system that has multiple components that are customizable.\u201dIn the hunt for autonomous systems that can foresee, learn, reason, act and iterate to solve a complex problem, we\u2019ve already come far in perception \u2014 foundational models are able to learn from text and images. We\u2019re still in the early stages of reasoning through complex problems, but today models can learn at far larger scales than ever before over the last decade. And these models are beginning to plan, from both an inner and an outer loop perspective. Today the outer loop is the human training the model. Next will be the agent handling parameters itself.The Meta AI agentToday\u2019s Meta AI agent is the first step in the evolution of LLMs as Meta moves away from a model to a system that has multiple, customizable components. The goal is to fine-tune the model for every use case, extend the context window, adapt to a new language and so on, for all four billion customers.\u201cWe also believe in a family of agents,\u201d he said. \u201cThis incarnation of Meta AI is a user assistant, but we also think everyone should be able to customize the agent in the way that they want. This is the family of agents where businesses can create a billing agent. Creators can have their own agent to reach a larger scale. Advertisers can have creation capabilities that are unprecedented.\u201dAgentic AI use cases: challenges and opportunitiesTo close out the night, Paige Costello, head of AI at Asana, Shubha Pant, VP, AI\/ML at Outshift by Cisco, Kumar Sricharan, VP of technology and chief architect for AI at Intuit, joined Marshall for a conversation about the use cases agentic AI will open up, and the challenges and opportunities that will come hand in hand.Real-world case studiesHandling requests within a workflow can be a huge time suck, but that\u2019s where agentic AI comes in. Asana has embedded agentic AI for both chat and workflow use cases. In the case of workflows, it can handle a request at the outset, determining where to prioritize it, whether there\u2019s enough information to get started and who should be included. It\u2019s a great place for a company to start adding agentic workflows, Costello added.\u201cThe agentic piece is, how much decision-making or autonomy does it have to do these things in the context of those workflows?\u201d\u201cThere are so many opportunities where AI can be a partner in doing this work,\u201d Costello said. \u201cThe agentic piece is, how much decision-making or autonomy does it have to do these things in the context of those workflows? We\u2019ve seen great success with security companies, with marketing agencies. There are many other use cases where we\u2019re starting to see things like creative requests, working through revisions and feedback and approval loops.\u201dAt Intuit, they\u2019re automating across their entire suite of financial products, and offering insights and financial guidance. Across that ecosystem of tasks, they\u2019re experimenting with AI, especially in areas where building hand-engineered solutions would be time-consuming, or even just fizzle. For instance, small businesses have a broad array of characteristics and needs. During onboarding, the customer is required to detail all that information so that Intuit can classify them.\u201cThese agents essentially autonomously operate on top of that information and help the customer onboard with minimal effort on their part.\u201d\u201cNow, with the rise of agentic AI, we\u2019re finding that we can use these systems, these different agents that can work in unison to allow the customer to give us access to the different sources of their information,\u201d Sricharan said. \u201cThen these agents essentially autonomously operate on top of that information and help the customer onboard with minimal effort on their part.\u201dInternally, agentic AI is helping the company navigate tax code changes that impact products. Instead of a team of developers researching and implementing new elements, they can use agentic AI there to span a variety of functions, all the way from detecting where the changes are, to making associations with our code and determining what changes need to be then made, acting as a copilot for developers.\u201cThe goal is to predict IT issues before they arise, identify root causes when issues occur and offer mitigation strategies until full resolution.\u201dOutshift has an incubation team focused on building a multi-agent predictive diagnostic and remediation tool for enterprises across their tech stacks. The goal is to predict IT issues before they arise, identify root causes when issues occur and offer mitigation strategies until full resolution.\u00a0There are other agentic AI projects in progress including architecture development, open standards for orchestration of agents, composition of multi agent systems, and an open agent protocol for inter-agent communication.\u201cThe main challenges for agentic AI right now are threefold,\u201d Pandey said. \u201cFirst, how AI agents discover each other and understand each others\u2019 capabilities. Second, how they collaborate to solve problems and handle uncertain outcomes. And third, how they communicate using imprecise natural language instead of fixed structures like traditional APIs.\u201d\u201cWe need to figure out how to create standards and open-source guidelines for these AI systems that deal with probabilities,\u201d he added. \u201cIt\u2019s time for the tech community to join forces and build these solutions together.\u201dSponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they\u2019re always clearly marked. For more information, contactsales@venturebeat.com."},{"topic_title":"Automation","article_title":"This open-source AI tool was built in a day and it\u2019s coming for Google\u2019s NotebookLM","article_url":"https:\/\/venturebeat.com\/ai\/this-open-source-ai-tool-was-built-in-a-day-and-its-coming-for-googles-notebooklm\/","article_date":"September 30, 2024 10:47 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreGabriel Chua, a data scientist at Singapore\u2019s GovTech agency, has created an open-source competitor to Google\u2019s increasingly popularNotebookLM.Dubbed \u201cOpen NotebookLM,\u201d Chua developed the entire system in just one afternoon using publicly available AI models.Open NotebookLM transforms PDF documents into personalized podcasts, mirroring a key feature of Google\u2019s product but with a crucial distinction: it\u2019s entirely open-source and free to use.The tool employs Meta\u2019sLlama 3.1 405Blanguage model, hosted onFireworks AI, alongsideMeloTTSfor voice synthesis. A user-friendly interface, built withGradioand hosted onHugging Face Spaces, makes the tool accessible to non-technical users.introducing Open NotebookLMturn any PDF ? into a personalized podcast ? in no time.the best part? it was all built in a single afternoon using open-source AI \u2728.? (1\/4)pic.twitter.com\/PLrr1Ol99D\u2014 gabriel (@gabrielchua_)September 29, 2024AI development in hours: The rise of quick replicationThe speed at which Chua developed and released Open NotebookLM highlights the increasing capabilities of open-source AI tools. It demonstrates that individual developers or small teams can now replicate and adapt complex AI applications, once the exclusive domain of tech giants, in a matter of hours.However, the rapid development of Open NotebookLM also raises questions about the quality and reliability of quickly assembled AI tools. While impressive in its scope, the open-source alternative may lack the rigorous testing and refinement that typically accompany commercial products. Users should approach such tools with caution, particularly when handling sensitive or confidential documents.The user interface of Open NotebookLM, an open-source alternative to Google\u2019s AI tool, allows users to convert PDFs into podcasts using publicly available AI models. The simple design belies the complex AI processes at work. (Image: Gabriel Chua\/Hugging Face)Google\u2019s edge: Why NotebookLM still holds the upper handGoogle\u2019sNotebookLMstill maintains several advantages over its open-source counterpart. It offers seamless integration with Google\u2019s ecosystem, including support for Google Slides and web URLs.The tech giant\u2019s vast computational resources and proprietary AI models also enable advanced features like fact-checking and study guide generation, which are currently beyond Open NotebookLM\u2019s capabilities.The emergence of Open NotebookLM represents a significant shift in the AI landscape. It exemplifies how the barrier to entry for creating sophisticated AI applications is lowering, allowing for more diverse and innovative solutions to emerge. This trend could lead to increased competition and potentially faster advancements in AI technology.Google\u2019s NotebookLM interface allows users to create AI-powered research notebooks by uploading documents and converting complex material into easily digestible formats. (Image: Google)The double-edged sword: Opportunities and risks in open-source AIThe proliferation of easily created AI tools also presents challenges. As more developers gain the ability to create powerful AI applications, concerns about data privacy, security, and the ethical use of AI become more pressing. The open-source nature of tools like Open NotebookLM allows for community scrutiny and improvement, but it also means that malicious actors could potentially adapt the technology for harmful purposes.For enterprise users and decision-makers, the rise of open-source AI tools like Open NotebookLM presents both opportunities and risks. On one hand, these tools offer cost-effective alternatives to proprietary solutions and the flexibility to customize applications to specific needs. On the other hand, they may lack the support, security guarantees, and ongoing development that come with commercial products.As the lines between proprietary and open-source AI continue to blur, we may be entering a new phase in software development. The power to create sophisticated AI applications is spreading beyond large tech companies, potentially fostering a more diverse AI ecosystem. However, this shift also underscores the need for robust frameworks to ensure the responsible development and use of AI technologies.Chua and the open-source community are capitalizing on their ability to rapidly replicate and iterate on proprietary AI technologies. As this trend continues, it may prompt tech giants to reconsider their approach to AI development, potentially leading to more collaboration between proprietary and open-source efforts in the future.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Automation","article_title":"Artisan raises $11.5M to deploy AI \u2019employees\u2019 for sales teams","article_url":"https:\/\/venturebeat.com\/business\/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams\/","article_date":"September 30, 2024 6:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreArtisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company\u2019s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails.Founded just last year, Artisan has already reached $1 million in annual recurring revenue, with over 120 companies using its platform. The seed round was led by Oliver Jung, with participation from Y Combinator, HubSpot Ventures, Day One Ventures, and others.\u201cWe create AI employees called artisans, and then we consolidate software tools together to create this unified software ecosystem where AI employees are managing and doing your work for you,\u201d said Jaspar Carmichael-Jack, Artisan\u2019s 23-year-old CEO and co-founder, in an interview with VentureBeat.How Artisan\u2019s AI assistant streamlines sales processesArtisan\u2019s approach aims to streamline the fragmented landscape of sales software. Rather than integrating multiple tools, the company offers a single platform that handles tasks ranging from lead generation to email outreach. At the center is Ava, an AI assistant that can operate autonomously to find prospects, research companies, and craft personalized messages.\u201cAva finds leads for people that match their ICP [ideal customer profile]. We have access to over 300 million different B2B lead profiles,\u201d Carmichael-Jack explained. \u201cAva enriches leads using data sources like CrunchBase, Apollo, Cognism\u2026writes emails to the leads and LinkedIn messages, and automates the entire process.\u201dAI\u2019s impact on sales jobs: A shift in rolesCarmichael-Jack acknowledged that AI will likely replace some roles, but argued this shift is ultimately beneficial: \u201cI think there\u2019s going to be a shift from the manual, repetitive, automatable roles to more human centered roles,\u201d he said. \u201cHumans will be shifted to more human activities.\u201dArtisan plans to expand beyond sales, with AI assistants for marketing and customer success in development. The involvement of HubSpot as an investor signals that even established software providers see potential in AI-first approaches.\u201cHubSpot backing us has been like a really meaningful thing to us, because it\u2019s showing that even the legacy software providers are ready for the next paradigm of software to come,\u201d Carmichael-Jack noted.The Future of AI in Business OperationsAs Artisan pushes forward with its AI sales assistants, the line between human and machine in the workplace continues to blur. The question now isn\u2019t whether AI will transform sales, but how quickly.For businesses, the future of sales may be less about closing deals and more about choosing the right digital companion. In this new landscape, the best salesperson might just be the one you never see.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Automation","article_title":"Here\u2019s how to try Meta\u2019s new Llama 3.2 with vision for free","article_url":"https:\/\/venturebeat.com\/ai\/heres-how-to-try-metas-new-llama-3-2-with-vision-for-free\/","article_date":"September 26, 2024 3:51 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreTogether AIhas made a splash in the AI world by offering developersfree accessto Meta\u2019s powerful new Llama 3.2 Vision model via Hugging Face.The model, known asLlama-3.2-11B-Vision-Instruct, allows users to upload images and interact with AI that can analyze and describe visual content.Try Llama 3.2 11B Vision for free in this@huggingfacespace!This model is free in the Together API for the next 3 months.https:\/\/t.co\/2oYwJK15KWpic.twitter.com\/JEh3LTr0M2\u2014 Together AI (@togethercompute)September 26, 2024For developers, this is a chance to experiment with cutting-edge multimodal AI without incurring thesignificant costsusually associated with models of this scale. All you need is an API key from Together AI, and you can get started today.This launch underscores Meta\u2019s ambitious vision for the future of artificial intelligence, which increasingly relies on models that can process both text and images\u2014a capability known as multimodal AI.With Llama 3.2, Meta is expanding the boundaries of what AI can do, while Together AI is playing a crucial role by making these advanced capabilities accessible to a broader developer community through afree, easy-to-use demo.Together AI\u2019s interface for accessing Meta\u2019s Llama 3.2 Vision model, showcasing the simplicity of using advanced AI technology with just an API key and adjustable parameters. (Credit: Hugging Face)Unleashing Vision: Meta\u2019s Llama 3.2 breaks new ground in AI accessibilityMeta\u2019s Llama models have been at the forefront of open-source AI development since thefirst versionwas unveiled in early 2023, challenging proprietary leaders like OpenAI\u2019sGPT models.Llama 3.2, launched atMeta\u2019s Connect 2024event this week, takes this even further by integrating vision capabilities, allowing the model to process and understand images in addition to text.This opens the door to a broader range of applications, from sophisticated image-based search engines to AI-powered UI design assistants.The launch of thefree Llama 3.2 Vision demoon Hugging Face makes these advanced capabilities more accessible than ever.Developers, researchers, and startups can now test the model\u2019s multimodal capabilities by simply uploading an image and interacting with the AI in real time.The demo,available here, is powered byTogether AI\u2019s API infrastructure, which has been optimized for speed and cost-efficiency.From code to reality: A step-by-step guide to harnessing Llama 3.2Trying the model is as simple as obtaining afree API keyfrom Together AI.Developers can sign up for an account on Together AI\u2019s platform, which includes$5 in free creditsto get started. Once the key is set up, users can input it into the Hugging Face interface and begin uploading images to chat with the model.The setup process takes mere minutes, and the demo provides an immediate look at how far AI has come in generating human-like responses to visual inputs.For example, users can upload a screenshot of a website or a photo of a product, and the model will generate detailed descriptions or answer questions about the image\u2019s content.For enterprises, this opens the door to faster prototyping and development of multimodal applications. Retailers could use Llama 3.2 to power visual search features, while media companies might leverage the model to automate image captioning for articles and archives.The bigger picture: Meta\u2019s vision for edge AILlama 3.2 is part of Meta\u2019s broader push into edge AI, where smaller, more efficient models can run on mobile and edge devices without relying on cloud infrastructure.While the11B Vision modelis now available for free testing, Meta has also introduced lightweight versions with as few as 1 billion parameters, designed specifically for on-device use.These models, which can run on mobile processors fromQualcommandMediaTek, promise to bring AI-powered capabilities to a much wider range of devices.In an era where data privacy is paramount, edge AI has the potential to offer more secure solutions by processing data locally on devices rather than in the cloud.This can be crucial for industries like healthcare and finance, where sensitive data must remain protected. Meta\u2019s focus on making these models modifiable and open-source also means that businesses can fine-tune them for specific tasks without sacrificing performance.Beyond the cloud: Meta\u2019s bold push into edge AI with Llama 3.2Meta\u2019scommitment to opennesswith the Llama models has been a bold counterpoint to the trend of closed, proprietary AI systems.With Llama 3.2, Meta is doubling down on the belief that open models can drive innovation faster by enabling a much larger community of developers to experiment and contribute.In a statement at the Connect 2024 event, Meta CEO Mark Zuckerberg noted that Llama 3.2 represents a \u201c10x growth\u201d in the model\u2019s capabilities since its previous version, and it\u2019s poised to lead the industry in both performance and accessibility.Together AI\u2019s role in this ecosystem is equally noteworthy. By offering free access to the Llama 3.2 Vision model, the company is positioning itself as a critical partner for developers and enterprises looking to integrate AI into their products.Together AI CEO Vipul Ved Prakash emphasized that their infrastructure is designed to make it easy for businesses of all sizes to deploy these models in production environments, whether in the cloud or on-prem.The future of AI: Open access and its implicationsWhile Llama 3.2 is available for free on Hugging Face, Meta and Together AI are clearly eyeing enterprise adoption.The free tier is just the beginning\u2014developers who want to scale their applications will likely need to move to paid plans as their usage increases. For now, however, the free demo offers a low-risk way to explore the cutting edge of AI, and for many, that\u2019s a game-changer.As the AI landscape continues to evolve, the line between open-source and proprietary models is becoming increasingly blurred.For businesses, the key takeaway is that open models like Llama 3.2 are no longer just research projects\u2014they\u2019re ready for real-world use. And with partners like Together AI making access easier than ever, the barrier to entry has never been lower.Want to try it yourself? Head over toTogether AI\u2019s Hugging Face demoto upload your first image and see what Llama 3.2 can do.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Enterprise Analytics","article_title":"OpenAI\u2019s DevDay 2024: 4 major updates that will make AI more accessible and affordable","article_url":"https:\/\/venturebeat.com\/ai\/openai-devday-2024-4-major-updates-that-will-make-ai-more-accessible-and-affordable\/","article_date":"October 1, 2024 10:15 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIn a marked contrast to last year\u2019ssplashy event, OpenAI held a more subduedDevDay conferenceon Tuesday, eschewing major product launches in favor of incremental improvements to its existing suite of AI tools and APIs.The company\u2019s focus this year was on empowering developers and showcasing community stories, signaling a shift in strategy as the AI landscape becomes increasingly competitive.The company unveiled four major innovations at the event: Vision Fine-Tuning, Realtime API, Model Distillation, and Prompt Caching. These new tools highlight OpenAI\u2019s strategic pivot towards empowering its developer ecosystem rather than competing directly in the end-user application space.Prompt caching: A boon for developer budgetsOne of the most significant announcements is the introduction ofPrompt Caching, a feature aimed at reducing costs and latency for developers.This system automatically applies a 50% discount on input tokens that the model has recently processed, potentially leading to substantial savings for applications that frequently reuse context.\u201cWe\u2019ve been pretty busy,\u201d said Olivier Godement, OpenAI\u2019s head of product for the platform, at a small press conference at the company\u2019s San Francisco headquarters kicking off the developer conference. \u201cJust two years ago, GPT-3 was winning. Now, we\u2019ve reduced [those] costs by almost 1000x. I was trying to come up with an example of technologies who reduced their costs by almost 1000x in two years\u2014and I cannot come up with an example.\u201dThis dramatic cost reduction presents a major opportunity for startups and enterprises to explore new applications, which were previously out of reach due to expense.A pricing table from OpenAI\u2019s DevDay 2024 reveals major cost reductions for AI model usage, with cached input tokens offering up to 50% savings compared to uncached tokens across various GPT models. The new o1 model showcases premium pricing, reflecting its advanced capabilities. (Credit: OpenAI)Vision fine-tuning: A new frontier in visual AIAnother major announcement is the introduction of vision fine-tuning forGPT-4o, OpenAI\u2019s latest large language model. This feature allows developers to customize the model\u2019s visual understanding capabilities using both images and text.The implications of this update are far-reaching, potentially impacting fields such as autonomous vehicles, medical imaging, and visual search functionality.Grab, a leading Southeast Asian food delivery and rideshare company, has already leveraged this technology to improve its mapping services, according to OpenAI.Using just 100 examples, Grab reportedly achieved a 20 percent improvement in lane count accuracy and a 13 percent boost in speed limit sign localization.This real-world application demonstrates the possibilities for vision fine-tuning to dramatically enhance AI-powered services across a wide range of industries using small batches of visual training data.Realtime API: Bridging the gap in conversational AIOpenAI also unveiled itsRealtime API, now in public beta. This new offering enables developers to create low-latency, multimodal experiences, particularly in speech-to-speech applications. This means that developers can start adding ChatGPT\u2019s voice controls to apps.To illustrate the API\u2019s potential, OpenAI demonstrated an updated version ofWanderlust, a travel planning app showcased atlast year\u2019s conference.With the Realtime API, users can speak directly to the app, engaging in a natural conversation to plan their trips. The system even allows for mid-sentence interruptions, mimicking human dialogue.While travel planning is just one example, the Realtime API opens up a wide range of possibilities for voice-enabled applications across various industries.From customer service to education and accessibility tools, developers now have a powerful new resource to create more intuitive and responsive AI-driven experiences.\u201cWhenever we design products, we essentially look at like both startups and enterprises,\u201d Godement explained. \u201cAnd so in the alpha, we have a bunch of enterprises using the APIs, the new models of the new products as well.\u201dThe Realtime API essentially streamlines the process of building voice assistants and other conversational AI tools, eliminating the need to stitch together multiple models for transcription, inference, and text-to-speech conversion.Early adopters likeHealthify, a nutrition and fitness coaching app, andSpeak, a language learning platform, have already integrated the Realtime API into their products.These implementations showcase the API\u2019s potential to create more natural and engaging user experiences in fields ranging from healthcare to education.The Realtime API\u2019s pricing structure, while not inexpensive at $0.06 per minute of audio input and $0.24 per minute of audio output, could still represent a significant value proposition for developers looking to create voice-based applications.Model distillation: A step toward more accessible AIPerhaps the most transformative announcement was the introduction of Model Distillation. This integrated workflow allows developers to use outputs from advanced models likeo1-previewandGPT-4oto improve the performance of more efficient models such asGPT-4o mini.The approach could enable smaller companies to harness capabilities similar to those of advanced models without incurring the same computational costs.It addresses a long-standing divide in the AI industry between cutting-edge, resource-intensive systems and their more accessible but less capable counterparts.Consider a small medical technology start-up developing an AI-powered diagnostic tool for rural clinics. Using Model Distillation, the company could train a compact model that captures much of the diagnostic prowess of larger models while running on standard laptops or tablets.This could bring sophisticated AI capabilities to resource-constrained environments, potentially improving healthcare outcomes in underserved areas.OpenAI\u2019s strategic shift: Building a sustainable AI ecosystemOpenAI\u2019s DevDay 2024 marks a strategic pivot for the company, prioritizing ecosystem development over headline-grabbing product launches.This approach, while less exciting for the general public, demonstrates a mature understanding of the AI industry\u2019s current challenges and opportunities.This year\u2019s subdued event contrasts sharply with the 2023 DevDay, which generatediPhone-like excitementwith the launch of the GPT Store and custom GPT creation tools.However, the AI landscape has evolved rapidly since then. Competitors have madesignificant strides, and concerns about data availability for training have intensified. OpenAI\u2019s focus on refining existing tools and empowering developers appears to be a calculated response to these shifts. By improving the efficiency and cost-effectiveness of their models, OpenAI aims to maintain its competitive edge while addressing concerns aboutresource intensityandenvironmental impact.As OpenAI transitions from a disruptor to a platform provider, its success will largely depend on its ability to foster a thriving developer ecosystem.By providing improved tools, reduced costs, and increased support, the company is laying the groundwork for long-term growth and stability in the AI sector.While the immediate impact may be less visible, this strategy could ultimately lead to more sustainable and widespread AI adoption across many industries.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Enterprise Analytics","article_title":"This open-source AI tool was built in a day and it\u2019s coming for Google\u2019s NotebookLM","article_url":"https:\/\/venturebeat.com\/ai\/this-open-source-ai-tool-was-built-in-a-day-and-its-coming-for-googles-notebooklm\/","article_date":"September 30, 2024 10:47 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreGabriel Chua, a data scientist at Singapore\u2019s GovTech agency, has created an open-source competitor to Google\u2019s increasingly popularNotebookLM.Dubbed \u201cOpen NotebookLM,\u201d Chua developed the entire system in just one afternoon using publicly available AI models.Open NotebookLM transforms PDF documents into personalized podcasts, mirroring a key feature of Google\u2019s product but with a crucial distinction: it\u2019s entirely open-source and free to use.The tool employs Meta\u2019sLlama 3.1 405Blanguage model, hosted onFireworks AI, alongsideMeloTTSfor voice synthesis. A user-friendly interface, built withGradioand hosted onHugging Face Spaces, makes the tool accessible to non-technical users.introducing Open NotebookLMturn any PDF ? into a personalized podcast ? in no time.the best part? it was all built in a single afternoon using open-source AI \u2728.? (1\/4)pic.twitter.com\/PLrr1Ol99D\u2014 gabriel (@gabrielchua_)September 29, 2024AI development in hours: The rise of quick replicationThe speed at which Chua developed and released Open NotebookLM highlights the increasing capabilities of open-source AI tools. It demonstrates that individual developers or small teams can now replicate and adapt complex AI applications, once the exclusive domain of tech giants, in a matter of hours.However, the rapid development of Open NotebookLM also raises questions about the quality and reliability of quickly assembled AI tools. While impressive in its scope, the open-source alternative may lack the rigorous testing and refinement that typically accompany commercial products. Users should approach such tools with caution, particularly when handling sensitive or confidential documents.The user interface of Open NotebookLM, an open-source alternative to Google\u2019s AI tool, allows users to convert PDFs into podcasts using publicly available AI models. The simple design belies the complex AI processes at work. (Image: Gabriel Chua\/Hugging Face)Google\u2019s edge: Why NotebookLM still holds the upper handGoogle\u2019sNotebookLMstill maintains several advantages over its open-source counterpart. It offers seamless integration with Google\u2019s ecosystem, including support for Google Slides and web URLs.The tech giant\u2019s vast computational resources and proprietary AI models also enable advanced features like fact-checking and study guide generation, which are currently beyond Open NotebookLM\u2019s capabilities.The emergence of Open NotebookLM represents a significant shift in the AI landscape. It exemplifies how the barrier to entry for creating sophisticated AI applications is lowering, allowing for more diverse and innovative solutions to emerge. This trend could lead to increased competition and potentially faster advancements in AI technology.Google\u2019s NotebookLM interface allows users to create AI-powered research notebooks by uploading documents and converting complex material into easily digestible formats. (Image: Google)The double-edged sword: Opportunities and risks in open-source AIThe proliferation of easily created AI tools also presents challenges. As more developers gain the ability to create powerful AI applications, concerns about data privacy, security, and the ethical use of AI become more pressing. The open-source nature of tools like Open NotebookLM allows for community scrutiny and improvement, but it also means that malicious actors could potentially adapt the technology for harmful purposes.For enterprise users and decision-makers, the rise of open-source AI tools like Open NotebookLM presents both opportunities and risks. On one hand, these tools offer cost-effective alternatives to proprietary solutions and the flexibility to customize applications to specific needs. On the other hand, they may lack the support, security guarantees, and ongoing development that come with commercial products.As the lines between proprietary and open-source AI continue to blur, we may be entering a new phase in software development. The power to create sophisticated AI applications is spreading beyond large tech companies, potentially fostering a more diverse AI ecosystem. However, this shift also underscores the need for robust frameworks to ensure the responsible development and use of AI technologies.Chua and the open-source community are capitalizing on their ability to rapidly replicate and iterate on proprietary AI technologies. As this trend continues, it may prompt tech giants to reconsider their approach to AI development, potentially leading to more collaboration between proprietary and open-source efforts in the future.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Enterprise Analytics","article_title":"Artisan raises $11.5M to deploy AI \u2019employees\u2019 for sales teams","article_url":"https:\/\/venturebeat.com\/business\/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams\/","article_date":"September 30, 2024 6:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreArtisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company\u2019s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails.Founded just last year, Artisan has already reached $1 million in annual recurring revenue, with over 120 companies using its platform. The seed round was led by Oliver Jung, with participation from Y Combinator, HubSpot Ventures, Day One Ventures, and others.\u201cWe create AI employees called artisans, and then we consolidate software tools together to create this unified software ecosystem where AI employees are managing and doing your work for you,\u201d said Jaspar Carmichael-Jack, Artisan\u2019s 23-year-old CEO and co-founder, in an interview with VentureBeat.How Artisan\u2019s AI assistant streamlines sales processesArtisan\u2019s approach aims to streamline the fragmented landscape of sales software. Rather than integrating multiple tools, the company offers a single platform that handles tasks ranging from lead generation to email outreach. At the center is Ava, an AI assistant that can operate autonomously to find prospects, research companies, and craft personalized messages.\u201cAva finds leads for people that match their ICP [ideal customer profile]. We have access to over 300 million different B2B lead profiles,\u201d Carmichael-Jack explained. \u201cAva enriches leads using data sources like CrunchBase, Apollo, Cognism\u2026writes emails to the leads and LinkedIn messages, and automates the entire process.\u201dAI\u2019s impact on sales jobs: A shift in rolesCarmichael-Jack acknowledged that AI will likely replace some roles, but argued this shift is ultimately beneficial: \u201cI think there\u2019s going to be a shift from the manual, repetitive, automatable roles to more human centered roles,\u201d he said. \u201cHumans will be shifted to more human activities.\u201dArtisan plans to expand beyond sales, with AI assistants for marketing and customer success in development. The involvement of HubSpot as an investor signals that even established software providers see potential in AI-first approaches.\u201cHubSpot backing us has been like a really meaningful thing to us, because it\u2019s showing that even the legacy software providers are ready for the next paradigm of software to come,\u201d Carmichael-Jack noted.The Future of AI in Business OperationsAs Artisan pushes forward with its AI sales assistants, the line between human and machine in the workplace continues to blur. The question now isn\u2019t whether AI will transform sales, but how quickly.For businesses, the future of sales may be less about closing deals and more about choosing the right digital companion. In this new landscape, the best salesperson might just be the one you never see.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Enterprise Analytics","article_title":"Here\u2019s how to try Meta\u2019s new Llama 3.2 with vision for free","article_url":"https:\/\/venturebeat.com\/ai\/heres-how-to-try-metas-new-llama-3-2-with-vision-for-free\/","article_date":"September 26, 2024 3:51 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreTogether AIhas made a splash in the AI world by offering developersfree accessto Meta\u2019s powerful new Llama 3.2 Vision model via Hugging Face.The model, known asLlama-3.2-11B-Vision-Instruct, allows users to upload images and interact with AI that can analyze and describe visual content.Try Llama 3.2 11B Vision for free in this@huggingfacespace!This model is free in the Together API for the next 3 months.https:\/\/t.co\/2oYwJK15KWpic.twitter.com\/JEh3LTr0M2\u2014 Together AI (@togethercompute)September 26, 2024For developers, this is a chance to experiment with cutting-edge multimodal AI without incurring thesignificant costsusually associated with models of this scale. All you need is an API key from Together AI, and you can get started today.This launch underscores Meta\u2019s ambitious vision for the future of artificial intelligence, which increasingly relies on models that can process both text and images\u2014a capability known as multimodal AI.With Llama 3.2, Meta is expanding the boundaries of what AI can do, while Together AI is playing a crucial role by making these advanced capabilities accessible to a broader developer community through afree, easy-to-use demo.Together AI\u2019s interface for accessing Meta\u2019s Llama 3.2 Vision model, showcasing the simplicity of using advanced AI technology with just an API key and adjustable parameters. (Credit: Hugging Face)Unleashing Vision: Meta\u2019s Llama 3.2 breaks new ground in AI accessibilityMeta\u2019s Llama models have been at the forefront of open-source AI development since thefirst versionwas unveiled in early 2023, challenging proprietary leaders like OpenAI\u2019sGPT models.Llama 3.2, launched atMeta\u2019s Connect 2024event this week, takes this even further by integrating vision capabilities, allowing the model to process and understand images in addition to text.This opens the door to a broader range of applications, from sophisticated image-based search engines to AI-powered UI design assistants.The launch of thefree Llama 3.2 Vision demoon Hugging Face makes these advanced capabilities more accessible than ever.Developers, researchers, and startups can now test the model\u2019s multimodal capabilities by simply uploading an image and interacting with the AI in real time.The demo,available here, is powered byTogether AI\u2019s API infrastructure, which has been optimized for speed and cost-efficiency.From code to reality: A step-by-step guide to harnessing Llama 3.2Trying the model is as simple as obtaining afree API keyfrom Together AI.Developers can sign up for an account on Together AI\u2019s platform, which includes$5 in free creditsto get started. Once the key is set up, users can input it into the Hugging Face interface and begin uploading images to chat with the model.The setup process takes mere minutes, and the demo provides an immediate look at how far AI has come in generating human-like responses to visual inputs.For example, users can upload a screenshot of a website or a photo of a product, and the model will generate detailed descriptions or answer questions about the image\u2019s content.For enterprises, this opens the door to faster prototyping and development of multimodal applications. Retailers could use Llama 3.2 to power visual search features, while media companies might leverage the model to automate image captioning for articles and archives.The bigger picture: Meta\u2019s vision for edge AILlama 3.2 is part of Meta\u2019s broader push into edge AI, where smaller, more efficient models can run on mobile and edge devices without relying on cloud infrastructure.While the11B Vision modelis now available for free testing, Meta has also introduced lightweight versions with as few as 1 billion parameters, designed specifically for on-device use.These models, which can run on mobile processors fromQualcommandMediaTek, promise to bring AI-powered capabilities to a much wider range of devices.In an era where data privacy is paramount, edge AI has the potential to offer more secure solutions by processing data locally on devices rather than in the cloud.This can be crucial for industries like healthcare and finance, where sensitive data must remain protected. Meta\u2019s focus on making these models modifiable and open-source also means that businesses can fine-tune them for specific tasks without sacrificing performance.Beyond the cloud: Meta\u2019s bold push into edge AI with Llama 3.2Meta\u2019scommitment to opennesswith the Llama models has been a bold counterpoint to the trend of closed, proprietary AI systems.With Llama 3.2, Meta is doubling down on the belief that open models can drive innovation faster by enabling a much larger community of developers to experiment and contribute.In a statement at the Connect 2024 event, Meta CEO Mark Zuckerberg noted that Llama 3.2 represents a \u201c10x growth\u201d in the model\u2019s capabilities since its previous version, and it\u2019s poised to lead the industry in both performance and accessibility.Together AI\u2019s role in this ecosystem is equally noteworthy. By offering free access to the Llama 3.2 Vision model, the company is positioning itself as a critical partner for developers and enterprises looking to integrate AI into their products.Together AI CEO Vipul Ved Prakash emphasized that their infrastructure is designed to make it easy for businesses of all sizes to deploy these models in production environments, whether in the cloud or on-prem.The future of AI: Open access and its implicationsWhile Llama 3.2 is available for free on Hugging Face, Meta and Together AI are clearly eyeing enterprise adoption.The free tier is just the beginning\u2014developers who want to scale their applications will likely need to move to paid plans as their usage increases. For now, however, the free demo offers a low-risk way to explore the cutting edge of AI, and for many, that\u2019s a game-changer.As the AI landscape continues to evolve, the line between open-source and proprietary models is becoming increasingly blurred.For businesses, the key takeaway is that open models like Llama 3.2 are no longer just research projects\u2014they\u2019re ready for real-world use. And with partners like Together AI making access easier than ever, the barrier to entry has never been lower.Want to try it yourself? Head over toTogether AI\u2019s Hugging Face demoto upload your first image and see what Llama 3.2 can do.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Enterprise Analytics","article_title":"Airtable just launched an AI platform that could change how you work","article_url":"https:\/\/venturebeat.com\/ai\/airtable-just-launched-an-ai-platform-that-could-change-how-you-work\/","article_date":"September 26, 2024 7:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreAs companies struggle to realize returns on massive investments in artificial intelligence,Airtableis betting it can help enterprises finally deploy AI into critical business workflows at scale.The San Francisco-based company announced on Thursday new capabilities that transform its collaborative app-building platform into what it calls a \u201ctrue enterprise-grade AI platform.\u201dThe additions include App Library, which allows companies to create standardized AI-powered applications that can be customized across an organization, and HyperDB, which enables integration of massive datasets of over 100 million records.Airtable\u2019s new App Library feature, showcasing pre-built applications like \u201cLaunch tracker\u201d and \u201cOKR tracker.\u201d The interface allows users to search and browse apps by category, illustrating Airtable\u2019s push to make AI-powered business tools more accessible to enterprise users. (Credit: Airtable)AI deployment: Moving beyond chatbots to workflow automation\u201cThere\u2019s been way too much emphasis on just the hard tech, and not nearly enough emphasis on the ergonomics and how to actually utilize LLMs today,\u201d said Howie Liu, Airtable\u2019s co-founder and CEO, in an interview with VentureBeat. He argued that while there\u2019s been fascination with ever-larger AI models, the focus needs to shift to deploying AI in real business use cases.The move positions Airtable to capitalize on surging enterprise interest in generative AI. Goldman Sachsforecasts $1 trillion in AI investmentsfrom tech firms, corporations and utilities in coming years. But many early AI initiatives have failed to deliver tangible business impact.\u201cWe\u2019re at a tipping point in the AI era, yet most enterprise AI adoption is still just scratching the surface of the powerful potential that could transform digital operations,\u201d the company said in its announcement.Airtable\u2019s new enterprise AI platform aims to integrate AI capabilities across business operations. The platform features AI-embedded apps, tools for citizen developers, and centralized governance, highlighting Airtable\u2019s strategy to make AI deployment more accessible and manageable for large organizations. (Credit: Airtable)Balancing standardization and customization: The Enterprise AI challengeAirtable claims its platform is already used by major media, retail and financial services companies to power critical operations. One unnamed \u201cleading streaming company\u201d reportedly saved 280 hours per week on content genre classification using custom AI solutions built on Airtable.The new enterprise offerings aim to strike a balance between standardization and customization \u2014 a common challenge for global organizations. App Library allows central teams to create standardized applications with embedded AI that can then be adapted by different business units.\u201cWe give them a Lego kit, and we make the technology really accessible,\u201d Liu said, emphasizing Airtable\u2019s focus on empowering business users rather than just technical teams.HyperDB, meanwhile, is designed to make massive datasets from systems like Snowflake and Salesforce more accessible for use in departmental applications while maintaining centralized governance.Airtable\u2019s new AI-powered platform orchestrates complex media production lifecycles, from strategic planning to performance measurement. The system integrates data from millions of titles and over 100 million users to streamline processes across global markets, showcasing the potential scale of AI application in enterprise media operations. (Credit: Airtable)Scaling AI: From chat interfaces to parallel processing of thousands of tasksAirtable faces competition from established enterprise software vendors racing to embed AI capabilities, as well as a crop of AI-native startups. But Liu believes Airtable\u2019s approach of enabling parallel deployment of AI across thousands of records or workflow steps is differentiated.\u201cIt would be like, could you hire overnight and just for five minutes worth of work, 10,000 decently smart interns to go work on a task,\u201d he said. \u201cThat is a really powerful kind of form factor.\u201dThe moves come as Airtable, valued at $11 billion in late 2021, navigates a more challenging funding environment for tech startups. The companylaid off about 250 employeeslast year and is reportedly preparing for a potential IPO.Airtable\u2019s enterprise push represents a significant pivot from its roots as a user-friendly collaborative spreadsheet tool. While the company has successfully built a large user base with its grassroots adoption strategy, competing in the enterprise market presents new challenges. Airtable will need to prove it can handle the complex security, compliance, and integration requirements of large organizations.This strategic shift positions Airtable in direct competition with tech giants likeMicrosoft,Salesforce, andServiceNow, all of which are rapidly integrating AI into their offerings. Airtable\u2019s success will likely depend on whether its approach\u2014empowering business users to create AI-enhanced applications\u2014can deliver tangible productivity gains more efficiently and cost-effectively than solutions from established vendors.As enterprises grapple with how to extract value from their AI investments, Airtable\u2019s platform could find a receptive audience. However, the company will need to clearly articulate its differentiation and ROI proposition to stand out in an increasingly crowded market for enterprise AI solutions.In the end, Airtable\u2019s ambitious leap from organizing data to orchestrating AI may just prove that in the world of enterprise software, the best way to think outside the box is to rebuild it entirely.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Data Decision Makers","article_title":"Why prompt engineering is one of the most valuable skills today","article_url":"https:\/\/venturebeat.com\/ai\/why-prompt-engineering-is-one-of-the-most-valuable-skills-today\/","article_date":"September 22, 2024 12:15 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIn a world that is rapidly embracinglarge language models(LLMs), prompt engineering has emerged as a new skill to unlocking their full potential. Think of it as the language to speak with these intelligent AI systems, enabling us to tap into their vast capabilities and reshape how we create, work, solve problems and do much more. It can allow anyone \u2014 including your grandma \u2014 to program a complex multi-billion parameterAI system in the cloud.LLMs are fundamentally built ondeep learning algorithmsand architectures. They are trained on massive datasets of text. Like a human who has devoured countless books, LLMs learn patterns, grammar, relationships and reasoning abilities from data. Internal settings can be tuned to change how the model processes information and adjusted to improve accuracy. When given a prompt at the inferencing stage, the LLMs use their learned knowledge and parameters to generate the most probable and contextually relevant output. It is because of these prompts that the LLMs can generate human-quality text, hold conversations, translate languages, write different kinds of creative content and answer questions in an informative way.Many free (open source) LLMs and paid (closed source) hosted LLM services are available today. LLMs are transforming every industry as well as every aspect of our lives. Here\u2019s how:Customer service: Powerful AI chatbots provide instant support and answer customer queries.Education: Personalized learning experiences and AI tutors are available.Healthcare: LLMs are being used to analyze medical issues, accelerate drug discovery and personalize treatment plans.Marketing and content creation: LLMs can generate engaging marketing copy, website content and scripts for videos.Software development: LLMs are assisting developers with code generation, debugging and documentation.Important prompt types and techniquesPrompts act as a guiding light for LLMs. A well-crafted prompt can significantly impact the quality and relevance of the output of LLMs. Imagine asking a personal assistant to \u201cmake a reservation for dinner.\u201d Depending on how much information you provide, such as preferred cuisine or time, you will get a more accurate result.Prompt engineeringis the art and science of crafting prompts to elicit desired outputs from AI systems. It involves designing and refining prompts to generate accurate, relevant and creative outputs that align with the user\u2019s intent.Let us delve deeper by looking at prompt engineering techniques that can help a user guide LLMs toward desired outcomes.From practice, prompts could be broadly classified as falling into one of the following categories:Direct prompts: These are small direct instructions, such as \u201cTranslate \u2018hello\u2019 into Spanish.\u201dContextual prompts: A bit more context is added to small direct instructions. Such as, \u201cI am writing a blog post about the benefits of AI. Write a catchy title.\u201dInstruction-based prompts: These are elaborate instructions with specific details of what to do and what not to do. For instance, \u201cWrite a short story about a talking cat. The cat should be grumpy and sarcastic.\u201dExamples-based prompts: Prompters might say, \u201cHere\u2019s an example of a haiku: An old silent pond \/ A frog jumps into the pond\u2014 \/ Splash! Silence again. Now, write your own haiku.\u201dThe following are important techniques that have been proven to be very effective inprompt engineering:Iterative refinement: This involves continuously refining prompts based on the AI\u2019s responses. It can lead to better results.Example: You might start with \u201cWrite a poem about a sunset.\u201d After seeing the output, refine it to \u201cWrite a melancholic poem about a sunset at the beach.\u201dChain of thought prompting: Encouraging step-by-step reasoning can help solve complex problems.Example: Instead of just a complex prompt like \u201cA farmer has 14 tractors, eight cows and 10 chickens. If he sells half his birds and buys 3 more cows, how many animals would give him milk?\u201d, adding \u201cThink step by step\u201d or \u201cExplain your reasoning\u201d is likely to give much better results and even clearly point out any intermediate errors that the model could have made.Role-playing: This means assigning a role or persona to the AI before handing it the task.Example: \u201cYou are a museum guide. Explain the paintingVista from a Grottoby David Teniers the Younger.\u201dMulti-turn prompting: This involves breaking down a complex task into a series of prompts. This technique involves using a series of prompts to guide the AI to the required answer.Example: \u201cCreate a detailed outline,\u201d followed by \u201cUse the outline to expand each point into a paragraph,\u201d followed by \u201cThe 2nd paragraph is missing X. Rewrite it to focus on\u2026\u201d and then finally completing the piece.Challenges and opportunities in prompt engineeringThere are some challenges and opportunities inprompt engineering. Although they have improved exponentially, LLMs may still struggle with abstract concepts, humor, complex reasoning and other tasks, which often requires carefully crafted prompts. AI models also can reflect biases present in their training data.Prompt engineers need to understand this and address and mitigate potential biases in their final solutions. Additionally, different models may naturally interpret and respond to prompts in different ways, which poses challenges for generalization across models. Most LLM creators usually have good documentation along with prompt templates and other guidelines that work well for that model. It is always useful to familiarize yourself to efficiently use models. Finally, although inferencing speeds are continuously improving, effective prompting also presents an opportunity to program LLMs precisely at inference time to save compute and energy resources.As AI becomes increasingly intertwined with our lives, prompt engineering is playing a crucial role in shaping how we interact with and benefit from its power. When done right, it holds immense potential to unleash possibilities that we have not imagined yet.Deven Panchal is withAT&T Labs.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"},{"topic_title":"Data Decision Makers","article_title":"\u2018Harvest now, decrypt later\u2019: Why hackers are waiting for quantum computing","article_url":"https:\/\/venturebeat.com\/security\/harvest-now-decrypt-later-why-hackers-are-waiting-for-quantum-computing\/","article_date":"September 21, 2024 12:05 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreHackers are waiting for the momentquantum computingbreaks cryptography and enables the mass decryption of years of stolen information. In preparation, they are harvesting even more encrypted data than usual. Here is what businesses can do in response.Why are hackers harvesting encrypted data?Most modern organizations encrypt multiple critical aspects of their operations. In fact, abouteight in 10 businessesextensively or partially use enterprise-level encryption for databases, archives, internal networks and internet communications. After all, it is acybersecurity best practice.Alarmingly, cybersecurity experts are growing increasingly concerned that cybercriminals are stealing encrypted data and waiting for the right time to strike. Their worries are not unfounded \u2014 more than70% of ransomware attacksnow exfiltrate information before encryption.The \u201charvest now, decrypt later\u201d phenomenon in cyberattacks \u2014 where attackers steal encryptedinformationin the hopes they will eventually be able to decrypt it \u2014 is becoming common. As quantum computing technology develops, it will only grow more prevalent.How \u2018harvest now, decrypt later\u2019 worksQuantum computers make the \u201charvest now, decrypt later\u201d phenomenon possible. In the past, encryption was enough to deter cybercriminals \u2014 or at least make their efforts pointless. Unfortunately, that is no longer the case.Whereas classical computers operate using binary digits \u2014 bits \u2014 that can either be a one or a zero, their quantum counterparts use quantum bits called qubits. Qubits can exist in two states simultaneously, thanks to superposition.Since qubits may be a one and a zero, quantum computers\u2019 processing speeds far outpace the competition. Cybersecurity experts are worried they will make modern ciphers \u2014 meaning encryption algorithms \u2014 useless, which has inspired exfiltration-driven cyberattacks.Encryption turns data, also known as plaintext, into a string of random, undecipherable code called ciphertext. Ciphers do this using complex mathematical formulas that are technically impossible to decode without a decryption key. However, quantum computing changes things.While a classical computer wouldtake 300 trillion yearsor more to decrypt a 2,048-bit Rivest-Shamir-Adleman encryption, a quantum one could crack it in seconds, thanks to qubits. The catch is that this technology isn\u2019t widely available \u2014 only places like research institutions and government labs can afford it.That does not deter cybercriminals, as quantum computing technology could become accessible within a decade. In preparation, they use cyberattacks to steal encrypted data and plan to decrypt it later.What types of data are hackers harvesting?Hackers usually steal personally identifiable information like names, addresses, job titles and social security numbers because they enable identity theft. Account data \u2014 like company credit card numbers or bank account credentials \u2014 are also highly sought-after.Withquantum computing, hackers can access anything encrypted \u2014 data storage systems are no longer their primary focus. They can eavesdrop on the connection between a web browser and a server, read cross-program communication or intercept information in transit.Human resources, IT and accounting departments are still high risks for the average business. However, they must also worry about their infrastructure, vendors and communication protocols. After all, both client and server-side encryption will soon be fair game.The consequences of qubits cracking encryptionCompanies may not even realize they have been affected by a data breach until the attackers use quantum computing to decrypt the stolen information. It may be business as usual until a sudden surge in account takeovers, identity theft, cyberattacks and phishing attempts.Legal issues and regulatory fines would likely follow. Considering the average data breachrose from $4.35 millionin 2022 to $4.45 million in 2023 \u2014 a 2.3% year-over-year increase \u2014 the financial losses could be devastating.In the wake of quantum computing, businesses can no longer rely on ciphers to communicate securely, share files, store data or use the cloud. Their databases, archives, digital signatures, internet communications, hard drives, e-mail and internal networks will soon be vulnerable. Unless they find an alternative, they may have to revert to paper-based systems.Why prepare if quantum isn\u2019t here yet?While the potential for broken cryptography is alarming, decision-makers should not panic. The average hacker will not be able to get a quantum computer for years \u2014 maybe even decades \u2014 because they are incredibly costly, resource-intensive, sensitive and prone to errors if they are not kept in ideal conditions.To clarify, these sensitive machines must stay just above absolute zero (459 degrees Fahrenheitto be exact) because thermal noise can interfere with their operations.However, quantum computing technology is advancing daily. Researchers are trying to make these computers smaller, easier to use and more reliable. Soon, they may become accessible enough that the average person can own one.Already, a startup based in China recently unveiled the world\u2019s first consumer-grade portable quantum computers. The Triangulum \u2014 the most expensive model \u2014 offersthe power of three qubitsfor roughly $58,000. The two cheaper two-qubit versions retail for less than $10,000.While these machines pale in comparison to the powerhouse computers found in research institutions and government-funded labs, they prove that the world is not far away from mass-market quantum computing technology. In other words, decision-makers must act now instead of waiting until it is too late.Besides, the average hacker is not the one companies should worry about \u2014 well-funded threat groups pose a much larger threat. A world where a nation-state or business competitor can pay for quantum computing as a service to steal intellectual property, financial data or trade secrets may soon be a reality.What can enterprises do to protect themselves?There are a few steps business leaders should take in preparation for quantum computing cracking cryptography.1. Adopt post-quantum ciphersThe Cybersecurity and Infrastructure Security Agency (CISA) and the National Institute of Standards and Technology (NIST) soon plan to releasepost-quantum cryptographic standards. The agencies are leveraging the latest techniques to make ciphers quantum computers cannot crack. Firms would be wise to adopt them upon release.2. Enhance breach detectionIndicators of compromise \u2014 signs that show a network or system intrusion occurred \u2014 can help security professionals react to data breaches swiftly, potentially making data useless to the attackers. For example, they can immediately change all employees\u2019 passwords if they notice hackers have stolen account credentials.3. Use a quantum-safe VPNA quantum-safe virtual private network (VPN) protects data in transit, preventing exfiltration and eavesdropping. One expert claims consumers should expect them soon, statingthey are in the testing phaseas of 2024. Companies would be wise to adopt solutions like these.4. Move sensitive dataDecision-makers should ask themselves whether the information bad actors steal will still be relevant when it is decrypted. They should also consider the worst-case scenario to understand the risk level. From there, they can decide whether or not to move sensitive data.One option is to transfer the data to a heavily guarded or constantly monitored paper-based filing system, preventing cyberattacks entirely. The more feasible solution is to store it on a local network not connected to the public internet, segmenting it with security and authorization controls.Decision-makers should begin preparing nowAlthough quantum-based cryptography cracking is still years \u2014 maybe decades \u2014 away, it will have disastrous effects once it arrives. Business leaders should develop a post-quantum plan now to ensure they are not caught by surprise.Zac Amos is features editor atReHack.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"},{"topic_title":"Data Decision Makers","article_title":"Have we reached peak human?","article_url":"https:\/\/venturebeat.com\/ai\/have-we-reached-peak-human\/","article_date":"September 18, 2024 10:05 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreTwo weeks ago, OpenAI\u2019s former chief scientist Ilya Sutskeverraised $1 billionto back his newly formed company, Safe Superintelligence (SSI). The startup aims to safely build AI systems that exceed human cognitive capabilities. Just a few months before that, Elon Musk\u2019s startup xAI raised $6 billion to pursue superintelligence, a goal Musk predicts will beachieved within five or six years. These are staggering rounds of funding for newly formed companies, and it only adds to the many billions already poured into OpenAI, Anthropic and other firms racing to build superintelligence.As a longtime researcher in this field, I agree with Musk that superintelligence will be achieved within years, not decades, but I am skeptical that it can be achieved safely.\u00a0Instead, I believe we must view this milestone as an\u201cevolutionary pressure point\u201dfor humanity \u2014 one in which our fitness as a species will be challenged by superior intelligences with interests that will eventually conflict with our own.I often compare this milestoneto the arrival of an advanced alien speciesfrom another planet and point out the \u201cArrival Mind Paradox\u201d \u2014 the fact that we would fear a superior alien intelligence far more than we fear the superior intelligences we\u2019re currently building here on earth. This is because most people wrongly believe we are crafting AI systems to \u201cbe human.\u201d This is not true. We are building AI systems to be very good at pretending to be human, and to know humans inside and out. But the way their brains work is very different from ours \u2014 as different as any alien brain that might show up from afar.And yet, we continue to push for superintelligence. In fact, 2024 may go down as the year we reach \u201cPeak Human.\u201d By this I mean, the moment in time when AI systems can cognitively outperform more than half of human adults.\u00a0 After we pass that milestone, we will steadily lose our cognitive edge until AI systems can outthink all individual humans \u2014 even the most brilliant among us.AI beats one-third of humans on reasoning tasksUntil recently, the average human could easily outperform even the most powerful AI systems when it comes to basic reasoning tasks. There are many ways to measure reasoning, none-of-which are considered the gold standard, but the best known is the classic IQ test. Journalist Maxim Lott has been testing all major large language models (LLMs) on a standardized Mensa IQ test.\u00a0Last week, for the very first time, an AI model significantly exceeded the median human IQ score of 100. The model that crossed the peak of the bell curve was OpenAI\u2019s new \u201co1\u201d system\u00a0\u2014 itreportedly scored a 120 IQ.\u00a0So, does this mean AI has exceeded the reasoning ability of most humans?Not so fast. It is not quite valid to administer standard IQ tests to AI systems because the data they trained on likely included the tests (and answers), which is fundamentally unfair.\u00a0To address this, Lott had a custom IQ test created that does not appear anywhere online and therefore is not in the training data. He gave that \u201coffline test\u201d to OpenAI\u2019s o1 model and itscored an IQ of 95.This is still an extremely impressive result. That score beats 37% of adults on the reasoning tasks. It also represents a rapid increase, as OpenAI\u2019s previous model GPT-4 (which was just released last year) was outperformed by 98% of adults on the same test.\u00a0At this rate of progress, it is very likely that an AI model will be able to beat 50% of adult humans on standard IQ tests this year.Does this mean we will reach peak human in 2024?Yes and no.First, I predict yes, at least one foundational AI model will be released in 2024 that can outthink more than 50% of adult humans on pure reasoning tasks. From this perspective, we will exceed my definition for peak human and will be on a downward path towards the rapidly approaching day when an AI is released that can outperform all individual humans, period.Second, I need to point out that we humans have another trick up our sleeves. It\u2019s called collective intelligence, and it relates to the fact that human groups can be smarter than individuals. And we humans have a lot of individuals \u2014 more than 8 billion at the moment.I bring this up because my personal focus as an AI researcher over the last decade has been the use of AI to connect groups of humans together into real-time systems that amplify our collective intelligence to superhuman levels. I call this goalcollective superintelligence, and I believe it is a viable pathway for keeping humanity cognitively competitive even after AI systems can outperform the reasoning ability of every individual among us.\u00a0I like to think of this as \u201cpeak humanity,\u201d and I am confident we can push it to intelligence levels that will surprise us all.Back in 2019, my research team atUnanimous AIconducted our first experiments in which we enabled groups of people to take IQ tests together by forming real-time systems mediated by Ai algorithms. This first-generation technology called \u201cSwarm AI\u201d enabled small groups of 6 to 10 randomly selected participants (who averaged 100 IQ) to amplify their collective performance to a collective IQ score of 114 when deliberating as an AI facilitated system (Willcox and Rosenberg). This was a good start, but not within striking distance of Collective Superintelligence.More recently, we unveiled a new technology calledconversational swarm intelligence(CSI). It enables large groups (up to 400 people) to hold real-time conversational deliberations that amplify the group\u2019s collective intelligence. In collaboration with Carnegie Mellon University, we conducted a 2024 study in which groups of 35 randomly selected people were tasked with taking IQ test questions together in real-time as AI-facilitated \u201cconversational swarms.\u201d As published this year, thegroups averaged IQ scores of 128(the 97th percentile). This is a strong result, but I believe we are just scratching the surface of how smart humans can become when we use AI to think together in far larger groups.I am passionate about pursuingcollective superintelligencebecause it has the potential to greatly amplify humanity\u2019s cognitive abilities, and unlike a digital superintelligence it is inherently instilled with human values, morals, sensibilities and interests. Of course, this begs the question \u2014 how long can we stay ahead of the purely digital AI systems? That depends on whether AI continues to advance at an accelerating pace or if we hit a plateau. Either way, amplifying our collective intelligence might help us maintain our edge long enough to figure out how to protect ourselves from being outmatched.When I raise the issue of peak human, many people point out that human intelligence is far more than just the logic and reasoning measured by IQ tests. I fully agree, but when we look at the most \u201chuman\u201d of all qualities \u2014 creativity and artistry \u2014 we see evidence that AI systems are catching up with us just as quickly. It was only a few years ago that virtually all artwork was crafted by humans.\u00a0Arecent analysisestimates that generative AI is producing 15 billion images per year and that rate is accelerating.Even more surprising, astudy published just last weekshowed that AI chatbots can outperform humans on creativity tests. To quote the paper, \u201cthe results suggest that AI has reached at least the same level, or even surpassed, the average human\u2019s ability to generate ideas in the most typical test of creative thinking (AUT).\u201dI\u2019m not sure I fully believe this result, but it\u2019s just a matter of time before it holds true.Whether we like it or not, our evolutionary position as the smartest and most creative brains on planet earth is likely to be challenged in the near future. We can debate whether this will be a net positive or a net negative for humanity, but either way, we need to be doing more toprotect ourselvesfrom being outmatched.Louis Rosenberg, is a computer scientist and entrepreneur in the fields of AI\u00a0 and mixed reality. His new book,Our Next Reality, explores the impact of AI and spatial computing on humanity.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"},{"topic_title":"Data Decision Makers","article_title":"Why data science alone won\u2019t make your product successful","article_url":"https:\/\/venturebeat.com\/programming-development\/why-data-science-alone-wont-make-your-product-successful\/","article_date":"September 15, 2024 12:15 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThe last decade has seen the divide between tech and commercial teams thin almost to the point of nonexistence. And I, for one, am in favor of it. Not every tech team works in a tech company, and blurring the lines between the commercial and technological means that we can build and ship product safe in the knowledge that it will be well received, widely adopted (not always a given), and contribute meaningfullyto the bottom line. Name a better way to motivate a high-performance tech team, and I\u2019ll listen.It\u2019s a change that was accelerated \u2014 if not caused by \u2014 data tech. We\u2019ve spent decades working through big data, business intelligence, andAI hype cycles. Each introduced new skills, problems and collaborators for the CTO and their team to get to grips with, and each moved us just a little further from the rest of the organization; no one else can do what we do, but everyone needs it done.Technical teams are not inherently commercial, and as these roles expanded to include building and delivering tools to support various teams across the organization, this gap became increasingly apparent. We\u2019ve all seen the stats about the number of data science projects, in particular, that never get productionized \u2014 and it\u2019s little wonder why. Tools built for commercial teams by people who don\u2019t fully understand their needs, goals or processes will always be of limited use.This waste of technology dollars was immensely justifiable in theearly days of AI\u2014 investors wanted to see investment in the technology, not outcomes \u2014 but the tech has matured, and the market has shifted. Now, we have to show actual returns on our technology investments, which means delivering innovations that have a measurable impact on the bottom line.Transitioning from support to a core functionThe growing pains of the data tech hype cycles have delivered two incredible boons to the modern CTO and their team (over and above the introduction of tools like machine learning (ML) and AI). The first is a mature, centralized data architecture that removes historical data silos across the business and gives us a clear picture \u2014 for the first time \u2014 of exactly what\u2019s happening on a commercial level and how one team\u2019s actions affect another. The second is the move from a support function to a core function.This second one is important. As a core function, tech workers now have a seat at the table alongside their commercial colleagues, and these relationships help to foster a greater understanding of processes outside of the technology team, including what these colleagues need to achieve and how that impacts the business.This, in turn, has given rise to new ways of working. For the first time,technical individualsare no longer squirreled away, fielding unconnected requests from across the business to pull this stat or crunch this data. Instead, they can finally see the impact they have on the business in monetary terms. It\u2019s a rewarding viewpoint and one that has given rise to a new way of working; an approach that maximizes this contribution and aims to generate as much value as quickly as possible.Introducing lean valueI hesitate to add another project management methodology to the lexicon, but lean-value warrants some consideration, particularly in an environment where return on tech investment is so heavily scrutinized. The guiding principle is \u2018ruthless prioritization to maximize value.\u2019 For my team, that means prioritizing research with the highest likelihood of either delivering value or progressing organizational goals. It also means deprioritizing non-critical tasks.We focus on attaining a minimum viable product (MVP), applying lean principles across engineering and architecture, and \u2014 here\u2019s the tricky bit \u2014 actively avoiding a perfect build in the initial pass. Each week, we review non-functional requirements and reprioritize them based on our objectives. This approach reduces unnecessary code and prevents teams from getting sidetracked or losing sight of the bigger picture. It\u2019s a way of working we\u2019ve also found to be inclusive of neurodiverse individuals within the team, since there\u2019s a very clear framework to remain anchored to.The result has been accelerated product rollouts. We have a dispersed, international team and operate a modularmicroservice architecture, which lends itself well to the lean-value approach. Weekly reviews keep us focused and prevent unnecessary development \u2014 itself a time saver \u2014 while allowing us to make changes incrementally and so avoid extensive redesigns.Leveraging LLMs to improve quality and speed up deliveryWe set quality levels we must achieve, but opting for efficiency over perfection means we\u2019re pragmatic about using tools such as AI-generated code. GPT 4o can save us time and money by generating architecture and feature recommendations. Our senior staff then spend their time critically assessing and refining those recommendations instead of writing the code from scratch themselves.There will be plenty who find that particular approach a turn-off or short-sighted, but we\u2019re careful to mitigate risks. Each build increment must be production-ready, refined and approved before we move on to the next. There is never a stage at which humans are out of the loop. All code\u00a0 \u2014 especially generated\u00a0 \u2014 is overseen and approved by experienced team members in line with our own ethical and technical codes of conduct.Data lakehouses: lean value data architectureInevitably, the lean-value framework spilled out into other areas of our process, and embracing large language models (LLMs) as a time-saving tool led us to data lakehousing; a portmanteau of data lake and data warehouse.Standardizing data and structuring unstructured data to deliver an enterprise data warehouse (EDW) is a years-long process, and it comes with downsides. EDWs are rigid, expensive and have limited utility for unstructured data or varied data formats.Whereas a data lakehouse can store both structured and unstructured data, using LLMs to process this reduces the time required to standardize and structure data and automatically transforms it into valuable insight. The lakehouse provides a single platform for data management that can support both analytics and ML workflows and requires fewer resources from the team to set up and manage. Combining LLMs and data lakehouses speeds up time to value, reduces costs, and maximizes ROI.As with the lean-value approach to product development, this lean-value approach to data architecture requires some guardrails. Teams need to have robust and well-considered data governance in place to maintain quality, security and compliance. Balancing the performance of querying large datasets while maintaining cost efficiency is also an ongoing challenge that requires constant performance optimization.A seat at the tableThe lean-value approach is a framework with the potential to change how technology teams integrate AI insight with strategic planning. It allows us to deliver meaningfully for our organizations, motivates high-performing teams and ensures they\u2019re used to maximum efficiency. Critically for the CTO, it ensures that the return on technology investments is clear and measurable, creating a culture in which the technology department drives commercial objectives and contributes as much to revenue as departments such as sales or marketing.Raghu Punnamraju is CTO atVelocity Clinical Research.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"},{"topic_title":"Data Decision Makers","article_title":"What does it cost to build a conversational AI?","article_url":"https:\/\/venturebeat.com\/ai\/what-does-it-cost-to-build-a-conversational-ai\/","article_date":"September 14, 2024 12:05 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMore than 40% of marketing, sales and customer service organizationshave adopted generative AI\u2014 making it second only to IT and cybersecurity. Of all gen AI technologies,conversational AIwill spread rapidly within these sectors, because of its ability to bridge current communication gaps between businesses and customers.Yet many marketing business leaders I\u2019ve spoken to get stuck at the crossroads of how to begin implementing that technology. They don\u2019t know which of the availablelarge language models(LLMs) to choose, and whether to opt for open source or closed source. They\u2019re worried about spending too much money on a new and uncharted technology.Companies can certainly buy off-the-shelf conversational AI tools, but if they\u2019re going to be a core part of the business, they can build their own in-house.To help lower the fear factor for those opting to build, I wanted to share some of the internal research my team and I have done in our own search for the best LLM to build our conversational AI. We spent some time looking at the different LLM providers, and how much you should expect to fork out for each one depending on inherent costs and the type of usage you\u2019re expecting from your target audience.We chose to compareGPT-4o(OpenAI) and Llama 3 (Meta). These are two of the major LLMs most businesses will be weighing against each other, and we consider them to be the highest quality models out there. They also allow us to compare a closed source (GPT) and an open source (Llama) LLM.How do you calculate LLM costs for a conversational AI?The two primary financial considerations when selecting an LLM are the set up cost and the eventual processing costs.Set up costs cover everything that\u2019s required to get the LLM up and running towards your end goal, including development and operational expenses. The processing cost is the actual cost of each conversation once your tool is live.When it comes to set up, the cost-to-value ratio will depend on what you\u2019re using the LLM for and how much you\u2019ll be using it. If you need to deploy your product ASAP,then you may be happy paying a premium for a model that comes with little to no set up, like GPT-4o. It may take weeks to get Llama 3 set up, during which time you could already have been fine-tuning a GPT product for the market.However, if you\u2019re managing a large number of clients, or want more control over your LLM, you may want to swallow the greater set up costs early to get greater benefits down the line.When it comes to conversation processing costs, we will be looking at token usage, as this allows the most direct comparison. LLMs like GPT-4o and Llama 3 use a basic metric called a \u201ctoken\u201d \u2014 a unit of text that these models can process as input and output. There\u2019s no universal standard for how tokens are defined across different LLMs. Some calculate tokens per word, per sub words, per character or other variations.Because of all these factors, it\u2019s hard to have an apples-to-apples comparison of LLMs, but we approximated this by simplifying the inherent costs of each model as much as possible.We found that while GPT-4o is cheaper in terms of upfront costs, over time Llama 3 turns out to be exponentially more cost effective. Let\u2019s get into why, starting with the setup considerations.What are the foundational costs of each LLM?Before we can dive into the cost per conversation of each LLM, we need to understand how much it will cost us to get there.GPT-4o is a closed source model hosted by OpenAI. Because of this, all you need to do is set your tool up to ping GPT\u2019s infrastructure and data libraries through a simple API call. There is minimal setup.Llama 3, on the other hand, is an open source model that must be hosted on your own private servers or on cloud infrastructure providers. Your business can download the model components at no cost \u2014 then it\u2019s up to you to find a host.The hosting cost is a consideration here. Unless you\u2019re purchasing your own servers, which is relatively uncommon to start, you have to pay a cloud provider a fee for using their infrastructure \u2014 and each different provider might have a different way of tailoring the pricing structure.Most of the hosting providers will \u201crent\u201d an instance to you, and charge you for the compute capacity by the hour or second. AWS\u2019s ml.g5.12xlarge instance, for example, charges per server time. Others might bundle usage in different packages and charge you yearly or monthly flat fees based on different factors, such as your storage needs.The provider Amazon Bedrock, however, calculates costs based on the number of tokens processed, which means it could prove to be a cost-effective solution for the business even if your usage volumes are low. Bedrock is a managed, serverless platform by AWS that also simplifies thedeployment of the LLMby handling the underlying infrastructure.Beyond the direct costs, to get your conversational AI operating on Llama 3 you also need to allocate far more time and money towards operations, including the initial selection and setting up a server or serverless option and running maintenance. You also need to spend more on the development of, for example, error logging tools and system alerts for any issues that may arise with the LLM servers.The main factors to consider when calculating the foundational cost-to-value ratio include the time to deployment; the level of product usage (if you\u2019re powering millions of conversations per month, the setup costs will rapidly be outweighed by your ultimate savings); and the level of control you need over your product and data (open source models work best here).What are the costs per conversation for major LLMs?Now we can explore the basic cost of every unit of conversation.For our modeling, we used the heuristic: 1,000 words = 7,515 characters = 1,870 tokens.We assumed the average consumer conversation to total 16 messages between the AI and the human. This was equal to an input of 29,920 tokens, and an output of 470 tokens \u2014 so 30,390 tokens in all. (The input is a lot higher due to prompt rules and logic).On GPT-4o, thepriceper 1,000 input tokens is $0.005, and per 1,000 output tokens $0.015, which results in the \u201cbenchmark\u201d conversation costing approximately $0.16.GPT-4o input \/ outputNumber of tokensPrice per 1,000 tokensCostInput tokens29,920$0.00500$0.14960Output tokens470$0.01500$0.00705Total cost per conversation$0.15665For Llama 3-70B on AWS Bedrock, thepriceper 1,000 input tokens is $0.00265, and per 1,000 output tokens $0.00350, which results in the \u201cbenchmark\u201d conversation costing approximately $0.08.Llama 3-70B input \/ outputNumber of tokensPrice per 1,000 tokensCostInput tokens29,920$0.00265$0.07929Output tokens470$0.00350$0.00165Total cost per conversation$0.08093In summary, once the two models have been fully set up, the cost of a conversation run on Llama 3 would cost almost 50% less than an equivalent conversation run on GPT-4o. However, any server costs would have to be added to the Llama 3 calculation.Keep in mind that this is only a snapshot of the full cost of each LLM. Many other variables come into play as you build out the product for your unique needs, such as whether you\u2019re using a multi-prompt approach or single-prompt approach.For companies that plan to leverage conversational AI as a core service, but not a fundamental element of their brand, it may well be that the investment of building the AI in-house simply isn\u2019t worth the time and effort compared to the quality you can get from off-the-shelf products.Whatever path you choose, integrating a conversational AI can be incredibly useful. Just make sure you\u2019re always guided by what makes sense for your company\u2019s context, and the needs of your customers.Sam Oliver is a Scottish tech entrepreneur and serial startup founder.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"},{"topic_title":"Virtual Communication","article_title":"ClickUp takes on Slack and Teams with its own AI-powered \u2018everything\u2019 chat","article_url":"https:\/\/venturebeat.com\/virtual\/clickup-takes-on-slack-and-teams-with-its-own-ai-powered-everything-chat\/","article_date":"September 17, 2024 10:46 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThose of us who use Slack or Microsoft Teams for work probably feel that there isn\u2019t much of a strong need for a new enterprise-focused chat application. But project management software companyClickUpdisagrees.Today, the7-year-old San Diego, California-based companyhas announcedthe launch of ClickUp Chat, a new tool that integrates team communication directly into the company\u2019s existing project management platform \u2014\u00a0and unlike rivals, includes AI-powered suggestions, summaries, and other features right from the jump.That is to say: even thoughTeamsandSlackhave been busy adding new AI-powered features to their offerings over the last several years, they didn\u2019t start with many, and ClickUp believes its approach to launch AI tools natively from day one offers a superior, more powerful, and more useful user experience.Indeed, by integrating chat with its existing \u201call-in-one\u201d task management software application for enterprises and teams, ClickUp is positioning itself as a leader in unifying communication that leads to actual, productive gains in the workplace among all members of teams.According to Zeb Evans, Founder and CEO of ClickUp, the new tool isn\u2019t just another chat app, but a new way of working.\u201cI wanted to build a platform that converges everything\u2026 not just bundling products together but actually building one application that does it all,\u201d Evans told VentureBeat in a video chat interview several days ago.ClickUp\u2019s mission is to reduce tool overloadClickUp was founded after Evans, a self-described lifelong entrepreneur, experienced several life-threatening events that inspired him to make better use of his limited time.Since then, ClickUp has aimed to simplify work management across teams and has gained 35 million users around the globe. Now, it sees generative AI and large language models (LLMs) to further accelerate its vision of an \u201ceverything\u201d workplace app where you can use one piece of software to coordinate all your communications, tasks, and project management across your team.\u201cWith AI, you\u2019ll be able to build software faster,\u201d Evans said. \u201cApplications will become more like super apps. We were one of the first to focus head-on on convergence.\u201dThe company has long aimed to reduce inefficiencies caused by the need for multiple disconnected tools, a problem Evans himself experienced in his previous ventures.\u201cWe had 15 different productivity tools\u2026 Trello for boards, Asana for lists, Slack and Skype\u2026 It felt like it was all necessary, but it created so much inefficiency,\u201d Evans added.And to be clear, ClickUp\u2019s customers can still use those other apps and integrate them into the platform, but ClickUp\u2019s goal is ultimately to persuade them to switch and lower their total costs by porting all the data from those rival task management products over.The introduction of chat functionality is the next step in this effort.The company points out that since the launch of Slack in 2014, workplace communication platforms have revolutionized how distributed teams work, but they have also introduced significant inefficiencies.Teams are often overwhelmed by noise\u2014constant notifications, scattered conversations, and fragmented tools that require frequent switching between apps.Evans notes that while chat apps like Slack and Microsoft Teams have made collaboration easier, they are typically isolated from the platforms where work is actually managed.In many cases, teams talk about work in one app and manage tasks in another, leading to disorganization, distractions, and inefficiencies.\u201cClickUp allows you to eliminate that \u2018toggle tax\u2019\u2026 Instead of using five different tools, you can get all those features in ClickUp and transfer your data over easily,\u201d he told us.How ClickUp chat works and new AI-powered featuresClickUp Chat introduces several innovative features designed to improve team communication and overall productivity:Connected Conversations and Tasks: Channels in ClickUp Chat go beyond basic conversation threads. They are linked to projects, tasks, documents, forms, and whiteboards. This allows users to stay organized, ensuring that conversations stay connected to the work they are discussing.FollowUps\u2122: This feature helps users triage messages, ensuring that important conversations and action items aren\u2019t lost. FollowUps allow teams to link messages to tasks and track them effortlessly, eliminating the need to toggle between chat platforms and project management tools.Convert Messages into Tasks: Teams can now turn any conversation into a task directly within ClickUp. Leveraging ClickUp\u2019s Brain\u2122 AI, the tool can capture the context from chat threads and create detailed tasks ready for execution. This eliminates the manual process of creating tasks from scratch.AI-Powered Insights: ClickUp Chat integrates several AI-driven features to manage communication more effectively. TheAI CatchUptool summarizes missed conversations so that users can quickly see what\u2019s relevant without reading through every message. Additionally,AI CleanUpidentifies action items and summarizes important updates in chat channels, reducing noise and improving organization.Enhanced Direct Messaging (DMs): Users can schedule meetings, assign tasks, and view team members\u2019 workloads directly within chat. This enables smoother workflows by providing better visibility into teammates\u2019 availability and priorities.SyncUps: ClickUp Chat supports live video and audio calls, allowing for screen sharing, task linking, and real-time collaboration. AI can automatically summarize these meetings and create action items, providing teams with a clear roadmap after each SyncUp.Reconnecting and organizing disconnected workflowsA major benefit of ClickUp Chat is its ability to synchronize conversations and work context. The platform\u2019s Synced Threads feature keeps threads aligned between chat and tasks, ensuring that conversations stay relevant to the work being done, no matter where they happen.For example, discussions about tasks, projects, or documents are automatically linked to their respective resources, providing users with a holistic view of everything connected to a conversation.The Posts feature allows for longer-form, asynchronous communication, making it easier to share updates, announcements, or detailed ideas without the fear of missing out on important chat threads.This supports deep work by giving users a dedicated space for more thoughtful discussions while maintaining full transparency across teams.AI-driven task managementClickUp Chat\u2019s integration of AI aims to bring communication and productivity together in its platform.Tools like AI Answersallow users to ask questions about any project or task in a channel, with AI pulling answers from the channel\u2019s history or the wider ClickUp workspace.This, ClickUp and Evans claim, reduces the need for repetitive back-and-forth communication and ensures that all necessary information is at team members\u2019 fingertips.An AI Task Creation feature allows users to natively create a new task item in ClickUp\u2019s platform from any chat message with a single click.The AI integration \u2014 powered by OpenAI\u2019s GPT-4 model for some AI functionalities, while also using proprietary models for specific tasks \u2014 will automatically write the task name, description, and link it to relevant chat context.This speeds up task management, ensuring that no conversation is left disconnected from action items.But what about the cost of these powerful new features? Fortunately, the new ClickUp chat is bundled with ClickUp.\u201cSo it starts at $7 per month per user,\u201d Evans said. \u201cWe want to provide the best value and the best product for a great price. I\u2019ve always wanted to provide the best value\u2026 I think we do have the best pricing amongst our competitors, and that\u2019s honestly my core philosophy\u2014getting it into the hands of as many people as possible to make them more efficient.\u201dBy keeping conversations directly linked to projects, tasks, and documents, and using AI to streamline workflows, ClickUp aims to create a more organized, focused, and productive workplace \u2014 getting closer to realizing its vision of becoming the \u201cEverything App for Work.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Virtual Communication","article_title":"Stability AI brings new Stable Diffusion models to Amazon Bedrock","article_url":"https:\/\/venturebeat.com\/ai\/stability-ai-looks-to-grow-stable-diffusion-text-to-image-ai-usage-with-amazon-bedrock\/","article_date":"September 4, 2024 10:30 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreStability AIhas been struggling of late trying to find its business footing in an increasingly competitive market for text-to-image generative AI tools.Today the company announced a significantly expanded effort withAmazon Web Services (AWS)that will see three text-to-image generative AI models come to theAmazon Bedrock service. Previously Stability AI had just made its older, though still very capableSDXL modelavailable on Amazon Bedrock.Stability AI is now bringing a trifecta of its most sophisticated text-to-image models to Amazon Bedrock including Stable Image Ultra, Stable Diffusion 3 Large and Stable Image Core. Each model brings its own set of strengths to the table, catering to a wide range of use cases from ultra-realistic imagery to high-volume asset generation. For enterprise users, the promise is the option of getting the right model for a given task.The new Stability AI models will join a growing list of models that Amazon Bedrock provides. AWS already makes multiple image models available on Amazon Bedrock including its ownTitan model.Inside the Stable Diffusion text to image AI model lineupWhen Stability AI got its start, the only model was Stable Diffusion. That has changed in recent years and especially this year as multiple variations of the company\u2019s Stable Diffusion model have emerged.Functionally the differences are much like how other generative AI models have evolved with different sizes. Larger models tend to be more powerful, as well as require more resources and cost than smaller models.Looking at the lineup,Stable Diffusion 3.0was first revealed as a preview in February of this year. Full availability occurred in Aprilvia an API.In June, the company announced itsStable Diffusion Mediummodel, at the same time rebranding the original sized model as Stable Diffusion Large. At the same time, Stability AI quietly released Stable Diffusion Ultra via API though no formal announcement was made.\u201cStable Image Ultra is our flagship model, blending the power of the SD3 Large with advanced workflows to deliver the highest quality images,\u201d Scott Trowbridge, VP of business development at Stability AI told VentureBeat. \u201cThis premium model is designed for industries that require unparalleled visual fidelity, such as marketing, advertising and architecture.\u201dStable Image Core on the other hand is based on SDXL. The goal of the core model is to provide a faster and more affordable model. Stability AI is not making Stable Diffusion 3 Medium available, at least not initially, on Amazon Bedrock.\u201cSD3 Medium is a two billion parameter model, making it perfect for running on consumer PCs and laptops as well as enterprise-tier GPUs,\u201d Trowbridge said. \u201cThe model is available for self-hosting and via our API.\u201dStability AI is growing its go-to-market optionsStability AI has been trying to grow its go-to-market options over the last 12 months.In December 2023, the company introduced a newmembership model, as a way to create some form of commercial business and revenue. The company also has its Stable Assistant chatbot that provides access to models. Stability AI charges users based on usage, via the API or Stable Assistant.Stability AI has also undergone multiple operational challenges this year. In March, the company\u2019s founder and CEOEmad Mostaque resigned. He was finally replaced in June, withnew CEO Prem Akkarajualongside a new infusion of capital.It\u2019s also not entirely clear, at least as of today, how the cost of running the models on Amazon Bedrock will compare with the other methods for running the Stability AI models. Stability AI did not directly respond to a question from VentureBeat about how the costs compare.In any event, having more paths to market and usage of its models represents more potential overall revenue that Stability AI can grow. The Amazon Bedrock deal is likely not the only one Stability AI will make with a cloud provider.\u201cStability AI is always considering ways to expand accessibility to our models, including via cloud service providers, system integrators and other model service providers,\u201d Trowbridge said.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Virtual Communication","article_title":"Thera scores $4M seed to make it easier to hire and pay international employees","article_url":"https:\/\/venturebeat.com\/virtual\/thera-scores-4m-seed-to-make-it-easier-to-hire-and-pay-international-employees\/","article_date":"August 22, 2024 6:02 AM","article_text":"The number of businesses hiring international employees is on the rise: spurred by rising domestic labor costs and the need for specific talent, 75% of small-to-medium-sized businessesin a 2023 surveyof 500 owners and decision-makers byGustosaid they planned to increase international headcount, and 54% planned to do so in the coming 1-3 years.However, for businesses looking to take advantage of the global talent pool, ensuring they are set up topaytheir international employees and contractors in a timely fashion can be a bear. EnterThera, a payroll and payments\u00a0startup founded in 2022 that promises to help businesses of all sizes navigate the maze of differing countries\u2019 labor rules and regulations and get employees paid no matter where they work from.\u201cWe started with this thesis that more businesses are going to be global from day one,\u201d said Akhil Reddy, Thera\u2019s founder and CEO, in a phone interview with VentureBeat earlier this week.The thesis has legs. Today, Thera announced it has raised $4 million in seed funding from Y Combinator, 10x Founders, Amino Capital, Zillionize and Bayhouse Capital. It also got funding from angel investors Oliver Jung, Chris Bakke, Andrew Yeung, Akash Magoon and Bobby Matson.Video showing Thera\u2019s dashboard. Credit: TheraThera\u2019s origin storyReddy has considerable experience designing systems to ease the flow of payments digitally, having previously built systems for Amazon Prime, the e-commerce giant\u2019s free-shipping and included media subscription service tier.\u201cTwo of the big things I learned at Amazon were the importance of selection and transferring affordable credits,\u201d Reddy told VentureBeat. \u201cWe\u2019re taking that same ethos and trying to apply it to SaaS [software-as-a-service.]\u201dInspired by that experience, Thera\u2019s new, built-from-scratch system replaces multiple financial tools, offering an ecosystem of native apps for payroll, treasury and accounts payable\/receivable (AP\/AR) services.\u201cPayments are a massive challenge for many companies,\u201d Reddy added in a press release provided to VentureBeat. \u201cThrough our unique bundled app model and customer-centric approach, we strive to create a streamlined experience to manage all financial operations while increasing our customer\u2019s bottom line. We are committed to providing the rates and transparency the industry so desperately needs.\u201dWhat Thera offersScreenshot of Thera client-side dashboard. Credit: TheraThera offers a range of services including U.S. payroll for hiring and paying employees in all 50 states, contractor management for more than 150 countries, multiple currencies and employer-of-record services in 150+ countries and payments in five methods. It also features Thera AP\/AR for global invoicing and payments, and Bill Pay to manage all payables in one place.Thanks to its integration with major payment providers and global systems, as well as a database of consistently updated information on the different labor laws around the world, combined with its platform of native apps for payroll and tracking, Thera expects to save its customers significant sums of money.On its website, the company boasts of being 80% more affordable than Deel AP\/AR for global payroll and 90% more affordable than Stripe Invoicing. Also, it claims a speed improvement over rivals that benefits employees and contracts, with 95% of payrolls arriving the same day compared to 2-10 days for other payroll companies.By saving on these costs and time, Thera also says it can pass the savings onto contractors \u2014 who can make up to 3% more than other providers.Thera\u2019s business customers can also expect to receive several direct support and success reps available to them 24\/7\/365 through Slack.\u201cYou get a customer success manager, and you also get a global HR specialist in a Slack channel,\u201d Reddy told VentureBeat. \u201cEveryone gets a shared Slack channel to answer any questions they might have.\u201dThera\u2019s initial successBy consolidating these services, Thera provides businesses with a seamless experience and some of the most competitive rates on the market.Already, Thera is processing $10M+ in payroll annually for thousands of workers worldwide at some of the fastest-growing companies in the U.S., including Oceans, Landed, 1840 & Company and Zendrop.\u201cWe saw a lot of customers complaining about the lack of transparency with their existing [payments] providers,\u201d he elaborated to VentureBeat. \u201cThere\u2019s so many hidden fees in the FX [financial experience], and then also one-time fees that they were unaware of.Where Thera goes nextThe $4 million Seed funding will be instrumental in accelerating Thera\u2019s growth.The company plans to use the funds to further develop its platform and expand its team in New York City, where it is currently headquartered.Reddy also told VentureBeat that while Thera\u2019s financial and payments apps for businesses are today available on desktop via the web, the company is working on mobile offerings that should be available soon.As Thera continues to grow, the company remains focused on its mission to streamline financial operations for businesses around the world.With the new funding, Thera is well-positioned to enhance its platform, expand its team, and solidify its position as a leading player in the B2B payments space.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Virtual Communication","article_title":"How Shift transformed Chromium into a browser for power users","article_url":"https:\/\/venturebeat.com\/virtual\/how-shift-transformed-chromium-into-a-browser-for-power-users\/","article_date":"August 19, 2024 7:20 AM","article_text":"Shift\u2019sinterface looks clean, fresh and simple \u2014 but under the hood, it\u2019s a powerful Chromium browser that\u2019s the first of its kind. Shift\u2019s browser merges web apps and search into a single interface, eliminating window-switching and making it easy for users to manage multiple email inboxes. In Shift, users can create custom browser workspaces with separate apps, accounts and tabs, promoting natural organization and reducing tab pile-ups.By developing the browser on top of a Chromium base and tapping into underlying APIs, Shift created a highly custom interface. Shift\u2019s Chromium foundations allow the development team to explore new features and iterate in a more responsive and agile way. It\u2019s how they reimagined the modern browser to align with how people are actually using the internet, says Michael Foucher, VP of Product at Shift. The flexibility of their software base is Shift\u2019s competitive advantage.\u201cWe chose Chromium because it\u2019s by far the most active open-source browser in the world, and they\u2019re constantly trying to make it better, more secure and more performant,\u201d Foucher says. \u201cWe\u2019ve built a browser that consolidates apps and search into a single interface, and Chromium gives us a rock-solid foundation to keep innovating.\u201dShift\u2019s approach to development is unique, he adds, and that\u2019s the secret sauce and their competitive advantage.Shift\u2019s browser is pushing the boundaries of ChromiumMost Chromium variants out there, like Opera, Brave and Microsoft Edge, are building their UI using bulky C++ code \u2014 resulting in all their products looking very much like Chrome \u2014 and it\u2019s not because they love the interface.\u201cIt\u2019s because developers don\u2019t want to touch it,\u201d Foucher says. \u201cIf you touch C++ code, everything blows up. It\u2019s very hard to make any substantial UI changes because any small change to that code can destroy the whole thing.\u201dShift\u2019s browser, however, leverages powerful web technologies to craft a unique user experience on top of Chromium, replacing the standard look and feel. Shift makes a number of customizations directly to Chromium, including proprietary APIs to facilitate communication between the parts of the browser that users interact with and the engine that drives it. Through this process, Shift creates a fully customized interface very quickly and easily makes alterations or additions \u2014 without the need for an army of developers.\u201cIt\u2019s a major productivity boost, and makes our small team mighty,\u201d Foucher says. \u201cWe can develop and test and make improvements in a flash. That speed and that ability to leverage the newest and best in web tech is a huge advantage for us.\u201dHowever, Chromium itself can slow down the development process significantly. At over 20 million lines of code, Chromium is massive. A build can take a whole day locally, or multiple hours in the cloud. To solve the issue, Shift turned to EngFlow for remote execution that distributes, builds and tests across a cluster of machines, and remotely caches the results. At Shift, build times have dropped from two or more hours to under 20 minutes.\u201cIt\u2019s not inexpensive. But it is a huge savings because of the productivity enhancement and the cost of developers not sitting around waiting for things to build,\u201d Foucher says. \u201cThat\u2019s a huge factor in allowing us to move forward quickly.\u201dDesigning a new kind of interfaceFrom the start, Shift has been intentional with the look and feel of its interface, working with multiple designers to create an environment that\u2019s organized, flexible and efficient \u2014 and more attractive than a bare-bones browser.While it\u2019s a new kind of browsing experience, Shift users can quickly get comfortable with the interface. Shift draws from the Chromium community for open-source features that have become staples in the browser experience and users are already familiar with.\u201cI look to them as the muscle memory we\u2019re trying to mimic for people. The keyboard shortcuts you love in Chrome, you\u2019ll be able to use them in Shift,\u201d Foucher says. \u201cWe try to take advantage of those familiar, heavily used features. Where we do diverge, it\u2019s to provide a net new experience that will make our browser better.\u201dBehind the scenes, Shift is exploring more customization in user settings with the ultimate goal of handing users more control over their browsers. These highly-anticipated features will be released in future product updates. In the meantime, Shift is continually rolling out upgrades to their browser including cosmetic changes that might appear to be minor on the surface, Foucher says, but they\u2019re important.\u201cIt shows a sense of pride in the product we\u2019re releasing,\u201d he explains. \u201cIt might be subtle, but it just shows that we really care about the user experience.\u201dSponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they\u2019re always clearly marked. For more information, contactsales@venturebeat.com."},{"topic_title":"Virtual Communication","article_title":"Exclusive: Resquared nabs $5M to take on leading CRMs with B2B local sales platform","article_url":"https:\/\/venturebeat.com\/ai\/exclusive-resquared-nabs-5m-to-take-on-leading-crms-with-b2b-local-sales-platform\/","article_date":"July 23, 2024 2:06 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreSales and software are a match made in heaven: just askSalesforceor evenHubspotorCreatio, all of which have built multi-billion dollar companies atop customer relationship management software suites specifically focused on sales teams.But are all of those firms missing out on an opportunity right under their noses?Resquared(also known as Re\u00b2) thinks so.The Y Combinator-backed, fully remote startup revealed to VentureBeat recently that it successfully closed a $5 million seed round for its B2B sales tool focused specifically on reachinglocalbusinesses across the U.S., those that may not have much or any online footprint except for consumer-facing social tools like Facebook pages and Instagram accounts.\u201cWe invest a lot of our own team\u2019s time and research towards figuring out what is the most effective way to reach local businesses,\u201d said Griffin Morris, Resquared\u2019s co-founder and CEO, in a teleconference interview with VentureBeat conducted last week. \u201cThey\u2019re typically not on LinkedIn, but if you reach out to a business through their Facebook or Instagram page, you can get a 20% reply rate and it\u2019s often from the owner of the business. So that\u2019s how we start all of our product development: with best practices.\u201dAlready,Resquared claimsto have collected data on more than 11 million SMBs throughout the U.S. and Canada. It offers a \u201csuite of AI-powered email and social media messaging tools.\u201d\u201cWe have thousands of sales reps on the platform and hundreds of sales organizations as clients,\u201d Morris told me. \u201cA lot of our growth comes from investing in content and our clients sharing their positive experiences on social media.\u201dThis funding round was spearheaded by SNR\u2019s Kevin Patrick Mahaffey, alongside 1984 Ventures, Merus Capital, Oleg Rogynskyy from People.ai, Don Tepman, famously known as \u201cStripMallGuy\u201d (with 230,000+ followers on X) and Twenty Two Ventures.The infusion of capital will expedite Resquared\u2019s mission to establish a new segment of B2B sales, tailored to meet the unique challenges of reaching small and medium-sized businesses (SMBs).The problem: 400,000 sales reps focused on local biz, but using the wrong tools for the jobMore than 400,000 sales representatives in the U.S. currently target local businesses, according to research obtained by Resquared, selling everything from real estate leases on new locations within cities to restaurant and office supplies to, of course, software tools.However, these local salespeople often resort to outdated methods like door-to-door sales or telemarketing for lead generation.While some use platforms like ZoomInfo or enterprise CRMs, these tools typically rely on LinkedIn data, which only 10% of local business owners use.This disconnect results in inefficient outreach efforts, with local businesses receiving numerous spam messages and having limited vendor choices.How Resquared provides salespeople with a data advantage to reach local SMBsResquared aims to solve this issue by supporting thousands of sales reps with campaigns that boast open rates twice the industry average.The platform provides a higher return on investment through increased revenue and time saved, with clients reporting up to 30% increases in monthly sales.It relies on a combination of AI-driven automated outreach over email and social media, and its underlying local business data, as well as comprehensive analytics to track performance of said messages to prospective customers.Screenshot of Resquared\u2019s local outreach platform. Credit: Resquared\u201cWe have always had AI and machine learning features, studying interactions and data to figure out what works best,\u201d Morris explained. \u201cWe use proprietary machine learning features and open AI APIs to enhance our platform.\u201dPricing for Resquared\u2019s platformvaries depending on the number of users and is customized for each client.\u201cIf you had a sales team with five reps, starting on the platform would be around $1,000 a month,\u201d Morris ssaid. \u201cEvery client gets one-on-one onboarding and training to ensure high-quality and personalized sales processes.\u201dKevin Patrick Mahaffey of SNR commented on the investment in the release, stating, \u201cSelling software online is like living in the future: AI-enabled tools at every layer of the stack, sophisticated pipelines, and highly orchestrated processes. Selling to real-world businesses, however, looks about the same as it did 30 years ago. Resquared gives superpowers to sales teams engaging with local businesses so main street can benefit from the onward march of progress as well.\u201dOrigin storyResquared was born from the shared experiences of its founders, Morris and Tyler Carlson, who have both focused their careers on the intersection of tech and the brick-and-mortar economy.Morris began his career selling door-to-door to local businesses before transitioning to product management.\u201cI had a sales job where my quota was to walk into 40 local businesses a day in St. Louis, Missouri. Twelve years later, that\u2019s still how it\u2019s done for the most part,\u201d noted Morris. \u201cCompanies often rely on high volume, low-quality marketing outreach like call centers.\u201dCarlson, on the other hand, worked closely with major brick-and-mortar franchises with thousands of local outposts including Subway and Burger King, helping them modernize their stores with technology.Building on this foundation, Resquared emerged with a vision to not just improve sales processes but to create an entirely new segment of B2B sales from the ground up.The team developed targeted playbooks based on local business needs, evolving into a comprehensive platform powered by a feedback loop of data-driven best practices, AI workflows, and automation. Resquared also offers free courses, playbooks, and a podcast titled \u201cSelling Local.\u201dInitial success storiesAs mentioned previously, Resquared provides a database and instant access to more than 11 million local businesses, using AI to automate high-quality, personalized outreach.Clients include publicly traded retail landlords such as Kimco, marketing agencies specializing in SEO, commercial services companies ranging from pest control to maintenance, commercial insurance, small business lending, and suppliers.During an interview, Morris elaborated on Resquared\u2019s approach. \u201cWe also gather data from public government records, social media sites, and information that businesses share on their websites. It\u2019s messy data, but we clean, organize, and match it to ensure quality.\u201dGrowth and future outlookResquared is experiencing rapid growth, with thousands of sales reps and hundreds of sales organizations using the platform.The company plans to use the new funding to expand its sales and engineering teams, focusing on an ambitious product roadmap.Currently, Resquared operates primarily in the U.S. and Canada, supporting a wide range of clients from billion-dollar publicly traded companies to regional businesses with significant sales teams.The company is fully remote, with team members spread across various hubs like New York, Southern California, and Florida. Morris himself operates out of Philadelphia, Pennsylvania, while co-founder Carlson works out of Tampa, Florida.This flexibility allows Resquared to hire the best talent from anywhere while maintaining strong team cohesion through regular in-person meetings and conferences.With this fresh injection of capital and a clear mission, Resquared is poised to revolutionize B2B sales for local business vendors, bringing high-quality, personalized sales techniques to a traditionally underserved market segment. To learn more about how Resquared is changing the game for companies targeting local businesses, visitRe2.ai.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Programming & Development","article_title":"Vera AI launches \u2018AI Gateway\u2019 to help companies safely scale AI without the risks","article_url":"https:\/\/venturebeat.com\/ai\/vera-ai-launches-ai-gateway-to-help-companies-safely-scale-ai-without-the-risks\/","article_date":"October 2, 2024 10:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreVera AI Inc., a startup focused on responsible artificial intelligence deployment, announced today the general availability of itsAI Gatewayplatform. The system aims to help organizations more quickly and safely implement AI technologies by providing customizable guardrails and model routing capabilities.\u201cWe\u2019re really excited to be announcing the general availability of our model routing and guardrails platform,\u201d said Liz O\u2019Sullivan, CEO and co-founder of Vera, in an interview with VentureBeat. \u201cWe\u2019ve been hard at work over the last year building something that could scalably and repeatably accelerate time to production for the kinds of business use cases that actually stand to generate a lot of excitement.\u201dVera AI\u2019s policy configuration interface, showcasing the platform\u2019s granular content moderation tools. The dashboard allows companies to customize AI safeguards, balancing the need for innovation with responsible content management \u2014 a key selling point in Vera\u2019s mission to make AI deployment both efficient and ethical. (Credit: Vera)Bridging the gap: How Vera\u2019s AI gateway tackles last-mile challengesThe launch comes at a time when many companies are eager to adopt generative AI and other advanced AI technologies, but remain hesitant due to potential risks and challenges in implementing safeguards. Vera\u2019s platform sits between users and AI models, enforcing policies and optimizing costs across different types of AI requests.\u201cBusinesses are only ever interested in doing one of three things, whether that\u2019s make more money, save more money, or reducing risk,\u201d O\u2019Sullivan explained. \u201cWe\u2019ve focused ourselves squarely on the last mile problems, which people think, just like regular software engineering, that it\u2019s going to be quick and easy, that these are just afterthoughts that you can apply to optimize costs or to reduce risks associated with things like disinformation and broad and CSAM, but they\u2019re actually quite hard.\u201dJustin Norman, CTO and co-founder of Vera, emphasized the importance of nuance in AI policy implementation: \u201cYou want to be able to set the bar for where your system will respond and where it will not respond and what it will do, without having to rely upon what some other companies made a decision for you on.\u201dVera AI\u2019s interface demonstrates its content moderation capabilities, blocking a user\u2019s input that failed to follow the specified rules \u2014 a key feature in the company\u2019s mission to provide guardrails for responsible AI deployment. (Credit: Vera)From AI safety activism to startup success: The minds behind VeraThe company\u2019s approach appears to be gaining traction. According to O\u2019Sullivan, Vera is already \u201cprocessing tens of thousands of model requests per month across a handful of paying customers.\u201d The startup offers API-based pricing at one cent per call, aligning its incentives with customer success in AI deployment. Additionally, Vera has introduced a 30-day free trial, which can be accessed using the code \u201cFRIENDS30,\u201d allowing potential customers to experience the platform\u2019s capabilities firsthand.Vera\u2019s launch is particularly noteworthy given the founders\u2019 backgrounds. O\u2019Sullivan, who serves on theNational AI Advisory Committee, has a history of AI safety activism, including her work atClarifai. Norman brings experience from government, academia, and industry, including PhD work at UC Berkeley focused onAI robustness and evaluation.Navigating the AI safety landscape: Vera\u2019s role in responsible innovationAs AI adoption accelerates across industries, platforms like Vera\u2019s could play a crucial role in addressing safety and ethical concerns while enabling innovation. The startup\u2019s focus on customizable guardrails and efficient model routing positions it well to serve both enterprise clients managing internal AI use and companies developing consumer-facing AI applications.However, Vera faces a competitive landscape with other AI safety and deployment startups also vying for market share. The company\u2019s success will likely depend on its ability to demonstrate clear value to customers and stay ahead of rapidly evolving AI technologies and associated risks.For organizations looking to responsibly implement AI, Vera\u2019s launch offers a new option to consider. As O\u2019Sullivan put it, \u201cWe\u2019re here to make it as easy as possible to enjoy the benefits of AI while reducing the risks that things do go wrong.\u201dVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Programming & Development","article_title":"How IT leaders can spearhead the charge to transform education","article_url":"https:\/\/venturebeat.com\/programming-development\/how-it-leaders-can-spearhead-the-charge-to-transform-education\/","article_date":"October 2, 2024 6:40 AM","article_text":"Presented by MSIIf a primary goal of education is to prepare kids for the future, IT leaders play a more pivotal role than ever. Technology has profoundly impacted work in every industry \u2014 and it\u2019s opened up vast new possibilities in new fields, from positions across STEM industries and AI, to esports and beyond. It\u2019s also transformed how students engage with learning, skill development and high-level problem-solving and critical thinking.\u201cExposing students to computer science and high-end technology is not only useful for the future as they inevitably use it in their careers, but it changes their relationship to school,\u201d says Mat Holley, esports program manager at MSI. \u201cWhen they\u2019re more engaged, they have better attendance. They have better grades. They\u2019re more prepared for college and the job market. The enthusiasm is remarkable.\u201dSchool boards are leading the charge for these initiatives, but they can\u2019t do it on their own. They must partner with IT leaders in their district, education specialists and technology industry professionals to deliver these learning experiences, and the challenge is to ensure that these programs are cost-effective, with technology, expertise and activities that are future-proof.How technology is transforming the learning experienceTo support these initiatives, the choice of hardware and software becomes critical. Holley points to the extracurricular club in the charter school district in Chula Vista, San Diego he worked with to help develop and outfit new technology learning initiatives. Students there work on video design, broadcasting, AI and music creation using Vector GP and Raider GE series laptops from MSI, integrating graphics hardware from Nvidia and processing power from Intel. And this high-end gaming hardware and software supports what\u2019s become the largest high school-run esports program in the U.S., the Kern High School District Esports League.\u201cI\u2019ve worked with schools that are far along their journey and ready to level up their hardware, to keep pace with how the kids are working and learning, and I\u2019ve also helped districts build the programs from the ground up, from the right hardware to student outreach,\u201d Holley explains. \u201cAnd though much of this is uncharted territory, the momentum is building, sometimes through word of mouth.\u201dThe surprising benefits of esportsEducators are sharing knowledge, sparking interest and collaborating with their peers, working toward developing a curriculum standard and blueprint for the hardware and software specifications that can support those programs.Though it\u2019s initially surprised many educators and leaders that esports can have such a profound effect on kids \u2014 especially the ones who often feel excluded from other sports \u2014 the number of esports programs is growing. Not only are there tremendous educational and social development benefits for the students that participate, esports also attracts kids who have never joined an extracurricular club: the girls who have felt left out in science and math classes, the BIPOC students who deserve bigger opportunities. The clubs raise their confidence in their own abilities, and more often than not, these students go on to study computer science or some other linked technology career.\u201cThere is no barrier to entry to be a gamer, and this goes for computer science at large,\u201d Holley says. \u201cYou don\u2019t even have to be a gamer to enter these clubs. More and more, esports is plugged into all the various technology clubs like design, broadcasting and journalism, and formerly disenfranchised kids are finding their calling through these clubs in an unprecedented way.\u201dBuilding the learning experiencefrom the ground upOf course, there continue to be challenges for school districts developing these programs, and many of them come down to major budget constraints. There are also the difficulties that come with ensuring security is solid, that new technology is integrated into existing networks, and moving the environment from on-prem to the cloud. MSI collaborates with educational institutions to ensure that they\u2019re not only hitting the district\u2019s hardware specs, but new hardware will be integrated seamlessly.\u201cAs we saw more esports integrated into schools, we worked with schools to meet the specifications of their price points, their warranty needs, which are typically longer than a retail warranty,\u201d Holley says. \u201cWe wanted to also make sure that these were machines that the students got excited to play on, that sophisticated esports titles were supported. As we started to work with more schools closely, we integrated products from our professional line to improve the student experience and give them access to even more tech areas to explore.\u201dEducational IT leaders rejoice: adding computer labs like these is easier than ever. As computing advances, the size of the hardware continues to shrink, making student computers lightweight and easy for IT teams to deploy. IT leaders should also look for hardware that\u2019s easy to integrate, especially from a security point of view \u2014 however, most districts are working with legacy hardware environments.\u201cAs you build a technology center for students, you have to consider whether existing hardware will play with the new, and whether it will move to the cloud securely,\u201d Holley says. \u201cBut as long as we can integrate security standards like content filters, custom imaging and Autopilot deployment, it\u2019s much easier to deploy at scale in almost any environment. We try to build directly in tandem with district-wide IT departments, so they can tell us what they need and what their road map looks like. Then from the manufacturer side, we\u2019re able to make sure that we all play along in the years to come.\u201dAnother major consideration is product life cycles, which are incredibly short in the consumer world. IT leaders should work with a partner that offers dedicated hardware for education, with life cycles long enough to mesh with the fairly long bidding and buying timeline for education purchases.And of course, as cloud computing becomes the standard, it\u2019s important to stay abreast of hardware and software changes and evolving risk scenarios. That means research, testing and working with your supplier to keep informed about the newest hardware and software advancements and when it\u2019s time to upgrade. It also means selecting hardware that\u2019s easily upgradable and expandable.Making hardware choices a whole lot easierTo support technology education, MSI offers theCubi NUC and DP21, which support Intel vPro and Windows Autopilot to simplify management, enhance security and streamline the deployment process. Thunderbolt 4 technology and power delivery offer fast connectivity and charging. They\u2019re also easily scalable, and offer real-time data processing for AI and machine learning. Their compact size offers flexible installation and a good performance vs. footprint ratio, plus flexible configuration.The company also offers STEM, gaming and content creation computers like the DP180,CreatorPro,Vector GP and Raider GE series laptopswith dedicated graphics hardware that accelerate graphics-heavy applications, and offer easy upgradability with expandable memory and storage options to ensure longevity.Veteran resellers and manufacturers will work with decision-makers to ensure schools get the best hardware and software their money can buy, plus keep IT teams in the loop what\u2019s coming next, and how to make sure students have every opportunity to learn with the newest technology possible.\u201cWe\u2019re paving a path for these students into the future, and it\u2019s important that we\u2019re equipping them for everything that\u2019s to come,\u201d Holley says. \u201cGaming and other high-tech hardware has become an integral part of the plan, so IT leaders must be willing to get creative when designing technology resources and work with allies across manufacturing and reselling to push initiatives forward.\u201dDig deeper:Learn more hereabout the technology solutions that power today\u2019s educational experiences.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they\u2019re always clearly marked. For more information, contactsales@venturebeat.com."},{"topic_title":"Programming & Development","article_title":"OpenAI\u2019s DevDay 2024: 4 major updates that will make AI more accessible and affordable","article_url":"https:\/\/venturebeat.com\/ai\/openai-devday-2024-4-major-updates-that-will-make-ai-more-accessible-and-affordable\/","article_date":"October 1, 2024 10:15 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIn a marked contrast to last year\u2019ssplashy event, OpenAI held a more subduedDevDay conferenceon Tuesday, eschewing major product launches in favor of incremental improvements to its existing suite of AI tools and APIs.The company\u2019s focus this year was on empowering developers and showcasing community stories, signaling a shift in strategy as the AI landscape becomes increasingly competitive.The company unveiled four major innovations at the event: Vision Fine-Tuning, Realtime API, Model Distillation, and Prompt Caching. These new tools highlight OpenAI\u2019s strategic pivot towards empowering its developer ecosystem rather than competing directly in the end-user application space.Prompt caching: A boon for developer budgetsOne of the most significant announcements is the introduction ofPrompt Caching, a feature aimed at reducing costs and latency for developers.This system automatically applies a 50% discount on input tokens that the model has recently processed, potentially leading to substantial savings for applications that frequently reuse context.\u201cWe\u2019ve been pretty busy,\u201d said Olivier Godement, OpenAI\u2019s head of product for the platform, at a small press conference at the company\u2019s San Francisco headquarters kicking off the developer conference. \u201cJust two years ago, GPT-3 was winning. Now, we\u2019ve reduced [those] costs by almost 1000x. I was trying to come up with an example of technologies who reduced their costs by almost 1000x in two years\u2014and I cannot come up with an example.\u201dThis dramatic cost reduction presents a major opportunity for startups and enterprises to explore new applications, which were previously out of reach due to expense.A pricing table from OpenAI\u2019s DevDay 2024 reveals major cost reductions for AI model usage, with cached input tokens offering up to 50% savings compared to uncached tokens across various GPT models. The new o1 model showcases premium pricing, reflecting its advanced capabilities. (Credit: OpenAI)Vision fine-tuning: A new frontier in visual AIAnother major announcement is the introduction of vision fine-tuning forGPT-4o, OpenAI\u2019s latest large language model. This feature allows developers to customize the model\u2019s visual understanding capabilities using both images and text.The implications of this update are far-reaching, potentially impacting fields such as autonomous vehicles, medical imaging, and visual search functionality.Grab, a leading Southeast Asian food delivery and rideshare company, has already leveraged this technology to improve its mapping services, according to OpenAI.Using just 100 examples, Grab reportedly achieved a 20 percent improvement in lane count accuracy and a 13 percent boost in speed limit sign localization.This real-world application demonstrates the possibilities for vision fine-tuning to dramatically enhance AI-powered services across a wide range of industries using small batches of visual training data.Realtime API: Bridging the gap in conversational AIOpenAI also unveiled itsRealtime API, now in public beta. This new offering enables developers to create low-latency, multimodal experiences, particularly in speech-to-speech applications. This means that developers can start adding ChatGPT\u2019s voice controls to apps.To illustrate the API\u2019s potential, OpenAI demonstrated an updated version ofWanderlust, a travel planning app showcased atlast year\u2019s conference.With the Realtime API, users can speak directly to the app, engaging in a natural conversation to plan their trips. The system even allows for mid-sentence interruptions, mimicking human dialogue.While travel planning is just one example, the Realtime API opens up a wide range of possibilities for voice-enabled applications across various industries.From customer service to education and accessibility tools, developers now have a powerful new resource to create more intuitive and responsive AI-driven experiences.\u201cWhenever we design products, we essentially look at like both startups and enterprises,\u201d Godement explained. \u201cAnd so in the alpha, we have a bunch of enterprises using the APIs, the new models of the new products as well.\u201dThe Realtime API essentially streamlines the process of building voice assistants and other conversational AI tools, eliminating the need to stitch together multiple models for transcription, inference, and text-to-speech conversion.Early adopters likeHealthify, a nutrition and fitness coaching app, andSpeak, a language learning platform, have already integrated the Realtime API into their products.These implementations showcase the API\u2019s potential to create more natural and engaging user experiences in fields ranging from healthcare to education.The Realtime API\u2019s pricing structure, while not inexpensive at $0.06 per minute of audio input and $0.24 per minute of audio output, could still represent a significant value proposition for developers looking to create voice-based applications.Model distillation: A step toward more accessible AIPerhaps the most transformative announcement was the introduction of Model Distillation. This integrated workflow allows developers to use outputs from advanced models likeo1-previewandGPT-4oto improve the performance of more efficient models such asGPT-4o mini.The approach could enable smaller companies to harness capabilities similar to those of advanced models without incurring the same computational costs.It addresses a long-standing divide in the AI industry between cutting-edge, resource-intensive systems and their more accessible but less capable counterparts.Consider a small medical technology start-up developing an AI-powered diagnostic tool for rural clinics. Using Model Distillation, the company could train a compact model that captures much of the diagnostic prowess of larger models while running on standard laptops or tablets.This could bring sophisticated AI capabilities to resource-constrained environments, potentially improving healthcare outcomes in underserved areas.OpenAI\u2019s strategic shift: Building a sustainable AI ecosystemOpenAI\u2019s DevDay 2024 marks a strategic pivot for the company, prioritizing ecosystem development over headline-grabbing product launches.This approach, while less exciting for the general public, demonstrates a mature understanding of the AI industry\u2019s current challenges and opportunities.This year\u2019s subdued event contrasts sharply with the 2023 DevDay, which generatediPhone-like excitementwith the launch of the GPT Store and custom GPT creation tools.However, the AI landscape has evolved rapidly since then. Competitors have madesignificant strides, and concerns about data availability for training have intensified. OpenAI\u2019s focus on refining existing tools and empowering developers appears to be a calculated response to these shifts. By improving the efficiency and cost-effectiveness of their models, OpenAI aims to maintain its competitive edge while addressing concerns aboutresource intensityandenvironmental impact.As OpenAI transitions from a disruptor to a platform provider, its success will largely depend on its ability to foster a thriving developer ecosystem.By providing improved tools, reduced costs, and increased support, the company is laying the groundwork for long-term growth and stability in the AI sector.While the immediate impact may be less visible, this strategy could ultimately lead to more sustainable and widespread AI adoption across many industries.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Programming & Development","article_title":"Artisan raises $11.5M to deploy AI \u2019employees\u2019 for sales teams","article_url":"https:\/\/venturebeat.com\/business\/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams\/","article_date":"September 30, 2024 6:00 AM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreArtisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company\u2019s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails.Founded just last year, Artisan has already reached $1 million in annual recurring revenue, with over 120 companies using its platform. The seed round was led by Oliver Jung, with participation from Y Combinator, HubSpot Ventures, Day One Ventures, and others.\u201cWe create AI employees called artisans, and then we consolidate software tools together to create this unified software ecosystem where AI employees are managing and doing your work for you,\u201d said Jaspar Carmichael-Jack, Artisan\u2019s 23-year-old CEO and co-founder, in an interview with VentureBeat.How Artisan\u2019s AI assistant streamlines sales processesArtisan\u2019s approach aims to streamline the fragmented landscape of sales software. Rather than integrating multiple tools, the company offers a single platform that handles tasks ranging from lead generation to email outreach. At the center is Ava, an AI assistant that can operate autonomously to find prospects, research companies, and craft personalized messages.\u201cAva finds leads for people that match their ICP [ideal customer profile]. We have access to over 300 million different B2B lead profiles,\u201d Carmichael-Jack explained. \u201cAva enriches leads using data sources like CrunchBase, Apollo, Cognism\u2026writes emails to the leads and LinkedIn messages, and automates the entire process.\u201dAI\u2019s impact on sales jobs: A shift in rolesCarmichael-Jack acknowledged that AI will likely replace some roles, but argued this shift is ultimately beneficial: \u201cI think there\u2019s going to be a shift from the manual, repetitive, automatable roles to more human centered roles,\u201d he said. \u201cHumans will be shifted to more human activities.\u201dArtisan plans to expand beyond sales, with AI assistants for marketing and customer success in development. The involvement of HubSpot as an investor signals that even established software providers see potential in AI-first approaches.\u201cHubSpot backing us has been like a really meaningful thing to us, because it\u2019s showing that even the legacy software providers are ready for the next paradigm of software to come,\u201d Carmichael-Jack noted.The Future of AI in Business OperationsAs Artisan pushes forward with its AI sales assistants, the line between human and machine in the workplace continues to blur. The question now isn\u2019t whether AI will transform sales, but how quickly.For businesses, the future of sales may be less about closing deals and more about choosing the right digital companion. In this new landscape, the best salesperson might just be the one you never see.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."},{"topic_title":"Programming & Development","article_title":"Cohere updates APIs to make it easier for devs to switch from other models","article_url":"https:\/\/venturebeat.com\/ai\/cohere-updates-apis-to-make-it-easier-for-devs-to-switch-from-other-models\/","article_date":"September 27, 2024 2:47 PM","article_text":"Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreCohere hasannounced the releaseof updated versions of its application programming interfaces (APIs) for its AI models Chat, Embed, Rerank, and Classify.Collectively, the new API updates are known as API V2, and Cohere is being transparent about the fact that the updates are meant to more closely align with AI industry standards to make it easier for developers to switch their applications over to be powered by Cohere\u2019s models in lieu of the competition: namely, OpenAI, Anthropic, Google, Mistral, and Meta.Earlier this month, Andreessen Horowitz (A16z) general partnerMartin Casado posted on Xan image of a graph showing the results of a survey from AI API platformKongof 800 enterprise leaders revealing the large language models (LLMs) they were using.OpenAI\u2019s ChatGPT dominated the chart with 27% market share compared to 18% using Microsoft\u2019s Azure AI cloud service and 17% for Google Gemini. Cohere was second-to-last with a distant 5%, showing how the Toronto-based startup \u2014 co-founded by some of the former Google researchersbehind the original 2017 Transformer paperthat ushered in the generative AI era \u2014\u00a0has a lot of ground to make up to win over the enterprise customers it\u2019s courting.Survey results of nearly 800 enterprise folks on LLM market share (run by Kong). Most notable to me is the dramatic gain in Gemini use. Amazing job by the Alphabet team.pic.twitter.com\/5EZx8IBBUT\u2014 martin_casado (@martin_casado)September 14, 2024Enhanced reliability with more precise settingsOne of the most significant changes in the V2 API release is the requirement for developers to specify the model version in their API calls.Previously, this field was optional, which sometimes led to unexpected behavior when new models were released and the default model changed.By making the model version a mandatory field, Cohere ensures that developers maintain consistent application performance, particularly in scenarios involving Embed models, where using different versions can impact results.The updated Chat API introduces several usability improvements, including the consolidation of input parameters into a singlemessagesarray, replacing the previous structure that required separatemessage,chat_history, andpreambleparameters.This change simplifies the input process, allowing for more complex use cases where roles such assystemorassistantcan be assigned to the latest message in a chat sequence.Improved tool integration and streaming supportCohere\u2019s new APIs also enhance tool integration capabilities. In the V2 release, tools are defined using JSON schema instead of Python types, making the process more flexible and compatible with a wider range of applications.Additionally, each tool call now includes a unique ID, enabling the API to correctly match tool results with their corresponding calls\u2014an improvement over the V1 API, which lacked this feature.For streaming interactions, the V2 Chat API has switched from JSON-stream events to Server Sent Events (SSE), providing a more robust and responsive experience for users.Support for existing APIsCohere has confirmed that the V1 suite of APIs will continue to be supported, ensuring that developers who are not yet ready to migrate can still rely on existing implementations.There will be no breaking changes to the V1 API or its associated SDKs.However, the company recommends upgrading to V2 for enhanced stability and access to the latest features, such as model version enforcement and advanced chat capabilities.Resources for developersTo facilitate the transition to API V2, Cohere has released a new SDK and an OpenAPI specification for its updated endpoint.These resources, along with a detailed Chat Migration Guide, are available on the Cohere platform. Developers are encouraged to provide feedback and suggestions via the company\u2019s Discord community.Cohere\u2019s API V2 release represents a significant step forward in making its platform more accessible and efficient for developers. With these updates, the company aims to offer a more streamlined and predictable development experience, and ultimately, win over users from OpenAI and other popular APIs.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."}]