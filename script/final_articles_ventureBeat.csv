topic_title,article_title,article_url,article_date,article_text
Artificial Intelligence,Cohere updates APIs to make it easier for devs to switch from other models,https://venturebeat.com/ai/cohere-updates-apis-to-make-it-easier-for-devs-to-switch-from-other-models/,"September 27, 2024 2:47 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreCohere hasannounced the releaseof updated versions of its application programming interfaces (APIs) for its AI models Chat, Embed, Rerank, and Classify.Collectively, the new API updates are known as API V2, and Cohere is being transparent about the fact that the updates are meant to more closely align with AI industry standards to make it easier for developers to switch their applications over to be powered by Cohere’s models in lieu of the competition: namely, OpenAI, Anthropic, Google, Mistral, and Meta.Earlier this month, Andreessen Horowitz (A16z) general partnerMartin Casado posted on Xan image of a graph showing the results of a survey from AI API platformKongof 800 enterprise leaders revealing the large language models (LLMs) they were using.OpenAI’s ChatGPT dominated the chart with 27% market share compared to 18% using Microsoft’s Azure AI cloud service and 17% for Google Gemini. Cohere was second-to-last with a distant 5%, showing how the Toronto-based startup — co-founded by some of the former Google researchersbehind the original 2017 Transformer paperthat ushered in the generative AI era — has a lot of ground to make up to win over the enterprise customers it’s courting.Survey results of nearly 800 enterprise folks on LLM market share (run by Kong). Most notable to me is the dramatic gain in Gemini use. Amazing job by the Alphabet team.pic.twitter.com/5EZx8IBBUT— martin_casado (@martin_casado)September 14, 2024Enhanced reliability with more precise settingsOne of the most significant changes in the V2 API release is the requirement for developers to specify the model version in their API calls.Previously, this field was optional, which sometimes led to unexpected behavior when new models were released and the default model changed.By making the model version a mandatory field, Cohere ensures that developers maintain consistent application performance, particularly in scenarios involving Embed models, where using different versions can impact results.The updated Chat API introduces several usability improvements, including the consolidation of input parameters into a singlemessagesarray, replacing the previous structure that required separatemessage,chat_history, andpreambleparameters.This change simplifies the input process, allowing for more complex use cases where roles such assystemorassistantcan be assigned to the latest message in a chat sequence.Improved tool integration and streaming supportCohere’s new APIs also enhance tool integration capabilities. In the V2 release, tools are defined using JSON schema instead of Python types, making the process more flexible and compatible with a wider range of applications.Additionally, each tool call now includes a unique ID, enabling the API to correctly match tool results with their corresponding calls—an improvement over the V1 API, which lacked this feature.For streaming interactions, the V2 Chat API has switched from JSON-stream events to Server Sent Events (SSE), providing a more robust and responsive experience for users.Support for existing APIsCohere has confirmed that the V1 suite of APIs will continue to be supported, ensuring that developers who are not yet ready to migrate can still rely on existing implementations.There will be no breaking changes to the V1 API or its associated SDKs.However, the company recommends upgrading to V2 for enhanced stability and access to the latest features, such as model version enforcement and advanced chat capabilities.Resources for developersTo facilitate the transition to API V2, Cohere has released a new SDK and an OpenAPI specification for its updated endpoint.These resources, along with a detailed Chat Migration Guide, are available on the Cohere platform. Developers are encouraged to provide feedback and suggestions via the company’s Discord community.Cohere’s API V2 release represents a significant step forward in making its platform more accessible and efficient for developers. With these updates, the company aims to offer a more streamlined and predictable development experience, and ultimately, win over users from OpenAI and other popular APIs.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Artificial Intelligence,Here’s how to try Meta’s new Llama 3.2 with vision for free,https://venturebeat.com/ai/heres-how-to-try-metas-new-llama-3-2-with-vision-for-free/,"September 26, 2024 3:51 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreTogether AIhas made a splash in the AI world by offering developersfree accessto Meta’s powerful new Llama 3.2 Vision model via Hugging Face.The model, known asLlama-3.2-11B-Vision-Instruct, allows users to upload images and interact with AI that can analyze and describe visual content.Try Llama 3.2 11B Vision for free in this@huggingfacespace!This model is free in the Together API for the next 3 months.https://t.co/2oYwJK15KWpic.twitter.com/JEh3LTr0M2— Together AI (@togethercompute)September 26, 2024For developers, this is a chance to experiment with cutting-edge multimodal AI without incurring thesignificant costsusually associated with models of this scale. All you need is an API key from Together AI, and you can get started today.This launch underscores Meta’s ambitious vision for the future of artificial intelligence, which increasingly relies on models that can process both text and images—a capability known as multimodal AI.With Llama 3.2, Meta is expanding the boundaries of what AI can do, while Together AI is playing a crucial role by making these advanced capabilities accessible to a broader developer community through afree, easy-to-use demo.Together AI’s interface for accessing Meta’s Llama 3.2 Vision model, showcasing the simplicity of using advanced AI technology with just an API key and adjustable parameters. (Credit: Hugging Face)Unleashing Vision: Meta’s Llama 3.2 breaks new ground in AI accessibilityMeta’s Llama models have been at the forefront of open-source AI development since thefirst versionwas unveiled in early 2023, challenging proprietary leaders like OpenAI’sGPT models.Llama 3.2, launched atMeta’s Connect 2024event this week, takes this even further by integrating vision capabilities, allowing the model to process and understand images in addition to text.This opens the door to a broader range of applications, from sophisticated image-based search engines to AI-powered UI design assistants.The launch of thefree Llama 3.2 Vision demoon Hugging Face makes these advanced capabilities more accessible than ever.Developers, researchers, and startups can now test the model’s multimodal capabilities by simply uploading an image and interacting with the AI in real time.The demo,available here, is powered byTogether AI’s API infrastructure, which has been optimized for speed and cost-efficiency.From code to reality: A step-by-step guide to harnessing Llama 3.2Trying the model is as simple as obtaining afree API keyfrom Together AI.Developers can sign up for an account on Together AI’s platform, which includes$5 in free creditsto get started. Once the key is set up, users can input it into the Hugging Face interface and begin uploading images to chat with the model.The setup process takes mere minutes, and the demo provides an immediate look at how far AI has come in generating human-like responses to visual inputs.For example, users can upload a screenshot of a website or a photo of a product, and the model will generate detailed descriptions or answer questions about the image’s content.For enterprises, this opens the door to faster prototyping and development of multimodal applications. Retailers could use Llama 3.2 to power visual search features, while media companies might leverage the model to automate image captioning for articles and archives.The bigger picture: Meta’s vision for edge AILlama 3.2 is part of Meta’s broader push into edge AI, where smaller, more efficient models can run on mobile and edge devices without relying on cloud infrastructure.While the11B Vision modelis now available for free testing, Meta has also introduced lightweight versions with as few as 1 billion parameters, designed specifically for on-device use.These models, which can run on mobile processors fromQualcommandMediaTek, promise to bring AI-powered capabilities to a much wider range of devices.In an era where data privacy is paramount, edge AI has the potential to offer more secure solutions by processing data locally on devices rather than in the cloud.This can be crucial for industries like healthcare and finance, where sensitive data must remain protected. Meta’s focus on making these models modifiable and open-source also means that businesses can fine-tune them for specific tasks without sacrificing performance.Beyond the cloud: Meta’s bold push into edge AI with Llama 3.2Meta’scommitment to opennesswith the Llama models has been a bold counterpoint to the trend of closed, proprietary AI systems.With Llama 3.2, Meta is doubling down on the belief that open models can drive innovation faster by enabling a much larger community of developers to experiment and contribute.In a statement at the Connect 2024 event, Meta CEO Mark Zuckerberg noted that Llama 3.2 represents a “10x growth” in the model’s capabilities since its previous version, and it’s poised to lead the industry in both performance and accessibility.Together AI’s role in this ecosystem is equally noteworthy. By offering free access to the Llama 3.2 Vision model, the company is positioning itself as a critical partner for developers and enterprises looking to integrate AI into their products.Together AI CEO Vipul Ved Prakash emphasized that their infrastructure is designed to make it easy for businesses of all sizes to deploy these models in production environments, whether in the cloud or on-prem.The future of AI: Open access and its implicationsWhile Llama 3.2 is available for free on Hugging Face, Meta and Together AI are clearly eyeing enterprise adoption.The free tier is just the beginning—developers who want to scale their applications will likely need to move to paid plans as their usage increases. For now, however, the free demo offers a low-risk way to explore the cutting edge of AI, and for many, that’s a game-changer.As the AI landscape continues to evolve, the line between open-source and proprietary models is becoming increasingly blurred.For businesses, the key takeaway is that open models like Llama 3.2 are no longer just research projects—they’re ready for real-world use. And with partners like Together AI making access easier than ever, the barrier to entry has never been lower.Want to try it yourself? Head over toTogether AI’s Hugging Face demoto upload your first image and see what Llama 3.2 can do.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Artificial Intelligence,Google says Gemini-powered automations coming to Workspace next month,https://venturebeat.com/automation/google-says-gemini-powered-automations-coming-to-workspace-next-month/,"September 26, 2024 3:10 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreAfter making manyGemini AIfeatures standard onGoogle Workspace, Google executives said it’s a step towards bringing AI agents to bear at scale across many organizations that use its cloud productivity platform (which includes popular enterprise apps such as Google Drive, Gmail, Google Calendar, Google Meet, Google Sheets, Google Slides, etc).Google Workspace is used by at least 8 million paying customers and commands a staggering 84.95% of the cloud work apps market, according toThrive, an online marketing agency.Aparna Pappu, vice president and general manager of Google Workspace, said Google has taken a “crawl, walk, run” approach to bringing AI agents to more users.“You can think of bringing agents as a series of building blocks where people are training their assistants to learn how they work,” Pappu said during a Q&A with reporters and analysts at a Gemini at Work event in New York Thursday. “We’re working toward a set of trained assistants that lead to agents.”Googleannounced earlier this weekthat the standalone Gemini chat app — powered by its Gemini AI model — is now integrated into Workspace for Business, Enterprise and Frontline paid accounts.Gemini for Workspace allows people to ask Gemini to summarize emails on Gmail, find information stored on multiple documents on Google Drive or write a natural language prompt on Sheets to generate a custom chart.Unlike other platforms that target specific workflows, such assales and marketing, which are releasing AI agents, Google Workspace reaches a wide and diverse audience.Pappu noted that Google has been considering agents on Workspace for a while now. She pointed to Chip, the AI Teammatedemoed during Google’s I/Odeveloper conference in May.“We introduced AI Teammates at Google I/O which is an AI coworker that does a specific task. We want to eventually do this in scale in an enterprise which we believe Workspace is helping enable,” Pappu said.Google also plans to slowly roll out Gemini-powered workflow automations on Workspace beginning in October.Workflow automation would let users set automatic tasks that automatically read an email or document, categorize it, and do an action.For example, if a user receives an email with an invoice, Gemini will know it’s related to finance and budgeting and then bring the invoice to the appropriate team for payment.That type of workflow orchestration is also one of the building blocks for AI agents.Agents for more usersAI agentshave become a touchpoint for many companies dealing with employee productivity. In the past month alone, several companies released AI agents or connections for AI agents.Slack added integrationfor AI agents from Salesforce,Asana,Cohere,Workday,Adobe ExpressandWriter.ServiceNowalso released its ownAI agents for customers.Meta also said it will add functionality forusers to build agentson WhatsApp or Messenger to answer questions.It’s not a surprise that a large organization like Google will offer access to AI agents to its many customers.Many businesses, small or large enterprises, do use Google’s Workspace suite of email, documents and spreadsheets along with other Google Cloud products that connect to customer management systems and the like.Google is betting on AI-powered productivityGoogle’s Gemini at Work event aimed to show how much the company’s flagship AI models have saved time for its customers.“We’re starting to hear from customers that they’re seeing more employee retention and even happiness because they use Gemini,” Pappu said. “It’s really more than just the time saved, but also about taking away the small annoying tasks.”Google said customers who use Gemini—in Workspace or otherwise—have been able to be more productive and cut down on time usually spent on tedious tasks like coding translations.Pappu said Gemini has pushed more businesses to be more competitive, even if they are still small because Gemini offers expertise without needing consultants.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Artificial Intelligence,Why countries are in a race to build AI factories in the name of sovereign AI,https://venturebeat.com/ai/why-countries-are-in-a-race-to-build-ai-factories-in-the-name-of-sovereign-ai/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreNow that AI has become a fundamentally important technology, and the world has gravitated toward intense geopolitical battles, it’s no wonder that “sovereign AI” is becoming a national issue.Think about it. Would the U.S. allow the data it generates for AI to be stored and processed in China? Would the European Union want its people’s data to be accessed by big U.S. tech giants? Would Russia trust NATO countries to manage its AI resources? Would Muslim nations entrust their data for AI to Israel?Nvidia has earmarked $110 million to help countries foster AI startups to invest in sovereign AI infrastructure, and plenty of countries are investing in AI infrastructure on their own. That’s some real money aimed at jumpstarting the world when it comes to embracing AI. The question becomes whether this discussion is a lot of thought leadership to enable a sales pitch, or whether nations truly need to embrace sovereign AI to be competitive with the rest of the world. Is it a new kind of arms race that makes sense for nations to pursue?A wake-up callDigital rendering of Nvidia’s Jensen HuangJensen Huang, CEO of Nvidia, pointed out the rise of “sovereign AI” during an earnings call in November 2023 as a reason for why demand is growing for Nvidia’s AI chips. The company noted that investment in national computer infrastructure was a new priority for governments around the world.“The number of sovereign AI clouds is really quite significant,” Huang said in the earnings call. He said Nvidia wants to enable every company to build its own custom AI models.The motivations weren’t just about keeping a country’s data in local tech infrastructure to protect it. Rather, they saw the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation, said Colette Kress, CFO of Nvidia, in the earnings call.That was around the time when the Biden administration was restricting sales of the most powerful AI chips to China, requiring a license from the U.S. government before shipments could happen. That licensing requirement is still in effect.As a result, China reportedly began its own attempts to create AI chips to compete with Nvidia’s. But it wasn’t just China. Kress also said Nvidia was working with the Indian government and its large tech companies like Infosys, Reliance and Tata to boost their “sovereign AI infrastructure.”Meanwhile, French private cloud provider Scaleway was investing in regional AI clouds to fuel AI advances in Europe as part of a “new economic imperative,” Kress said. The result was a “multi-billion dollar opportunity” over the next few years, she said.Huang said Sweden and Japan have embarked on creating sovereign AI clouds.“You’re seeing sovereign AI infrastructures, people, countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data, and develop their own AI. You see that in India,” Huang said.He added, “Sovereign AI clouds coming up from all over the world as people realize that they can’t afford to export their country’s knowledge, their country’s culture for somebody else to then resell AI back to them.”Nvidia itself definessovereign AIas “a nation’s capabilities to produce artificial intelligence using its own infrastructure, data, workforce and business networks.”Keeping sovereign AI secureCredit: VentureBeat using DALL-EIn an interview with VentureBeat in February 2024,Huangdoubled down on the concept, saying, “We now have a new type of data center that is about AI generation, an AI generation factory. And you’ve heard me describe it as AI factories. Basically, it takes raw material which is data, transforms it with these AI supercomputers and Nvidia builds and it turns them into incredibly valuable tokens. These tokens are what people experience on the amazing” generative AI platforms like Midjourney.I asked Huang why, if data is kept secure regardless of its location in the world, does sovereign AI need to exist within the borders of any given country.He replied, “There’s no reason to let somebody else come and scrape your internet, take your history, your data. And a lot of it is still locked up in libraries. In our case, it’s Library of Congress. In other cases, national libraries. And they’re digitized, but they haven’t been put on the internet.”He added, “And so people are starting to realize that they had to use their own data to create their own AI, and transform their raw material into something of value for their own country, by their own country. And so you’re going to see a lot. Almost every country will do this. And they’re going to build the infrastructure. Of course, the infrastructure is hardware. But they don’t want to export their data using AI.”The $110 million investmentShilpa Kolhatkar (left) of Nvidia speaks with Jon Metzler of U.C. Berkeley.Nvidia has earmarked $110 million to invest in AI startups helping with sovereign AI projects and other AI-related businesses.Shilpa Kolhatkar, global head of AI Nations at Nvidia, gave a deeper dive on sovereign AI at theU.S.-Japan Innovation Symposiumat Stanford University. The July event was staged by the Japan Society of Northern California and the Stanford US-Asia Technology Management Center.Kolhatkar did the interview with Jon Metzler, a continuing lecturer at the Haas School of Business at the University of California, Berkeley. That conversation focused on how to achieve economic growth through investments in AI technology. Kolhatkar noted how Nvidia has transformed itself from a graphics company to a high-performance computing and AI company long before ChatGPT arrived.“Lots of governments around the world are looking today at how can they capture this opportunity that AI has presented and they [have focused] on domestic production of AI,” Kolhatkar said. “We have the Arab nations program, which kind of matches the AI strategy that nations have in place today. About 60 to 70 nations have an AI strategy in place, built around the major pillars of creating the workforces and having the ecosystem. But it’s also around having already everything within the policy framework.”AI readiness?Examples of generative AI by Getty Images.Nvidia plays a role in setting up the ecosystem and infrastructure, or supercomputers. The majority of Nvidia’s focus and its engineering efforts is in the software stack on top of the chips, she said. As a result, Nvidia has become more of a platform company, rather than a chip company. Metzler asked Kolhatkar to define how a country might develop “AI readiness.”Kolhatkar said that one notion is to look at how much computing power a country has, in terms of raw AI compute, storage and the energy related to power such systems. Does it have a skilled workforce to operate the AI? Is the population ready to take advantage of AI’s great democratization so that the knowledge spreads well beyond data scientists?When ChatGPT-3.5 emerged in Nov. 2022 and generative AI exploded, it signaled that AI was really finally working in a way that ordinary consumers could use to automate many tasks and find new information or create things like images on their own. If there were errors in the results, it could be because the data model wasn’t fed the correct information. Then it quickly followed that different regions had their own views on what was considered correct information.“That model was trained primarily on a master data set and a certain set of languages in western [territories],” Kolhatkar said. “That is why the internationalization of having something which is sovereign, which is specific to a nation’s own language, culture and nuances, came to the forefront.”Then countries started developing generative AI models that cater to the specificities of a particular region or particular nation, and, of course, the ownership of that data, she said.“The ownership is every country’s data and proprietary data, which they realized should stay within the borders,” she said.AI factoriesNvidia’s notion of AI factories.Nvidia is now in the process of helping countries create such sovereign infrastructure in the form of “AI factories,” Kolhatkar said. That’s very similar to the drive that nations ignited with factories during the Industrial Revolution more than 100 years ago.“Factories use raw materials that go in and then goods come out and that was tied to the domestic GDP. Now the paradigm is that your biggest asset is your data. Every nation has its own unique language and data. That’s the raw material that goes into the AI factory, which consists of algorithms, which consists of models and out comes intelligence,” she said.Now countries like Japan have to consider whether they’re ahead or falling behind when it comes to being ready with AI factories. Kolhatkar said that Japan is leading the way when it comes to investments, collaborations and research to create a successful “AI nation.”She said companies and nations are seriously considering how much of AI should be classified as “critical infrastructure” for the sake of economic or national security. Where industrial factories could create thousands of jobs in a given city, now data centers can create a lot of jobs in a given region as well. Are these AI factories like the dams and airports of decades ago?“You’re kind of looking at past precedents from physical manufacturing as to what the multiplier might be for AI factories,” Metzler said. “The notion of AI factories as maybe civic infrastructure is super interesting.”National AI strategies?Cerebras Condor Galaxy at Colovore Data CenterMetzler brought up the notion of the kind of strategies that can happen when it comes to the AI race. For instance, he noted that maybe smaller countries need to team up to create their own larger regional networks, to create some measure of sovereignty.Kolhatkar said that can make sense if your country, for instance, doesn’t have the resources of any given tech giant like Samsung. She noted the Nordic nations are collaborating with each other, as are nations like the U.S. and Japan when it comes to AI research. Different industries or government ministries can also get together for collaboration on AI.If Nvidia is taking a side on this, it’s in spreading the tech around so that everyone becomes AI literate. Nvidia has an online university dubbed the Deep Learning Institute for self-paced e-learning courses. It also has a virtual incubatorNvidia Inception, which has supported more than19,000 AI startups.“Nvidia really believes in democratization of AI because the full potential of AI can not be achieved unless everybody’s able to use it,” Kolhatkar said.Energy consumption?AI power consumptionAs for dealing with the fallout of sovereign AI, Metzler noted that countries will have to deal with sustainability issues in terms ofhow much power is being consumed.In May, theElectric Power Research Institute(EPRI) released awhitepaperthat quantified the exponential growth potential of AI power requirements. It projected that total data center power consumption by U.S. data centers alone could more than double to 166% by 2030.It noted that each ChatGPT request can consume 2.9 watt-hours of power. That means AI queries are estimated to require 10 times the electricity of traditional Google queries, which use about 0.3 watt-hours each. That’s not counting emerging, computation-intensive capabilities such as image, audio and video generation, which don’t have a comparision precedent.EPRI looked at four scenarios. Under the highest growth scenario, data center electricity usage could rise to 403.9 TWh/year by 2030, a 166% increase from 2023 levels. Meanwhile, the low growth scenario projected a 29% increase to 196.3 TWh/year.“It’s about the energy efficiency, sustainability is pretty top of mind for everyone,” Kolhatkar said.Nvidia is trying to make each generation of AI chip more power efficient even as it makes each one more performant. She also noted the industry is trying to create and use sources of renewable energy. Nvidia also uses its output from AI, in the form of Nvidia Omniverse software, to create digital twins of data centers. These buildings can be architected with energy consumption in mind and with the notion of minimizing duplicative effort.Once they’re done, the virtual designs can be built in the physical world with a minimum of inefficiency. Nvidia is even creating a digital twin of the Earth to predict climate change for decades to come. And the AI tech can also be applied to making physical infrastructure more efficient, like making India’s infrastructure more resistant to monsoon weather. In these ways, Kolhatkar thinks AI can be used to “save the world.”She added, “Data is the biggest asset that a nation has. It has your proprietary data with your language, your culture, your values, and you are the best person to own it and codify it into an intelligence that you want to use for your analysis. So that is what sovereignty is. That is at the domestic level. The local control of your assets, your biggest asset, [matters].”A change in computing infrastructureNvidia Blackwell has 208 billion transistors.Computers, of course, don’t know national borders. If you string internet cables around the world, the information flows and a single data center could theoretically provide its information on a global basis. If that data center has layers of security built in, there should be no worry about where it’s located. This is the notion of the advantage of computers of creating a “virtual” infrastructure.But these data centers need backups, as the world has learned that extreme centralization isn’t good for things like security and control. A volcanic eruption in Iceland, a tsunami in Japan, an earthquake in China,  a terrorist attack on infrastructure or possible government spying in any given country — these are all reasons for having more than one data center to store data.Besides disaster backup, national security is another reason driving each country to require their own computing infrastructure within their borders. Before the generative AI boom, there was a movement to ensure data sovereignty, in part because some tech giants overreached when it came to disintermediating users and their applications that developed personalized data. Data best practices resulted.Roblox CEO Dave Baszucki said at the Roblox Developer Conference that his company operates a network of 27 data centers around the world to provide the performance needed to keep its game platform operating on different computing platforms around the world. Roblox has 79.5 million daily active users who are spread throughout the world.Given that governments around the world are coming up with data security and privacy laws, Roblox might very well have to change its data center infrastructure so that it has many more data centers that are operating in given jurisdictions.There are 195 nation states in the world, and if the policies become restrictive, a company might conceivably need to have 195 data centers. Not all of these divisions are parochial. For instance, some countries might want to deliberately reduce the “digital divide” between rich nations and poor ones, Kolhatkar said.There’s another factor driving the decentralization of AI — the need for privacy. Not only for the governments of the world, but also for companies and people. The celebrated “AI PC” trend of 2024 offers consumers personal computers with powerful AI tech to ensure the privacy of operating AI inside their own homes. This way, it’s not so easy for the tech giants to learn what you’re searching for and the data that you’re using to train your own personal AI network.Do we need sovereign AI?Nvidia humanoid robots.Huang suggested that countries perceive it as needed so that a large language model (LLM) can be built with knowledge of local customs. As an example, Chernobyl is spelled with an “e” in Russian. But in Ukraine, it’s spelled “Chornobyl.” That’s just a small example of why local customs and culture need to be taken into account for systems used in particular countries.Some people are concerned about the trend as it drives the world toward more geographic borders, which in the case of computing, really don’t or shouldn’t exist.Kate Edwards, CEO of Geogrify and an expert on geopolitics in the gaming industry, said in a message, “I think it’s a dangerous term to leverage, as ‘sovereignty’ is a concept that typically implies a power dynamic that often forms a cornerstone of nationalism, and populism in more extreme forms. I get why the term is being used here but I think it’s the wrong direction for how we want to describe AI.”She added, “‘Sovereign’ is the wrong direction for this nomenclature. It instantly polarizes what AI is for, and effectively puts it in the same societal tool category as nuclear weapons and other forms of mass disruption. I don’t believe this is how we really want to approach this resource, especially as it could imply that a national government essentially has an enslaved intelligence whose purpose is to reinforce and serve the goals of maintaining a specific nation’s sovereignty — which is the basis for the great majority of geopolitical conflict.”Are countries taking Nvidia’s commentary seriously or do they view it as a sales pitch? Nvidia isn’t the only company succeeding with the pitch.AMD competes with Nvidia in AI/graphics chips as well as CPUs. Like Nvidia, it is seeing an explosion in demand for AI chips. AMD also continues to expand its efforts in software, with the acquisition of AI software firms like Nod.AI and Silo AI. AI is consistently driving AMD’s revenues and demand for both its CPUs and GPUs/AI chips.Cerebras WSE-3Cerebras Systems, for instance,announcedin July 2023 that it was shipping its giant wafer-size CPUs to the technology holding groupG42, which was building the world’s largest supercomputer for AI training, namedCondor Galaxy, in the United Arab Emirates.It started with a network of nine interconnected supercomputers aimed at reducing AI model training time significantly, with a total capacity of 36 exaFLOPs, thanks to the first AI supercomputer on the network, Condor Galaxy 1 (CG-1), which had 4 exaFLOPs and 54 million cores, said Andrew Feldman, CEO ofCerebras, in an interview with VentureBeat. Those computers were based in the U.S., but they are being operated by the firm in Abu Dhabi. (That raises the question, again, of whether sovereign AI tech has to be located in the country that uses the computing power).NowCerebras has broken groundon a new generation of Condor Galaxy supercomputers for G42.Rather than make individual chips for its centralized processing units (CPUs), Cerebras takes entire silicon wafers and prints its cores on the wafers, which are the size of pizza. These wafers have the equivalent of hundreds of chips on a single wafer, with many cores on each wafer. And that’s how they get to 54 million cores in a single supercomputer.Feldman said, “AI is not just eating the U.S. AI is eating the world. There’s an insatiable demand for compute. Models are proliferating. And data is the new gold. This is the foundation.”VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Artificial Intelligence,AI’s middle layer still needs powerful hardware,https://venturebeat.com/ai/ais-middle-layer-still-needs-powerful-hardware/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThis article is part of a VB Special Issue called “Fit for Purpose: Tailoring AI Infrastructure.”Catch all the other stories here.With more enterprises looking to build more AI applications or even AI agents, it’s becoming increasingly clear that organizations should use different language models and databases to get the best results.However, switching an application from Llama 3 to Mistral in a flash may take a bit of technology infrastructure finesse. This is where thecontext and orchestration layercomes in; the so-called middle layer that connects foundation models to applications will ideally control the traffic of API calls to models to execute tasks.The middle layer mainly consists of software likeLangChainorLlamaIndexthat help bridge databases, but the question is, will the middle layer solely consist of software, or is there a role hardware can still play here beyond powering much of the models that power AI applications in the first place.The answer is that hardware’s role is tosupport frameworks like LangChainand the databases that bring applications to life. Enterprises need to have hardware stacks that can handle massive data flows and even look at devices that can do a lot of data center work on device.>>Don’t miss our special issue:Fit for Purpose: Tailoring AI Infrastructure.<<“While it’s true that the AI middle layer is primarily a software concern, hardware providers can significantly impact its performance and efficiency,” said Scott Gnau, head of data platforms at data management companyInterSystems.Many AI infrastructure experts told VentureBeat that while software underpins AI orchestration, none would work if the servers andGPUscould not handle massive data movement.In other words, for the software AI orchestration layer to work, the hardware layer needs to be smart and efficient, focusing on high-bandwidth, low-latency connections to data and models to handle heavy workloads.“This model orchestration layer needs to be backed with fast chips,” said Matt Candy, managing partner of generative AI atIBM Consulting, in an interview. “I could see a world where the silicon/chips/servers are able to optimize based on the type and size of the model being used for different tasks as the orchestration layer is switching between them.”Current GPUs, if you have access, will already workJohn Roese, global CTO and chief AI officer atDell, told VentureBeat that hardware like the ones Dell makes still has a role in this middle layer.“It’s both a hardware and software issue because the thing people forget about AI is that it appears as software,” Roese said. “Software always runs on hardware, and AI software is the most demanding we’ve ever built, so you have to understand the performance layer of where are the MIPs, where is the compute to make these things work properly.”This AI middle layer may need fast, powerful hardware, but there is no need for new specialized hardware beyond the GPUs and other chips currently available.“Certainly, hardware is a key enabler, but I don’t know that there’s specialized hardware that would really move it forward, other than the GPUs that make the models run faster, Gnau said. “I think software and architecture are where you can optimize in a kind fabric-y way the ability to minimize data movement.”AI agents make AI orchestration even more importantThe rise of AI agents has made strengthening the middle layer even more critical. When AI agents start talking to other agents and doing multiple API calls, the orchestration layer directs that traffic and fast servers are crucial.“This layer also provides seamless API access to all of the different types of AI models and technology and a seamless user experience layer that wraps around them all,” said IBM’s Candy. “I call it an AI controller in this middleware stack.”AI agentsare the current hot topic for the industry, and they will likely influence how enterprises build a lot of their AI infrastructure going forward.Roese added another thing enterprises need to consider:on-device AI, another hot topic in the space. He said companies will want to imagine when their AI agents will need to run locally because the old internet may go down.“The second thing to consider is where do you run?” Roese said. “That’s where things like the AI PC comes into play because the minute I have a collection of agents working on my behalf and they can talk to each other, do they all have to be in the same place.”He added Dell explored the possibility of adding “concierge” agents on device “so if you’re ever disconnected from the internet, you can continue doing your job.”Explosion of the tech stack now, but not alwaysGenerative AI has allowed the expansion of the tech stack, as more tasks became more abstracted, bringing new service providers offering GPU space, new databases orAIOpsservices. This won’t be the case forever, saidUniphoreCEO Umesh Sachdev, and enterprises must remember that.“The tech stack has exploded, but I do think we’re going to see it normalize,” said Sachdev. “Eventually, people will bring things in-house and the capacity demand in GPUs will ease out. The layer and vendor explosion always happens with new technologies and we’re going to see the same with AI.”For enterprises, it’s clear that thinking about the entire AI ecosystem, from software to hardware, is the best practice for AI workflows that make sense.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Security,"With 23andMe in crisis, strengthening DNA security has never been more urgent",https://venturebeat.com/security/with-23andmes-directors-quitting-your-data-is-at-risk-time-to-double-down-on-identity-securitywith-23andmes-directors-quitting-your-data-is-at-risk-time-to-double-down-on-identity-security/,"September 24, 2024 6:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreWith all seven independent directorsresigningfrom23andMelast week, the company has become a cautionary tale of why cybersecurity is a business decision for any enterprise first, as there are immediate and lasting impacts to any organization ignoring that.  Customers aren’t sure how the company plans to strengthen its security and protect their DNA and other confidential personally identifiable information (PII). Enterprises can’t afford to allow security to become a liability.Multiple large-scalesecurity breacheshave jilted existing customers’ confidence and made potential customers think twice about sharing their DNA data with 23andMe.The independent board members unanimously resigned in response to CEO Anne Wojcicki’s push to take the company private onSept. 17. The resignation states that they haven’t seen progress on an actionable plan for taking the company private that benefits all shareholders.The independent directors also cite differences of opinion with Wojcicki on the company’s future direction and believe it’s best to resign instead of fueling potential internal conflict.23andMe’s leadership crisis further jeopardizes DNA securityIt’s rare to see an entire board resign at once. That signals a fundamental disconnect between how the board and senior management see the future of the business. 23andMe can’t afford a disconnect between identity and access management (IAM) and privileged access management (PAM), improving their security infrastructure and ensuring a more robust security posture. Now would be a perfect time to reinvent themselves from a security standpoint, protecting customers’ identities and their DNA data.DNA data provides the most permanent personal data there is, exposing victims of identity attacks based on the data to a lifetime of potential liability. As Tina Srivastava, co-founder ofBadge, told VentureBeat in a recent interview, “With 23andMe and DNA, you can’t reset it, you can’t change it if it’s compromised. It’s like a one-and-done situation. It’s not revocable. What Badge does is that we eliminate the storage of biometric data.”David Aronchick, CEO ofExpansotold VentureBeat that “one of the fundamental challenges for 23andMe is that while they possess an enormous amount of sensitive genetic data, they may not be fully equipped to extract its maximum value internally, especially without extensive research facilities.” Aronchick added that “traditionally, sharing this data with external parties has involved allowing downloads and trusting third parties to handle it responsibly—a method fraught with security risks – especially because the only way to enforce good behavior of the data is legally and with deep audits.” He said 23andMe would struggle with the scale the solution approach would require.Merritt Baer, CISO atRecotold VentureBeat in a recent interview, “Identity security isn’t just a technical issue, it’s a fundamental component of corporate trust between a company and its users. When executive leadership is in flux, the entire organization is exposed to questions around how an entity will enforce both the strategic and the tactical behaviors that customers need to see”.Financial instability is amplifying security concernsFor its first quarter of fiscal year 2025 (FY25), which ended June 30, 2024, 23andMe reported a34%year-over-year revenue decline, dropping from $61 million to $40 million. The steep decline was influenced by the termination of its partnership with GSK and a drop in personal genetic services (PGS) sales.Despite some improvement in adjusted EBITDA, the company’s net losses were still significant at$69 millionfor the quarter. Their struggling research business contributes to a multimillion-dollar loss, known for being exceptionally expensive yet failing to deliver substantial revenue, as their quarterly results show.CNN reportsthat last month, 23andMe shuttered its internal drug research group.With only $170 million in cash left, 23andMe faces a significant cash burn. It will need to raise additional funds and consider an acquisition or an investment from private equity firms pursuing healthcare. The Wall Street Journal recentlywrote, “23andMe has never made a profit and is burning cash so quickly it could run out next year.” 23andMe also announced a telehealth platform, Lemonaid, selling weekly injections of compounded semaglutide, the active ingredient in Wegovy and Ozempic, through a new subscription product in an attempt to capitalize on the popularity of GLP-1 medications for weight loss, according to the WSJ.Private equity firms are known for the depth of their due diligence before investing in or acquiring companies, often drilling down into the security infrastructure and tech stack. Given 23andMe’s distressed financial state, chances are it’s already on the acquisition radar of private equity firms.Their ongoing security vulnerabilities may further reduce the company’s valuation, making it more attractive to private equity firms looking for distressed assets. Any future breaches would likely compound the company’s financial instability and purchase price.23andMe’s new board needs to include at least one CISO from healthcare who knows how to protect healthcare data and is familiar with the many compliance requirements and laws in that industry.Baer remarked on the core challenges facing 23andMe’s board from a CISO perspective. “The board should be an accountability mechanism for the company— not just when it is convenient. The entire value proposition of 23andMe resides in the idea that folks will buy a genetic testing kit, but that was a questionable hypothesis (what happens after you buy it once? Your genes don’t change). Now it’s a questionable proposition because it relies on a presumption of trust—one that feels unreliable.”23andMe is an appealing private equity buyDespite its challenges, 23andMe’s massive base of genetic data based on over 12 million kits being sold combined with the work it’s been doing with healthcare professionals, medical researchers and the scientific community make it an appealing target for private equity firms.The company’s current market capitalization is $170 million, with an enterprise value of approximately $69 million. Private equity firms with substantial investments in healthcare technology and services providers include Blackstone who recentlyacquired Ancestry, KKR and TPG. Each of these firms and others potentially see the company’s condition and challenges as an opportunity to acquire 23andMe at a discount.The sale of 23andMe to an offshore private equity firm would raise significant concerns about U.S. citizens’ genetic data security. When VentureBeat asked industry leaders, including Srivastava for their perspective on a foreign buyer acquiring 23andMe, she said, “And I hope that given the national security implications of this, we don’t allow this to be given over, like you said to foreign parties that don’t respect the privacy of Americans.”Eric Chien, Fellow, Symantec Threat Hunter Team at Broadcom, stressed the importance of a few things when VentureBeat interviewed him recently. The major one is “knowing who has access to that data and the chain of custody.” Without these safeguards, 23andMe’s sensitive data could be at risk of exploitation, further complicating any potential sale.“This is a fairly unique situation (all of the independent directors resigned), but it’s emblematic of other issues in governance, trust, security and the damage to the company when external and internal folks lose confidence,” Baer told VentureBeat.Attackers after DNA data also targeted ethnic groupsIn October 2023, 23andMe suffered a significant data breach due tocredential stuffing attacks, where hackers used login details obtained from other breaches to access user accounts. The breach compromised the personal and genetic data of nearly 7 million individuals. The information exposed included names, birth years and ancestry data from 5.5 million customers using the“DNA Relatives”feature and 1.4 million users using the “Family Tree”feature.One of the most alarming breaches of identities ever was the specific targeting ofunique demographic groups, including 1 millionAshkenazi Jewsand anyone in the 23AndMe data set ofChinese descent. Attackers were quick to leak the breached DNA data on BreachForums and Reddit. Attackers also breached exposed raw genotype data, raising concerns about the potential misuse of genetic information for blackmail, unauthorized genetic research, or employment and insurance discrimination​.23andMe delayed tellingAshkenazi Jews and Chinese that their data had been stolen. As a result, in January 2024, the company faced a class-action lawsuit accusing it of failing to protect sensitive genetic data adequately. The lawsuit was settled this month for $30 million, which included compensation for affected customers and commitments to strengthening cybersecurity measures.“With great power comes great responsibility. 23andme plays in a space that they knew— or should have known— was extremely sensitive. And they are paying a settlement that responds to a suit specifically related to their failure to exercise enough security protection for the targeted attack against customers with Chinese or Ashkenazi Jewish ancestry,” Baer told VentureBeat.Despite the settlement, 23andMe denied wrongdoing but agreed to implement additional security protocols, such as mandatory two-factor authentication and annual cybersecurity audits, to prevent similar incidents​.The company continues to face lawsuits, including one where theyattempted to deflect blameby telling users that hackers took advantage of recycled credentials.Where 23andMe needs to startDNA is by far the most potent form of identity data that exists. 23andMe’s initial efforts at MFA and audits don’t go far enough. However, with adversarial AI challenging MFA’s reliability more and more, the company has to reinvent itself significantly from a security standpoint as it attempts to expand into therapeutics and clinical trials.Here are five suggestions of where to start:Audit all access credentials and delete any accounts that aren’t being used now: A comprehensive audit of all access credentials is essential to eliminating “zombie credentials,” asIvanti’sCPO, Srinivas Mukkamala told VentureBeat, “Large organizations often fail to account for the huge ecosystem of apps, platforms and third-party services that grant access well past an employee’s termination. We call these zombie credentials, and a shockingly large number of security professionals — and even leadership-level executives — still have access to former employers’ systems and data.” Given 23andMe’s history of breaches, this is an excellent place to start.Thoroughly audit how new accounts are created and start auditing every account with admin privileges.Attackers look to take over the new account creation process first, especially for admin privileges, because that gives them the control surface they need to take over the entire infrastructure. Many of the longest-dwelling breaches happened because attackers could use admin privileges to deactivate entire systems’ accounts and detection workflows to shut down attempts at discovering their breach.Passwordless is the future, so start planning for it now.23andMe’s senior management needs to consider moving away from passwords and adopting a zero-trust approach to identity security.Gartner predictsthat by 2025, 50% of the workforce and 20% of customer authentication transactions will be passwordless. Leading passwordless authentication providers include Ivanti’s Zero Sign-On (ZSO) solution, Microsoft Azure Active Directory (Azure AD), OneLogin Workforce Identity, Thales SafeNet Trusted Access and others. Ivanti’s Zero Sign-On (ZSO) solution is among the most versatile solutions, combining passwordless authentication, zero trust and a simplified user experience while supporting biometrics, including Apple’s Face ID.Verify every machine and human identity before granting access to any resources.One of the core concepts of zero trust is least privileged access. 23andMe needs to enforce it for every machine and human identity before granting access. That means current methods of password authentication and how customers can traverse family trees and DNA Relative structures need to be more hardened against lateral movement.Get a quick win in microsegmentation by not allowing the implementation to drag on.Microsegmentation is a security strategy to divide networks into smaller, isolated segments. It’s proven effective in reducing the size and vulnerability of an attack surface, allowing organizations to identify and isolate any suspicious activity on their networks quickly. Microsegmentation is a crucial component ofzero trust, as outlined in theNIST’s zero-trust framework.The path forward“In light of the current boardroom issues, establishing robust protocols for data governance is crucial. For instance, in the event of bankruptcy or significant organizational changes, the data could remain protected within a secure vault, accessible only under strict oversight by appointed custodians,” Aronchick advised VentureBeat.The challenges facing 23andMe go beyond financial losses and security failures. With leadership in flux and the company’s future uncertain, it must act swiftly to modernize its IAM infrastructure and secure its data assets.As their efforts to reinvent themselves from a security standpoint go, so will the success or failure of their efforts to regain investor confidence and prevent further breaches. The consequences of inaction are clear: delays in securing its systems could invite additional cyberattacks, eroding shareholder value and further endangering its financial stability.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Security,OpenAI tackles global language divide with massive multilingual AI dataset release,https://venturebeat.com/ai/openai-tackles-global-language-divide-with-massive-multilingual-ai-dataset-release/,"September 23, 2024 5:33 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreOpenAI took a major step toward expanding the global reach of artificial intelligence by releasing a multilingual dataset that evaluates the performance of language models across 14 languages, including Arabic, German, Swahili, Bengali and Yoruba.The company shared theMultilingual Massive Multitask Language Understanding (MMMLU) dataseton the open data platform Hugging Face. This new evaluation builds on the popularMassive Multitask Language Understanding (MMLU) benchmark, which tested an AI system’s knowledge across 57 disciplines from mathematics to law and computer science, but only in English.By incorporating a diverse array of languages into the new multilingual evaluation, some of which have limited resources for AI training data, OpenAI set a new benchmark for multilingual AI capabilities. This benchmark could open up more equitable global access to the technology. The AI industry has faced criticism for its inability to develop language models that can understand languages spoken by millions of people worldwide.OpenAI delivers global benchmark for evaluating multilingual AIThe MMMLU dataset challenges AI models to perform in diverse linguistic environments, reflecting the growing need for AI systems that can engage with users across the globe. As businesses and governments increasingly adopt AI-driven solutions, the demand for models that can understand and generate text inmultiple languageshas become more pressing.Until recently, AI research has focusedprimarily on Englishand a few widely spoken languages, leaving many low-resource languages behind. OpenAI’s decision to include languages like Swahili and Yoruba, spoken by millions but often neglected in AI research, signals a shift toward more inclusive AI technology. This move is especially important for enterprises looking to deploy AI solutions in emerging markets, where language barriers have traditionally posed significant challenges.Human translation raises the bar for multilingual AI accuracyOpenAI used professionalhuman translatorsto create the MMMLU dataset, ensuring higher accuracy than comparable datasets that rely on machine translation. Automated translation tools often introduce subtle errors, particularly in languages with fewer resources to train on. By relying on human expertise, OpenAI ensures that the dataset provides a more reliable foundation for evaluating AI models in multiple languages.This decision is crucial for industries where precision is non-negotiable. In sectors like healthcare, law, and finance, even minor translation errors can have serious implications. OpenAI’s focus on translation quality positions the MMMLU dataset as a critical tool for enterprises that require AI systems to perform reliably across linguistic and cultural boundaries.Hugging Face partnership boosts open access to multilingual AI dataBy releasing the MMMLU dataset on Hugging Face, a popular platform for sharing machine learning models and datasets, OpenAI is engaging the broader AI research community. Hugging Face has become a go-to destination for open-source AI tools, and the addition of the MMMLU dataset signals OpenAI’s commitment to advancing open access in AI research.However, this release comes at a time when OpenAI has faced growing scrutiny over its approach to openness.Criticism has mountedin recent months, especially fromco-founder Elon Musk, who has accused the company of straying from its original mission of being an open-source, nonprofit entity.Musk’s lawsuit, filed earlier this year, claims that OpenAI’s shift toward for-profit activities—particularly its partnership with Microsoft—contradicts the company’s founding principles.Despite this, OpenAI has defended its current strategy, arguing that it prioritizes “open access” rather than open source. In this framework, OpenAI aims to provide broad access to its technologies without necessarily sharing the inner workings of its most advanced models. The release of the MMMLU dataset fits within this philosophy, offering the research community a powerful tool while maintaining control over its proprietary models.OpenAI Academy: Expanding access to AI in emerging marketsIn addition to the MMMLU dataset release, OpenAI is furthering its commitment to global AI accessibility through the launch of theOpenAI Academy. Announced on the same day as the MMMLU dataset, the Academy is designed to invest in developers and mission-driven organizations that are leveraging AI to tackle critical problems in their communities, particularly in low- and middle-income countries.The Academy will provide training, technical guidance, and$1 million in API creditsto ensure that local AI talent can access cutting-edge resources. By supporting developers who understand the unique social and economic challenges of their regions, OpenAI hopes to empower communities to build AI applications tailored to local needs.This initiative complements the MMMLU dataset by emphasizing OpenAI’s goal of making advanced AI tools and education available to diverse, global communities. Both the MMMLU dataset and the Academy reflect OpenAI’s long-term strategy of ensuring that AI development benefits all of humanity, especially communities that have traditionally been underserved by the latest AI advancements.Multilingual AI gives businesses a competitive edgeFor enterprises, the MMMLU dataset presents an opportunity to benchmark their own AI systems in aglobal context. As companies expand into international markets, the ability to deploy AI solutions that understand multiple languages becomes critical. Whether it’s customer service, content moderation, or data analysis, AI systems that perform well across languages can offer a competitive advantage by reducing friction in communication and improving user experience.The dataset’s focus on professional and academic subjects adds another layer of value for businesses. Companies in law, education, and research can use the MMMLU dataset to test how well their AI models perform in specialized domains, ensuring that their systems meet the high standards required for these sectors. As AI continues to evolve, the ability to handle complex, domain-specific tasks in multiple languages will become a key differentiator for businesses competing on a global stage.A multilingual future: What the MMMLU dataset means for AIThe release of the MMMLU dataset is likely to have lasting implications for the AI industry. As more companies and researchers begin to test their models against this multilingual benchmark, the demand for AI systems that can operate seamlessly across languages will only grow. This could lead to new innovations in language processing, as well as greater adoption of AI solutions in parts of the world that have traditionally been underserved by technology.For OpenAI, the MMMLU dataset represents both a challenge and an opportunity. On one hand, the company is positioning itself as a leader in multilingual AI, offering tools that address a critical gap in the current AI landscape. On the other hand, OpenAI’s evolving stance on openness will continue to be scrutinized as it navigates the tensions between public good and private interest.As AI becomes increasingly integrated into the global economy, companies and governments alike will need to grapple with the ethical and practical implications of these technologies. OpenAI’s release of the MMMLU dataset is a step in the right direction, but it also raises important questions about how much of the AI revolution will be open to all.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Security,"‘Harvest now, decrypt later’: Why hackers are waiting for quantum computing",https://venturebeat.com/security/harvest-now-decrypt-later-why-hackers-are-waiting-for-quantum-computing/,"September 21, 2024 12:05 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreHackers are waiting for the momentquantum computingbreaks cryptography and enables the mass decryption of years of stolen information. In preparation, they are harvesting even more encrypted data than usual. Here is what businesses can do in response.Why are hackers harvesting encrypted data?Most modern organizations encrypt multiple critical aspects of their operations. In fact, abouteight in 10 businessesextensively or partially use enterprise-level encryption for databases, archives, internal networks and internet communications. After all, it is acybersecurity best practice.Alarmingly, cybersecurity experts are growing increasingly concerned that cybercriminals are stealing encrypted data and waiting for the right time to strike. Their worries are not unfounded — more than70% of ransomware attacksnow exfiltrate information before encryption.The “harvest now, decrypt later” phenomenon in cyberattacks — where attackers steal encryptedinformationin the hopes they will eventually be able to decrypt it — is becoming common. As quantum computing technology develops, it will only grow more prevalent.How ‘harvest now, decrypt later’ worksQuantum computers make the “harvest now, decrypt later” phenomenon possible. In the past, encryption was enough to deter cybercriminals — or at least make their efforts pointless. Unfortunately, that is no longer the case.Whereas classical computers operate using binary digits — bits — that can either be a one or a zero, their quantum counterparts use quantum bits called qubits. Qubits can exist in two states simultaneously, thanks to superposition.Since qubits may be a one and a zero, quantum computers’ processing speeds far outpace the competition. Cybersecurity experts are worried they will make modern ciphers — meaning encryption algorithms — useless, which has inspired exfiltration-driven cyberattacks.Encryption turns data, also known as plaintext, into a string of random, undecipherable code called ciphertext. Ciphers do this using complex mathematical formulas that are technically impossible to decode without a decryption key. However, quantum computing changes things.While a classical computer wouldtake 300 trillion yearsor more to decrypt a 2,048-bit Rivest-Shamir-Adleman encryption, a quantum one could crack it in seconds, thanks to qubits. The catch is that this technology isn’t widely available — only places like research institutions and government labs can afford it.That does not deter cybercriminals, as quantum computing technology could become accessible within a decade. In preparation, they use cyberattacks to steal encrypted data and plan to decrypt it later.What types of data are hackers harvesting?Hackers usually steal personally identifiable information like names, addresses, job titles and social security numbers because they enable identity theft. Account data — like company credit card numbers or bank account credentials — are also highly sought-after.Withquantum computing, hackers can access anything encrypted — data storage systems are no longer their primary focus. They can eavesdrop on the connection between a web browser and a server, read cross-program communication or intercept information in transit.Human resources, IT and accounting departments are still high risks for the average business. However, they must also worry about their infrastructure, vendors and communication protocols. After all, both client and server-side encryption will soon be fair game.The consequences of qubits cracking encryptionCompanies may not even realize they have been affected by a data breach until the attackers use quantum computing to decrypt the stolen information. It may be business as usual until a sudden surge in account takeovers, identity theft, cyberattacks and phishing attempts.Legal issues and regulatory fines would likely follow. Considering the average data breachrose from $4.35 millionin 2022 to $4.45 million in 2023 — a 2.3% year-over-year increase — the financial losses could be devastating.In the wake of quantum computing, businesses can no longer rely on ciphers to communicate securely, share files, store data or use the cloud. Their databases, archives, digital signatures, internet communications, hard drives, e-mail and internal networks will soon be vulnerable. Unless they find an alternative, they may have to revert to paper-based systems.Why prepare if quantum isn’t here yet?While the potential for broken cryptography is alarming, decision-makers should not panic. The average hacker will not be able to get a quantum computer for years — maybe even decades — because they are incredibly costly, resource-intensive, sensitive and prone to errors if they are not kept in ideal conditions.To clarify, these sensitive machines must stay just above absolute zero (459 degrees Fahrenheitto be exact) because thermal noise can interfere with their operations.However, quantum computing technology is advancing daily. Researchers are trying to make these computers smaller, easier to use and more reliable. Soon, they may become accessible enough that the average person can own one.Already, a startup based in China recently unveiled the world’s first consumer-grade portable quantum computers. The Triangulum — the most expensive model — offersthe power of three qubitsfor roughly $58,000. The two cheaper two-qubit versions retail for less than $10,000.While these machines pale in comparison to the powerhouse computers found in research institutions and government-funded labs, they prove that the world is not far away from mass-market quantum computing technology. In other words, decision-makers must act now instead of waiting until it is too late.Besides, the average hacker is not the one companies should worry about — well-funded threat groups pose a much larger threat. A world where a nation-state or business competitor can pay for quantum computing as a service to steal intellectual property, financial data or trade secrets may soon be a reality.What can enterprises do to protect themselves?There are a few steps business leaders should take in preparation for quantum computing cracking cryptography.1. Adopt post-quantum ciphersThe Cybersecurity and Infrastructure Security Agency (CISA) and the National Institute of Standards and Technology (NIST) soon plan to releasepost-quantum cryptographic standards. The agencies are leveraging the latest techniques to make ciphers quantum computers cannot crack. Firms would be wise to adopt them upon release.2. Enhance breach detectionIndicators of compromise — signs that show a network or system intrusion occurred — can help security professionals react to data breaches swiftly, potentially making data useless to the attackers. For example, they can immediately change all employees’ passwords if they notice hackers have stolen account credentials.3. Use a quantum-safe VPNA quantum-safe virtual private network (VPN) protects data in transit, preventing exfiltration and eavesdropping. One expert claims consumers should expect them soon, statingthey are in the testing phaseas of 2024. Companies would be wise to adopt solutions like these.4. Move sensitive dataDecision-makers should ask themselves whether the information bad actors steal will still be relevant when it is decrypted. They should also consider the worst-case scenario to understand the risk level. From there, they can decide whether or not to move sensitive data.One option is to transfer the data to a heavily guarded or constantly monitored paper-based filing system, preventing cyberattacks entirely. The more feasible solution is to store it on a local network not connected to the public internet, segmenting it with security and authorization controls.Decision-makers should begin preparing nowAlthough quantum-based cryptography cracking is still years — maybe decades — away, it will have disastrous effects once it arrives. Business leaders should develop a post-quantum plan now to ensure they are not caught by surprise.Zac Amos is features editor atReHack.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Security,Adversarial attacks on AI models are rising: what should you do now?,https://venturebeat.com/security/adversarial-attacks-on-ai-models-are-rising-what-should-you-do-now/,"September 20, 2024 5:22 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreAdversarial attacks on machine learning (ML) models are growing in intensity, frequency and sophistication with more enterprises admitting they have experienced an AI-related security incident.AI’s pervasive adoption is leading to a rapidly expanding threat surface that all enterprises struggle to keep up with. A recent Gartnersurveyon AI adoption shows that 73% of enterprises have hundreds or thousands of AI models deployed.HiddenLayer’s earlierstudyfound that 77% of the companies identified AI-related breaches, and the remaining companies were uncertain whether their AI models had been attacked.Two in five organizationshad an AI privacy breach or security incident of which 1 in 4 were malicious attacks.A growing threat of adversarial attacksWith AI’s growing influence across industries, malicious attackers continue to sharpen their tradecraft to exploit ML models’ growing base of vulnerabilities as the variety and volume of threat surfaces expand.Adversarial attacks on ML models look to exploit gaps by intentionally attempting to redirect the model with inputs, corrupted data, jailbreak prompts and byhiding malicious commands in imagesloaded back into a model for analysis. Attackers fine-tune adversarial attacks to make models deliver false predictions and classifications, producing the wrong output.VentureBeat contributor Ben Dicksonexplainshow adversarial attacks work, the many forms they take and the history of research in this area.Gartner also found that41%of organizations reported experiencing some form of AI security incident, including adversarial attacks targeting ML models. Of those reported incidents, 60% were data compromises by an internal party, while 27% were malicious attacks on the organization’s AI infrastructure.Thirty percentof all AI cyberattacks will leverage training-data poisoning, AI model theft or adversarial samples to attack AI-powered systems.Adversarial ML attacks on network security are growingDisrupting entire networks with adversarial ML attacks is the stealth attack strategy nation-states are betting on to disrupt their adversaries’ infrastructure, which will have a cascading effect across supply chains. The2024 Annual Threat Assessment of the U.S. Intelligence Communityprovides a sobering look at how important it is to protect networks from adversarial ML model attacks and why businesses need to consider better securing their private networks against adversarial ML attacks.A recentstudyhighlighted how the growing complexity of network environments demands more sophisticated ML techniques, creating new vulnerabilities for attackers to exploit.Researchersare seeing that the threat of adversarial attacks on ML in network security is reaching epidemic levels.The quickly accelerating number of connected devices and the proliferation of data put enterprises into an arms race with malicious attackers, many financed by nation-states seeking to control global networks for political and financial gain. It’s no longer a question of if an organization will face an adversarial attack but when. The battle against adversarial attacks is ongoing, but organizations can gain the upper hand with the right strategies and tools.Cisco, Cradlepoint( a subsidiary of Ericsson), DarkTrace, Fortinet, Palo Alto Networks, and other leading cybersecurity vendors have deep expertise in AI and ML to detect network threats and protect network infrastructure. Each is taking a unique approach to solving this challenge. VentureBeat’s analysis ofCisco’sandCradlepoint’s latest developmentsindicates how fast vendors address this and other network and model security threats. Cisco’srecent acquisitionof Robust Intelligence accentuates how important protecting ML models is to the network giant.Understanding adversarial attacksAdversarial attacks exploit weaknesses in the data’s integrity and the ML model’s robustness. According toNIST’s Artificial Intelligence Risk Management Framework, these attacks introduce vulnerabilities, exposing systems to adversarial exploitation.There are several types of adversarial attacks:Data Poisoning:Attackers introduce malicious data into a model’s training set to degrade performance or control predictions. According to a Gartner report from 2023, nearly 30% of AI-enabled organizations, particularly those in finance and healthcare, have experienced such attacks. Backdoor attacks embed specific triggers in training data, causing models to behave incorrectly when these triggers appear in real-world inputs. A 2023MIT studyhighlights the growing risk of such attacks as AI adoption grows, making defense strategies such as adversarial training increasingly important.Evasion Attacks:These attacks alter input data to mispredict. Slight image distortions can confuse models into misclassified objects. A popular evasion method, the Fast Gradient Sign Method (FGSM) uses adversarial noise to trick models. Evasion attacks in the autonomous vehicle industry have caused safety concerns, with altered stop signs misinterpreted as yield signs. A 2019 study found that a small sticker on a stop sign misled a self-driving car into thinking it was a speed limit sign.Tencent’s Keen Security Labused road stickers to trick a Tesla Model S’s autopilot system. These stickers steered the car into the wrong lane, showing how small carefully crafted input changes can be dangerous. Adversarial attacks on critical systems like autonomous vehicles are real-world threats.Model Inversion:Allows adversaries to infer sensitive data from a model’s outputs, posing significant risks when trained on confidential data like health or financial records. Hackers query the model and use the responses to reverse-engineer training data. In 2023,Gartner warned, “The misuse of model inversion can lead to significant privacy violations, especially in healthcare and financial sectors, where adversaries can extract patient or customer information from AI systems.”Model Stealing:Repeated API queries are used to replicate model functionality. These queries help the attacker create a surrogate model that behaves like the original. AI Securitystates, “AI models are often targeted through API queries to reverse-engineer their functionality, posing significant risks to proprietary systems, especially in sectors like finance, healthcare, and autonomous vehicles.” These attacks are increasing as AI is used more, raising concerns about IP and trade secrets in AI models.Recognizing the weak points in your AI systemsSecuring ML models against adversarial attacks requires understanding the vulnerabilities in AI systems. Key areas of focus need to include:Data Poisoning and Bias Attacks:Attackers target AI systems by injecting biased or malicious data, compromising model integrity. Healthcare, finance, manufacturing and autonomous vehicle industries have all experienced these attacks recently. The2024 NISTreportwarns that weak data governance amplifies these risks. Gartner notes that adversarial training and robust data controls can boost AI resilience by up to30%.Implementing secure data pipelines and constant validation is essential to protecting critical models.Model Integrity and Adversarial Training:Machine learning models can be manipulated without adversarial training. Adversarial training uses adverse examples and significantly strengthens a model’s defenses.Researcherssay adversarial training improves robustness but requires longer training times and may trade accuracy for resilience. Although flawed, it is an essential defense against adversarial attacks.Researchershave also found that poor machine identity management in hybrid cloud environments increases the risk of adversarial attacks on machine learning models.API Vulnerabilities:Model-stealing and other adversarial attacks are highly effective against public APIs and are essential for obtaining AI model outputs. Many businesses are susceptible to exploitation because they lack strong API security, as was mentioned atBlackHat 2022. Vendors, including Checkmarx and Traceable AI, are automating API discovery and ending malicious bots to mitigate these risks. API security must be strengthened to preserve the integrity of AI models and safeguard sensitive data.Best practices for securing ML modelsImplementing the following best practices can significantly reduce the risks posed by adversarial attacks:Robust Data Management and Model Management:NISTrecommends strict data sanitization and filtering to prevent data poisoning in machine learning models. Avoiding malicious data integration requires regular governance reviews of third-party data sources. ML models must also be secured by tracking model versions, monitoring production performance and implementing automated, secured updates.BlackHat 2022researchers stressed the need for continuous monitoring and updates to secure software supply chains by protecting machine learning models. Organizations can improve AI system security and reliability through robust data and model management.Adversarial Training:ML models are strengthened by adversarial examples created using the Fast Gradient Sign Method (FGSM). FGSM adjusts input data by small amounts to increase model errors, helping models recognize and resist attacks. According toresearchers, this method can increase model resilience by 30%. Researcherswritethat “adversarial training is one of the most effective methods for improving model robustness against sophisticated threats.”Homomorphic Encryption and Secure Access:When safeguarding data in machine learning, particularly in sensitive fields like healthcare and finance, homomorphic encryption provides robust protection by enabling computations on encrypted data without exposure.EYstates, “Homomorphic encryption is a game-changer for sectors that require high levels of privacy, as it allows secure data processing without compromising confidentiality.” Combining this with remote browser isolation further reduces attack surfaces ensuring that managed and unmanaged devices are protected through secure access protocols.API Security:Public-facing APIs must be secured to prevent model-stealing and protect sensitive data.BlackHat 2022noted that cybercriminals increasingly use API vulnerabilities to breach enterprise tech stacks and software supply chains. AI-driven insights like network traffic anomaly analysis help detect vulnerabilities in real time and strengthen defenses. API security can reduce an organization’s attack surface and protect AI models from adversaries.Regular Model Audits:Periodic audits are crucial for detecting vulnerabilities and addressing data drift in machine learning models. Regular testing for adversarial examples ensures models remain robust against evolving threats. Researchersnotethat “audits improve security and resilience in dynamic environments.” Gartner’s recentreporton securing AI emphasizes that consistent governance reviews and monitoring data pipelines are essential for maintaining model integrity and preventing adversarial manipulation. These practices safeguard long-term security and adaptability.Technology solutions to secure ML modelsSeveral technologies and techniques are proving effective in defending against adversarial attacks targeting machine learning models:Differential privacy:This technique protects sensitive data by introducing noise into model outputs without appreciably lowering accuracy. This strategy is particularly crucial for sectors like healthcare that value privacy. Differential privacy is a technique used by Microsoft and IBM among other companies to protect sensitive data in their AI systems.AI-Powered Secure Access Service Edge (SASE): As enterprises increasingly consolidate networking and security, SASE solutions are gaining widespread adoption. Major vendors competing in this space include Cisco, Ericsson, Fortinet, Palo Alto Networks, VMware and Zscaler. These companies offer a range of capabilities to address the growing need for secure access in distributed and hybrid environments. With Gartner predicting that 80% of organizations will adopt SASE by 2025 this market is set to expand rapidly.Ericsson distinguishes itself by integrating 5G-optimized SD-WAN and Zero Trust security. This combination enables Ericsson to deliver a cloud-based SASE solution tailored for hybrid workforces and IoT deployments. Its Ericsson NetCloud SASE platform has proven valuable in providing AI-powered analytics and real-time threat detection to the network edge. Their platform integrates Zero Trust Network Access (ZTNA), identity-based access control, and encrypted traffic inspection. Ericsson’s cellular intelligence and telemetry data train AI models that aim to improve troubleshooting assistance. Their AIOps can automatically detect latency, isolate it to a cellular interface, determine the root cause as a problem with the cellular signal and then recommend remediation.Federated Learning with Homomorphic Encryption: Federated learning allows decentralized ML training without sharing raw data, protecting privacy. Computing encrypted data with homomorphic encryption ensures security throughout the process. Google, IBM, Microsoft, and Intel are developing these technologies, especially in healthcare and finance. Google and IBM use these methods to protect data during collaborative AI model training, while Intel uses hardware-accelerated encryption to secure federated learning environments. Data privacy is protected by these innovations for secure, decentralized AI.Defending against attacksGiven the potential severity of adversarial attacks, including data poisoning, model inversion, and evasion, healthcare and finance are especially vulnerable, as these industries are favorite targets for attackers. By employing techniques including adversarial training, robust data management, and secure API practices, organizations can significantly reduce the risks posed by adversarial attacks. AI-powered SASE, built with cellular-first optimization and AI-driven intelligence has proven effective in defending against attacks on  networks.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Security,"Microsoft’s GRIN-MoE AI model takes on coding and math, beating competitors in key benchmarks",https://venturebeat.com/ai/microsofts-grin-moe-ai-model-takes-on-coding-and-math-beating-competitors-in-key-benchmarks/,"September 19, 2024 11:15 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMicrosofthas unveiled a groundbreaking artificial intelligence model,GRIN-MoE(Gradient-Informed Mixture-of-Experts), designed to enhance scalability and performance in complex tasks such as coding and mathematics. The model promises to reshape enterprise applications by selectively activating only a small subset of its parameters at a time, making it both efficient and powerful.GRIN-MoE, detailed in the research paper “GRIN: GRadient-INformed MoE,” uses a novel approach to the Mixture-of-Experts (MoE) architecture. By routing tasks to specialized “experts” within the model, GRIN achieves sparse computation, allowing it to utilize fewer resources while delivering high-end performance. The model’s key innovation lies in usingSparseMixer-v2to estimate the gradient for expert routing, a method that significantly improves upon conventional practices.“The model sidesteps one of the major challenges of MoE architectures: the difficulty of traditional gradient-based optimization due to the discrete nature of expert routing,” the researchers explain. GRIN MoE’s architecture, with 16×3.8 billion parameters, activates only 6.6 billion parameters during inference, offering a balance between computational efficiency and task performance.GRIN-MoE outperforms competitors in AI BenchmarksIn benchmark tests, Microsoft’s GRIN MoE has shown remarkable performance, outclassing models of similar or larger sizes. It scored 79.4 on theMMLU(Massive Multitask Language Understanding) benchmark and 90.4 onGSM-8K, a test for math problem-solving capabilities. Notably, the model earned a score of 74.4 onHumanEval, a benchmark for coding tasks, surpassing popular models likeGPT-3.5-turbo.GRIN MoE outshines comparable models such asMixtral (8x7B)andPhi-3.5-MoE (16×3.8B), which scored 70.5 and 78.9 on MMLU, respectively. “GRIN MoE outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data,” the paper notes.This level of performance is particularly important for enterprises seeking to balance efficiency with power in AI applications. GRIN’s ability to scale without expert parallelism or token dropping—two common techniques used to manage large models—makes it a more accessible option for organizations that may not have the infrastructure to support bigger models like OpenAI’sGPT-4oor Meta’sLLaMA 3.1.GRIN MoE, Microsoft’s new AI model, achieves high performance on the MMLU benchmark with just 6.6 billion activated parameters, outperforming comparable models like Mixtral and LLaMA 3 70B. The model’s architecture offers a balance between computational efficiency and task performance, particularly in reasoning-heavy tasks such as coding and mathematics. (Credit: arXiv.org)AI for enterprise: How GRIN-MoE boosts efficiency in coding and mathGRIN MoE’s versatility makes it well-suited for industries that require strong reasoning capabilities, such as financial services, healthcare, and manufacturing. Its architecture is designed to handle memory and compute limitations, addressing a key challenge for enterprises.The model’s ability to “scale MoE training with neither expert parallelism nor token dropping” allows for more efficient resource usage in environments with constrained data center capacity. In addition, its performance on coding tasks is a highlight. Scoring 74.4 on the HumanEval coding benchmark, GRIN MoE demonstrates its potential to accelerate AI adoption for tasks like automated coding, code review, and debugging in enterprise workflows.In a test of mathematical reasoning based on the 2024 GAOKAO Math-1 exam, Microsoft’s GRIN MoE (16×3.8B) outperformed several leading AI models, including GPT-3.5 and LLaMA3 70B, scoring 46 out of 73 points. The model demonstrated significant potential in handling complex math problems, trailing only behind GPT-4o and Gemini Ultra-1.0. (Credit: arXiv.org)GRIN-MoE Faces Challenges in Multilingual and Conversational AIDespite its impressive performance, GRIN MoE has limitations. The model is optimized primarily for English-language tasks, meaning its effectiveness may diminish when applied to other languages or dialects that are underrepresented in the training data. The research acknowledges, “GRIN MoE is trained primarily on English text,” which could pose challenges for organizations operating in multilingual environments.Additionally, while GRIN MoE excels in reasoning-heavy tasks, it may not perform as well in conversational contexts or natural language processing tasks. The researchers concede, “We observe the model to yield a suboptimal performance on natural language tasks,” attributing this to the model’s training focus on reasoning and coding abilities.GRIN-MoE’s potential to transform enterprise AI applicationsMicrosoft’s GRIN-MoE represents a significant step forward in AI technology, especially for enterprise applications. Its ability to scale efficiently while maintaining superior performance in coding and mathematical tasks positions it as a valuable tool for businesses looking to integrate AI without overwhelming their computational resources.“This model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI-powered features,” the research team explains. As AI continues to play an increasingly critical role in business innovation, models like GRIN MoE are likely to be instrumental in shaping the future of enterprise AI applications.As Microsoft pushes the boundaries of AI research, GRIN-MoE stands as a testament to the company’s commitment to delivering cutting-edge solutions that meet the evolving needs of technical decision-makers across industries.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Infrastructure,Why countries are in a race to build AI factories in the name of sovereign AI,https://venturebeat.com/ai/why-countries-are-in-a-race-to-build-ai-factories-in-the-name-of-sovereign-ai/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreNow that AI has become a fundamentally important technology, and the world has gravitated toward intense geopolitical battles, it’s no wonder that “sovereign AI” is becoming a national issue.Think about it. Would the U.S. allow the data it generates for AI to be stored and processed in China? Would the European Union want its people’s data to be accessed by big U.S. tech giants? Would Russia trust NATO countries to manage its AI resources? Would Muslim nations entrust their data for AI to Israel?Nvidia has earmarked $110 million to help countries foster AI startups to invest in sovereign AI infrastructure, and plenty of countries are investing in AI infrastructure on their own. That’s some real money aimed at jumpstarting the world when it comes to embracing AI. The question becomes whether this discussion is a lot of thought leadership to enable a sales pitch, or whether nations truly need to embrace sovereign AI to be competitive with the rest of the world. Is it a new kind of arms race that makes sense for nations to pursue?A wake-up callDigital rendering of Nvidia’s Jensen HuangJensen Huang, CEO of Nvidia, pointed out the rise of “sovereign AI” during an earnings call in November 2023 as a reason for why demand is growing for Nvidia’s AI chips. The company noted that investment in national computer infrastructure was a new priority for governments around the world.“The number of sovereign AI clouds is really quite significant,” Huang said in the earnings call. He said Nvidia wants to enable every company to build its own custom AI models.The motivations weren’t just about keeping a country’s data in local tech infrastructure to protect it. Rather, they saw the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation, said Colette Kress, CFO of Nvidia, in the earnings call.That was around the time when the Biden administration was restricting sales of the most powerful AI chips to China, requiring a license from the U.S. government before shipments could happen. That licensing requirement is still in effect.As a result, China reportedly began its own attempts to create AI chips to compete with Nvidia’s. But it wasn’t just China. Kress also said Nvidia was working with the Indian government and its large tech companies like Infosys, Reliance and Tata to boost their “sovereign AI infrastructure.”Meanwhile, French private cloud provider Scaleway was investing in regional AI clouds to fuel AI advances in Europe as part of a “new economic imperative,” Kress said. The result was a “multi-billion dollar opportunity” over the next few years, she said.Huang said Sweden and Japan have embarked on creating sovereign AI clouds.“You’re seeing sovereign AI infrastructures, people, countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data, and develop their own AI. You see that in India,” Huang said.He added, “Sovereign AI clouds coming up from all over the world as people realize that they can’t afford to export their country’s knowledge, their country’s culture for somebody else to then resell AI back to them.”Nvidia itself definessovereign AIas “a nation’s capabilities to produce artificial intelligence using its own infrastructure, data, workforce and business networks.”Keeping sovereign AI secureCredit: VentureBeat using DALL-EIn an interview with VentureBeat in February 2024,Huangdoubled down on the concept, saying, “We now have a new type of data center that is about AI generation, an AI generation factory. And you’ve heard me describe it as AI factories. Basically, it takes raw material which is data, transforms it with these AI supercomputers and Nvidia builds and it turns them into incredibly valuable tokens. These tokens are what people experience on the amazing” generative AI platforms like Midjourney.I asked Huang why, if data is kept secure regardless of its location in the world, does sovereign AI need to exist within the borders of any given country.He replied, “There’s no reason to let somebody else come and scrape your internet, take your history, your data. And a lot of it is still locked up in libraries. In our case, it’s Library of Congress. In other cases, national libraries. And they’re digitized, but they haven’t been put on the internet.”He added, “And so people are starting to realize that they had to use their own data to create their own AI, and transform their raw material into something of value for their own country, by their own country. And so you’re going to see a lot. Almost every country will do this. And they’re going to build the infrastructure. Of course, the infrastructure is hardware. But they don’t want to export their data using AI.”The $110 million investmentShilpa Kolhatkar (left) of Nvidia speaks with Jon Metzler of U.C. Berkeley.Nvidia has earmarked $110 million to invest in AI startups helping with sovereign AI projects and other AI-related businesses.Shilpa Kolhatkar, global head of AI Nations at Nvidia, gave a deeper dive on sovereign AI at theU.S.-Japan Innovation Symposiumat Stanford University. The July event was staged by the Japan Society of Northern California and the Stanford US-Asia Technology Management Center.Kolhatkar did the interview with Jon Metzler, a continuing lecturer at the Haas School of Business at the University of California, Berkeley. That conversation focused on how to achieve economic growth through investments in AI technology. Kolhatkar noted how Nvidia has transformed itself from a graphics company to a high-performance computing and AI company long before ChatGPT arrived.“Lots of governments around the world are looking today at how can they capture this opportunity that AI has presented and they [have focused] on domestic production of AI,” Kolhatkar said. “We have the Arab nations program, which kind of matches the AI strategy that nations have in place today. About 60 to 70 nations have an AI strategy in place, built around the major pillars of creating the workforces and having the ecosystem. But it’s also around having already everything within the policy framework.”AI readiness?Examples of generative AI by Getty Images.Nvidia plays a role in setting up the ecosystem and infrastructure, or supercomputers. The majority of Nvidia’s focus and its engineering efforts is in the software stack on top of the chips, she said. As a result, Nvidia has become more of a platform company, rather than a chip company. Metzler asked Kolhatkar to define how a country might develop “AI readiness.”Kolhatkar said that one notion is to look at how much computing power a country has, in terms of raw AI compute, storage and the energy related to power such systems. Does it have a skilled workforce to operate the AI? Is the population ready to take advantage of AI’s great democratization so that the knowledge spreads well beyond data scientists?When ChatGPT-3.5 emerged in Nov. 2022 and generative AI exploded, it signaled that AI was really finally working in a way that ordinary consumers could use to automate many tasks and find new information or create things like images on their own. If there were errors in the results, it could be because the data model wasn’t fed the correct information. Then it quickly followed that different regions had their own views on what was considered correct information.“That model was trained primarily on a master data set and a certain set of languages in western [territories],” Kolhatkar said. “That is why the internationalization of having something which is sovereign, which is specific to a nation’s own language, culture and nuances, came to the forefront.”Then countries started developing generative AI models that cater to the specificities of a particular region or particular nation, and, of course, the ownership of that data, she said.“The ownership is every country’s data and proprietary data, which they realized should stay within the borders,” she said.AI factoriesNvidia’s notion of AI factories.Nvidia is now in the process of helping countries create such sovereign infrastructure in the form of “AI factories,” Kolhatkar said. That’s very similar to the drive that nations ignited with factories during the Industrial Revolution more than 100 years ago.“Factories use raw materials that go in and then goods come out and that was tied to the domestic GDP. Now the paradigm is that your biggest asset is your data. Every nation has its own unique language and data. That’s the raw material that goes into the AI factory, which consists of algorithms, which consists of models and out comes intelligence,” she said.Now countries like Japan have to consider whether they’re ahead or falling behind when it comes to being ready with AI factories. Kolhatkar said that Japan is leading the way when it comes to investments, collaborations and research to create a successful “AI nation.”She said companies and nations are seriously considering how much of AI should be classified as “critical infrastructure” for the sake of economic or national security. Where industrial factories could create thousands of jobs in a given city, now data centers can create a lot of jobs in a given region as well. Are these AI factories like the dams and airports of decades ago?“You’re kind of looking at past precedents from physical manufacturing as to what the multiplier might be for AI factories,” Metzler said. “The notion of AI factories as maybe civic infrastructure is super interesting.”National AI strategies?Cerebras Condor Galaxy at Colovore Data CenterMetzler brought up the notion of the kind of strategies that can happen when it comes to the AI race. For instance, he noted that maybe smaller countries need to team up to create their own larger regional networks, to create some measure of sovereignty.Kolhatkar said that can make sense if your country, for instance, doesn’t have the resources of any given tech giant like Samsung. She noted the Nordic nations are collaborating with each other, as are nations like the U.S. and Japan when it comes to AI research. Different industries or government ministries can also get together for collaboration on AI.If Nvidia is taking a side on this, it’s in spreading the tech around so that everyone becomes AI literate. Nvidia has an online university dubbed the Deep Learning Institute for self-paced e-learning courses. It also has a virtual incubatorNvidia Inception, which has supported more than19,000 AI startups.“Nvidia really believes in democratization of AI because the full potential of AI can not be achieved unless everybody’s able to use it,” Kolhatkar said.Energy consumption?AI power consumptionAs for dealing with the fallout of sovereign AI, Metzler noted that countries will have to deal with sustainability issues in terms ofhow much power is being consumed.In May, theElectric Power Research Institute(EPRI) released awhitepaperthat quantified the exponential growth potential of AI power requirements. It projected that total data center power consumption by U.S. data centers alone could more than double to 166% by 2030.It noted that each ChatGPT request can consume 2.9 watt-hours of power. That means AI queries are estimated to require 10 times the electricity of traditional Google queries, which use about 0.3 watt-hours each. That’s not counting emerging, computation-intensive capabilities such as image, audio and video generation, which don’t have a comparision precedent.EPRI looked at four scenarios. Under the highest growth scenario, data center electricity usage could rise to 403.9 TWh/year by 2030, a 166% increase from 2023 levels. Meanwhile, the low growth scenario projected a 29% increase to 196.3 TWh/year.“It’s about the energy efficiency, sustainability is pretty top of mind for everyone,” Kolhatkar said.Nvidia is trying to make each generation of AI chip more power efficient even as it makes each one more performant. She also noted the industry is trying to create and use sources of renewable energy. Nvidia also uses its output from AI, in the form of Nvidia Omniverse software, to create digital twins of data centers. These buildings can be architected with energy consumption in mind and with the notion of minimizing duplicative effort.Once they’re done, the virtual designs can be built in the physical world with a minimum of inefficiency. Nvidia is even creating a digital twin of the Earth to predict climate change for decades to come. And the AI tech can also be applied to making physical infrastructure more efficient, like making India’s infrastructure more resistant to monsoon weather. In these ways, Kolhatkar thinks AI can be used to “save the world.”She added, “Data is the biggest asset that a nation has. It has your proprietary data with your language, your culture, your values, and you are the best person to own it and codify it into an intelligence that you want to use for your analysis. So that is what sovereignty is. That is at the domestic level. The local control of your assets, your biggest asset, [matters].”A change in computing infrastructureNvidia Blackwell has 208 billion transistors.Computers, of course, don’t know national borders. If you string internet cables around the world, the information flows and a single data center could theoretically provide its information on a global basis. If that data center has layers of security built in, there should be no worry about where it’s located. This is the notion of the advantage of computers of creating a “virtual” infrastructure.But these data centers need backups, as the world has learned that extreme centralization isn’t good for things like security and control. A volcanic eruption in Iceland, a tsunami in Japan, an earthquake in China,  a terrorist attack on infrastructure or possible government spying in any given country — these are all reasons for having more than one data center to store data.Besides disaster backup, national security is another reason driving each country to require their own computing infrastructure within their borders. Before the generative AI boom, there was a movement to ensure data sovereignty, in part because some tech giants overreached when it came to disintermediating users and their applications that developed personalized data. Data best practices resulted.Roblox CEO Dave Baszucki said at the Roblox Developer Conference that his company operates a network of 27 data centers around the world to provide the performance needed to keep its game platform operating on different computing platforms around the world. Roblox has 79.5 million daily active users who are spread throughout the world.Given that governments around the world are coming up with data security and privacy laws, Roblox might very well have to change its data center infrastructure so that it has many more data centers that are operating in given jurisdictions.There are 195 nation states in the world, and if the policies become restrictive, a company might conceivably need to have 195 data centers. Not all of these divisions are parochial. For instance, some countries might want to deliberately reduce the “digital divide” between rich nations and poor ones, Kolhatkar said.There’s another factor driving the decentralization of AI — the need for privacy. Not only for the governments of the world, but also for companies and people. The celebrated “AI PC” trend of 2024 offers consumers personal computers with powerful AI tech to ensure the privacy of operating AI inside their own homes. This way, it’s not so easy for the tech giants to learn what you’re searching for and the data that you’re using to train your own personal AI network.Do we need sovereign AI?Nvidia humanoid robots.Huang suggested that countries perceive it as needed so that a large language model (LLM) can be built with knowledge of local customs. As an example, Chernobyl is spelled with an “e” in Russian. But in Ukraine, it’s spelled “Chornobyl.” That’s just a small example of why local customs and culture need to be taken into account for systems used in particular countries.Some people are concerned about the trend as it drives the world toward more geographic borders, which in the case of computing, really don’t or shouldn’t exist.Kate Edwards, CEO of Geogrify and an expert on geopolitics in the gaming industry, said in a message, “I think it’s a dangerous term to leverage, as ‘sovereignty’ is a concept that typically implies a power dynamic that often forms a cornerstone of nationalism, and populism in more extreme forms. I get why the term is being used here but I think it’s the wrong direction for how we want to describe AI.”She added, “‘Sovereign’ is the wrong direction for this nomenclature. It instantly polarizes what AI is for, and effectively puts it in the same societal tool category as nuclear weapons and other forms of mass disruption. I don’t believe this is how we really want to approach this resource, especially as it could imply that a national government essentially has an enslaved intelligence whose purpose is to reinforce and serve the goals of maintaining a specific nation’s sovereignty — which is the basis for the great majority of geopolitical conflict.”Are countries taking Nvidia’s commentary seriously or do they view it as a sales pitch? Nvidia isn’t the only company succeeding with the pitch.AMD competes with Nvidia in AI/graphics chips as well as CPUs. Like Nvidia, it is seeing an explosion in demand for AI chips. AMD also continues to expand its efforts in software, with the acquisition of AI software firms like Nod.AI and Silo AI. AI is consistently driving AMD’s revenues and demand for both its CPUs and GPUs/AI chips.Cerebras WSE-3Cerebras Systems, for instance,announcedin July 2023 that it was shipping its giant wafer-size CPUs to the technology holding groupG42, which was building the world’s largest supercomputer for AI training, namedCondor Galaxy, in the United Arab Emirates.It started with a network of nine interconnected supercomputers aimed at reducing AI model training time significantly, with a total capacity of 36 exaFLOPs, thanks to the first AI supercomputer on the network, Condor Galaxy 1 (CG-1), which had 4 exaFLOPs and 54 million cores, said Andrew Feldman, CEO ofCerebras, in an interview with VentureBeat. Those computers were based in the U.S., but they are being operated by the firm in Abu Dhabi. (That raises the question, again, of whether sovereign AI tech has to be located in the country that uses the computing power).NowCerebras has broken groundon a new generation of Condor Galaxy supercomputers for G42.Rather than make individual chips for its centralized processing units (CPUs), Cerebras takes entire silicon wafers and prints its cores on the wafers, which are the size of pizza. These wafers have the equivalent of hundreds of chips on a single wafer, with many cores on each wafer. And that’s how they get to 54 million cores in a single supercomputer.Feldman said, “AI is not just eating the U.S. AI is eating the world. There’s an insatiable demand for compute. Models are proliferating. And data is the new gold. This is the foundation.”VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Infrastructure,Building and securing a governed AI infrastructure for the future,https://venturebeat.com/security/building-and-securing-a-governed-ai-infrastructure-for-the-future/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThis article is part of a VB Special Issue called “Fit for Purpose: Tailoring AI Infrastructure.”Catch all the other stories here.Unlocking AI’s potential to deliver greater efficiency, cost savings and deeper customer insights requires a consistent balance between cybersecurity and governance.AI infrastructure must be designed to adapt and flex to a business’ changing directions. Cybersecurity must protect revenue and governance must stay in sync with compliance internally and across a company’s footprint.Any business looking to scale AI safely must continually look for new ways to strengthen the core infrastructure components. Just as importantly, cybersecurity, governance and compliance must share a common data platform that enables real-time insights.“AI governance defines a structured approach to managing, monitoring and controlling the effective operation of a domain and human-centric use and development of AI systems,” Venky Yerrapotu, founder and CEO of4CRisk, told VentureBeat. “Packaged or integrated AI tools do come with risks, including biases in the AI models, data privacy issues and the potential for misuse.”A robust AI infrastructure makes audits easier to automate, helps AI teams find roadblocks and identifies the most significant gaps in cybersecurity, governance and compliance.>>Don’t miss our special issue:Fit for Purpose: Tailoring AI Infrastructure.<<“With little to no current industry-approved governance or compliance frameworks to follow, organizations must implement the proper guardrails to innovate safely with AI,” Anand Oswal, SVP and GM of network security atPalo Alto Networks, told VentureBeat. “The alternative is too costly, as adversaries are actively looking to exploit the newest path of least resistance: AI.”Defending against threats to AI infrastructureWhile malicious attackers’ goals vary from financial gain todisrupting or destroyingconflicting nations’ AI infrastructure, all seekto improve their tradecraft. Malicious attackers, cybercrime gangs and nation-state actors are all moving faster than even the most advanced enterprise or cybersecurity vendor.“Regulations and AI are like a race between a mule and a Porsche,” Etay Maor, chief security strategist atCato Networks, told VentureBeat. “There’s no competition. Regulators always play catch-up with technology, but in the case of AI, that’s particularly true. But here’s the thing: Threat actors don’t play nice. They’re not confined by regulations and are actively finding ways to jailbreak the restrictions on new AI tech.”Chinese, North Korean and Russian-based cybercriminal and state-sponsored groups are activelytargeting both physical and AI infrastructureand using AI-generated malware to exploit vulnerabilities more efficiently and in ways that are often undecipherable to traditional cybersecurity defenses.Security teams are stillat risk of losing the AI waras well-funded cybercriminal organizations and nation-states target AI infrastructures of countries and companies alike.One effective security measure is model watermarking, which embeds a unique identifier into AI models to detect unauthorized use or tampering. Additionally, AI-driven anomaly detection tools are indispensable for real-time threat monitoring.All of the companies VentureBeat spoke with on the condition of anonymity are actively using red teaming techniques. Anthropic, for one, proved the value ofhuman-in-the-middle designto close security gaps in model testing.“I think human-in-the-middle design is with us for the foreseeable future to provide contextual intelligence, human intuition to fine-tune an [large language model] LLM and to reduce the incidence of hallucinations,” Itamar Sher, CEO ofSeal Security, told VentureBeat.Models are the high-risk threat surfaces of an AI infrastructureEvery model released into production is a new threat surface an organization needs to protect. Gartner’s annual AI adoptionsurveyfound that 73% of enterprises have deployed hundreds or thousands of models.Malicious attackers exploit weaknesses in models using a broad base of tradecraft techniques.NIST’s Artificial Intelligence Risk Management Frameworkis an indispensable document for anyone building AI infrastructure and provides insights into the most prevalent types of attacks, including data poisoning, evasion and model stealing.AI Securitywrites, “AI models are often targeted through API queries to reverse-engineer their functionality.”Getting AI infrastructure right is also a moving target, CISOs warn. “Even if you’re not using AI in explicitly security-centric ways, you’re using AI in ways that matter for your ability to know and secure your environment,” Merritt Baer, CISO atReco, told VentureBeat.Put design-for-trust at the center of AI infrastructureJust as an operating system has specific design goals that strive to deliver accountability, explainability, fairness, robustness and transparency, so too does AI infrastructure.Implicit throughout theNIST frameworkis a design-for-trust roadmap, which offers a practical, pragmatic definition to guide infrastructure architects. NIST emphasizes that validity and reliability are must-have design goals, especially in AI infrastructure, to deliver trustworthy, reliable results and performance.Source: NIST, January 2023, DOI:10.6028/NIST.AI.100-1.The critical role of governance in AI InfrastructureAI systems and models must be developed, deployed and maintained ethically, securely and responsibly.  Governance must be designed to deliver workflows, visibility and real-time updates on algorithmic transparency, fairness, accountability and privacy. The cornerstone of strong governance starts when models are continuously monitored, audited and aligned with societal values.Governance frameworks should be integrated into AI infrastructure from the first phases of development. “Governance by design” embeds these principles into the process.“Implementing an ethical AI framework requires focus on security, bias and data privacy aspects not only during the designing process of the solution but also throughout the testing and validation of all the guardrails before deploying the solutions to end users,”WinWireCTO Vineet Arora told VentureBeat.Designing AI infrastructures to reduce biasIdentifying and reducing biases in AI models is critical to delivering accurate, ethically sound results. Organizations need to step up and take accountability for how their AI infrastructures monitor, control and improve to reduce and eliminate biases.Organizations that take accountability for their AI infrastructures rely on adversarial debiasing train models to minimize the relationship between protected attributes (including race or gender) and outcomes, reducing the risk of discrimination. Another approach is resampling training data to ensure a balanced representation relevant to different industries.“Embedding transparency and explainability into the design of AI systems enables organizations to understand better how decisions are being made, allowing for more effective detection and correction of biased outputs,”saysNIST. Providing transparent insights into how AI models make decisions allows organizations to better detect, correct and learn from biases.How IBM is managing AI governanceIBM’s AI Ethics Boardoversees the company’s AI infrastructure and AI projects, ensuring each stays ethically compliant with industry and internal standards. IBM initially established a governance framework to include what they’re calling “focal points,” or mid-level executives with AI expertise, who review projects in development to ensure compliance with IBM’s Principles of Trust and Transparency​.IBMsays this framework helps reduce and control risks at the project level, alleviating risks to AI infrastructures.Christina Montgomery, IBM’s chief privacy and trust officer,says, “Our AI ethics board plays a critical role in overseeing our internal AI governance process, creating reasonable internal guardrails to ensure we introduce technology into the world responsibly and safely.”Governance frameworks must be embedded in AI infrastructure from the design phase. The concept ofgovernance by designensures that transparency, fairness and accountability are integral parts of AI development and deployment.AI infrastructure must deliver explainable AIClosing gaps between cybersecurity, compliance and governance is accelerating across AI infrastructure use cases. Two trends emerged from VentureBeat research: agentic AI and explainable AI. Organizations with AI infrastructure are looking to flex and adapt their platforms to make the most of each.Of the two, explainable AI is nascent in providing insights to improve model transparency and troubleshoot biases. “Just as we expect transparency and rationale in business decisions, AI systems should be able to provide clear explanations of how they reach their conclusions,” Joe Burton, CEO ofReputation, told VentureBeat. “This fosters trust and ensures accountability and continuous improvement.”Burton added: “By focusing on these governance pillars — data rights, regulatory compliance, access control and transparency — we can leverage AI’s capabilities to drive innovation and success while upholding the highest standards of integrity and responsibility.”VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Infrastructure,Going beyond GPUs: The evolving landscape of AI chips and accelerators,https://venturebeat.com/data-infrastructure/going-beyond-gpus-the-evolving-landscape-of-ai-chips-and-accelerators/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThis article is part of a VB Special Issue called “Fit for Purpose: Tailoring AI Infrastructure.”Catch all the other stories here.Data centers are the backend of the internet we know. Whether it’s Netflix or Google, all major companies leverage data centers, and the computer systems they host, to deliver digital services to end users. As the focus of enterprises shifts toward advanced AI workloads, data centers’ traditional CPU-centric servers are being buffed with the integration of new specialized chips or “co-processors.”At the core, the idea behind these co-processors is to introduce an add-on of sorts to enhance the computing capacity of the servers. This enables them to handle the calculational demands of workloads like AI training, inference, database acceleration and network functions. Over the last few years, GPUs, led by Nvidia, have been the go-to choice for co-processors due to their ability to process large volumes of data at unmatched speeds. Due to increased demand GPUs accounted for 74% of the co-processors powering AI use cases within data centers last year, according to a study fromFuturum Group.According to the study, the dominance of GPUs is only expected to grow, with revenues from the category surging 30% annually to $102 billion by 2028. But, here’s the thing: while GPUs, with their parallel processing architecture, make a strong companion for accelerating all sorts of large-scale AI workloads (like training and running massive, trillion parameter language models or genome sequencing), their total cost of ownership can be very high. For example, Nvidia’s flagshipGB200 “superchip”, which combines a Grace CPU with two B200 GPUs, is expected to cost between $60,000 and $70,000. A server with 36 of these superchips is estimated to cost around $2 million.While this may work in some cases, like large-scale projects, it is not for every company. Many enterprise IT managers are looking to incorporate new technology to support select low- to medium-intensive AI workloads with a specific focus on total cost of ownership, scalability and integration. After all, most AI models (deep learning networks, neural networks, large language models etc) are in the maturing stage and the needs are shifting towards AI inferencing and enhancing the performance for specific workloads like image recognition, recommender systems or object identification — while being efficient at the same time.>>Don’t miss our special issue:Fit for Purpose: Tailoring AI Infrastructure.<<This is exactly where the emerging landscape of specialized AI processors and accelerators, being built by chipmakers, startups and cloud providers, comes in.What exactly are AI processors and accelerators?At the core, AI processors and accelerators are chips that sit within servers’ CPU ecosystem and focus on specific AI functions. They commonly revolve around three key architectures: Application-Specific Integrated Circuited (ASICs), Field-Programmable Gate Arrays (FPGAs), and the most recent innovation of Neural Processing Units (NPUs).The ASICs and FPGAs have been around for quite some time, with programmability being the only difference between the two. ASICs are custom-built from the ground up for a specific task (which may or may not be AI-related), while FPGAs can be reconfigured at a later stage to implement custom logic. NPUs, on their part, differentiate from both by serving as the specialized hardware that can only accelerate AI/ML workloads like neural network inference and training.“Accelerators tend to be capable of doing any function individually, and sometimes with wafer-scale or multi-chip ASIC design, they can be capable of handling a few different applications. NPUs are a good example of a specialized chip (usually part of a system) that can handle a number of matrix-math and neural network use cases as well as various inference tasks using less power,” Futurum group CEO Daniel Newman tells Venturebeat.The best part is that accelerators, especially ASICs and NPUs built for specific applications, can prove more efficient than GPUs in terms of cost and power use.“GPU designs mostly center on Arithmetic Logic Units (ALUs) so that they can perform thousands of calculations simultaneously, whereas AI accelerator designs mostly center on Tensor Processor Cores (TPCs) or Units. In general, the AI accelerators’ performance versus GPUs performance is based on the fixed function of that design,” Rohit Badlaney, the general manager for IBM’s cloud and industry platforms, tells VentureBeat.Currently, IBM follows a hybrid cloud approach and uses multiple GPUs and AI accelerators, including offerings from Nvidia and Intel, across its stack to provide enterprises with choices to meet the needs of their unique workloads and applications — with high performance and efficiency.“Our full-stack solutions are designed to help transform how enterprises, developers and the open-source community build and leverage generative AI. AI accelerators are one of the offerings that we see as very beneficial to clients looking to deploy generative AI,” Badlaney said. He added while GPU systems are best suited for large model training and fine-tuning, there are many AI tasks that accelerators can handle equally well – and at a lesser cost.For instance, IBM Cloud virtual serversuse Intel’s Gaudi 3 acceleratorwith a custom software stack designed specifically for inferencing and heavy memory demands. The company also plans to use the accelerator for fine-tuning and small training workloads via small clusters of multiple systems.“AI accelerators and GPUs can be used effectively for some similar workloads, such as LLMs and diffusion models (image generation like Stable Diffusion) to standard object recognition, classification, and voice dubbing. However, the benefits and differences between AI accelerators and GPUs entirely depend on the hardware provider’s design. For instance, the Gaudi 3 AI accelerator was designed to provide significant boosts in compute, memory bandwidth, and architecture-based power efficiency,” Badlaney explained.This, he said, directly translates to price-performance benefits.Beyond Intel, other AI accelerators are also drawing attention in the market. This includes not only custom chips built for and by public cloud providers such as Google, AWS and Microsoft but also dedicated products (NPUs in some cases) from startups such as Groq, Graphcore, SambaNova Systems and Cerebras Systems. They all stand out in their own way, challenging GPUs in different areas.In one case, Tractable, a company developing AI to analyze damage to property and vehicles for insurance claims, was able to leverage Graphcore’s Intelligent Processing Unit-POD system (a specialized NPU offering) for significant performance gains compared to GPUs they had been using.“We saw a roughly 5X speed gain,” Razvan Ranca, co-founder and CTO at Tractable,wrotein a blog post. “That means a researcher can now run potentially five times more experiments, which means we accelerate the whole research and development process and ultimately end up with better models in our products.”AI processors are also powering training workloads in some cases. For instance, the AI supercomputer at Aleph Alpha’s data center is usingCerebras CS-3, the system powered by the startup’s third-generation Wafer Scale Engine with 900,000 AI cores, to build next-gen sovereign AI models. Even Google’s recently introduced custom ASIC,TPU v5p, is driving some AI training workloads for companies like Salesforce and Lightricks.What should be the approach to picking accelerators?Now that it’s established there are many AI processors beyond GPUs to accelerate AI workloads, especially inference, the question is: how does an IT manager pick the best option to invest in? Some of these chips may deliver good performance with efficiencies but might be limited in terms of the kind of AI tasks they could handle due to their architecture. Others may do more but the TCO difference might not be as massive when compared to GPUs.Since the answer varies with the design of the chips, all experts VentureBeat spoke to suggested the selection should be based upon the scale and type of the workload to be processed, the data, the likelihood of continued iteration/change and cost and availability needs.According to Daniel Kearney, the CTO atSustainable Metal Cloud, which helps companies with AI training and inference, it is also important for enterprises to run benchmarks to test for price-performance benefits and ensure that their teams are familiar with the broader software ecosystem that supports the respective AI accelerators.“While detailed workload information may not be readily in advance or may be inconclusive to support decision-making, it is recommended to benchmark and test through with representative workloads, real-world testing and available peer-reviewed real-world information where available to provide a data-driven approach to choosing the right AI accelerator for the right workload. This upfront investigation can save significant time and money, particularly for large and costly training jobs,” he suggested.Globally, with inference jobs on track to grow, the total market of AI hardware, including AI chips, accelerators and GPUs, is estimated to grow 30% annually to touch $138 billion by 2028.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Infrastructure,"Cloud, edge or on-prem? Navigating the new AI infrastructure paradigm",https://venturebeat.com/ai/cloud-edge-or-on-prem-navigating-the-new-ai-infrastructure-paradigm/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThis article is part of a VB Special Issue called “Fit for Purpose: Tailoring AI Infrastructure.”Catch all the other stories here.No doubt, enterprise data infrastructure continues to transform with technological innovation — most notably today due to data-and-resource hungry generative AI.As gen AI changes the enterprise itself, leaders continue to grapple with the cloud/edge/on-prem question. On the one hand, they need near-instant access to data; on the other, they need to know that that data is protected.As they face this conundrum, more and more enterprises are seeing hybrid models as the way forward, as they can exploit the different advantages of what cloud, edge and on-prem models have to offer. Case in point:85% of cloud buyersare either deployed or in the process of deploying a hybrid cloud, according to IDC.“The pendulum between the edge and the cloud and all the hybrid flavors in between has kept shifting over the past decade,” Priyanka Tembey, co-founder and CTO at runtime application security companyOperant, told VentureBeat. “There are quite a few use cases coming up where compute can benefit from running closer to the edge, or as a combination of edge plus cloud in a hybrid manner.”>>Don’t miss our special issue:Fit for Purpose: Tailoring AI Infrastructure.<<The shifting data infrastructure pendulumFor a long time, cloud was associated with hyperscale data centers — but that is no longer the case, explained Dave McCarthy, research VP and global research lead for IDC’s cloud and edge services. “Organizations are realizing that the cloud is an operating model that can be deployed anywhere,” he said.“Cloud has been around long enough that it is time for customers to rethink their architectures,” he said. “This is opening the door for new ways of leveraging hybrid cloud and edge computing to maximize the value of AI.”AI, notably, is driving the shift to hybrid cloud and edge because models need more and more computational power as well as access to large datasets, noted Miguel Leon, senior director at app modernization companyWinWire.“The combination of hybrid cloud, edge computing and AI is changing the tech landscape in a big way,” he told VentureBeat. “As AI continues to evolve and becomes a de facto embedded technology to all businesses, its ties with hybrid cloud and edge computing will only get deeper and deeper.”Edge addresses issues cloud can’t aloneAccording to IDC research, spending on edge is expected to reach$232 billion this year. This growth can be attributed to several factors, McCarthy noted — each of which addresses a problem that cloud computing can’t solve alone.One of the most significant is latency-sensitive applications. “Whether introduced by the network or the number of hops between the endpoint and server, latency represents a delay,” McCarthy explained. For instance, vision-based quality inspection systems used in manufacturing require real-time response to activity on a production line. “This is a situation where milliseconds matter, necessitating a local, edge-based system,” he said.“Edge computing processes data closer to where it’s generated, reducing latency and making businesses more agile,” Leon agreed. It also supports AI apps that need fast data processing for tasks like image recognition and predictive maintenance.Edge is beneficial for limited connectivity environments, as well, such as internet of things (IoT) devices that may be mobile and move in and out of coverage areas or experience limited bandwidth, McCarthy noted. In certain cases — autonomous vehicles, for one — AI must be operational even if a network is unavailable.Another issue that spans all computing environments is data — and lots of it. According to thelatest estimates, approximately 328.77 million terabytes of data are generated every day. By 2025, the volume of data is expected to increase to more than 170 zettabytes, representing a more than 145-fold increase in 15 years.As data in remote locations continues to increase, costs associated with transmitting it to a central data store also continue to grow, McCarthy pointed out. However, in the case of predictive AI, most inference data does not need to be stored long-term. “An edge computing system can determine what data is necessary to keep,” he said.Also, whether due to government regulation or corporate governance, there can be restrictions to where data can reside, McCarthy noted. As governments continue to pursue data sovereignty legislation, businesses are increasingly challenged with compliance. This can occur when cloud or data center infrastructure is located outside a local jurisdiction. Edge can come in handy here, as well,With AI initiatives quickly moving from proof-of-concept trials to production deployments, scalability has become another big issue.“The influx of data can overwhelm core infrastructure,” said McCarthy. He explained that, in the early days of the internet, content delivery networks (CDNs) were created to cache content closer to users. “Edge computing will do the same for AI,” he said.Benefits and uses of hybrid modelsDifferent cloud environments have different benefits, of course. For example, McCarthy noted, that auto-scaling to meet peak usage demands is “perfect” for public cloud. Meanwhile, on-premises data centers and private cloud environments can help secure and provide better control over proprietary data. The edge, for its part, provides resiliency and performance in the field. Each plays its part in an enterprise’s overall architecture.“The benefit of a hybrid cloud is that it allows you to choose the right tool for the job,” said McCarthy.He pointed to numerous use cases for hybrid models: For instance, in financial services, mainframe systems can be integrated with cloud environments so that institutions can maintain their own data centers for banking operations while leveraging the cloud for web and mobile-based customer access. Meanwhile, in retail, local in-store systems can continue to process point-of-sale transactions and inventory management independently of the cloud should an outage occur.“This will become even more important as these retailers roll out AI systems to track customer behavior and prevent shrinkage,” said McCarthy.Tembey also pointed out that a hybrid approach with a combination of AI that runs locally on a device, at the edge and in larger private or public models using strict isolation techniques can preserve sensitive data.Not to say that there aren’t downsides — McCarthy pointed out that, for instance, hybrid can increase management complexity, especially in mixed vendor environments.“That is one reason why cloud providers have been extending their platforms to both on-prem and edge locations,” he said, adding that original equipment manufacturers (OEMs) and independent software vendors (ISVs) have also increasingly been integrating with cloud providers.Interestingly, at the same time, 80% of respondents to an IDC survey indicated that they either have or plan to move some public cloud resources back on-prem.“For a while, cloud providers tried to convince customers that on-premises data centers would go away and everything would run in the hyperscale cloud,” McCarthy noted. “That has proven not to be the case.”VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Infrastructure,From cost center to competitive edge: The strategic value of custom AI Infrastructure,https://venturebeat.com/ai/from-cost-center-to-competitive-edge-the-strategic-value-of-custom-ai-infrastructure/,"September 26, 2024 9:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThis article is part of a VB Special Issue called “Fit for Purpose: Tailoring AI Infrastructure.”Catch all the other stories here.AI is no longer just a buzzword — it’s a business imperative. As enterprises across industries continue to adopt AI, the conversation around AI infrastructure has evolved dramatically. Once viewed as a necessary but costly investment, custom AI infrastructure is now seen as a strategic asset that can provide a critical competitive edge.Mike Gualtieri, vice president and principal analyst atForrester, emphasizes the strategic importance of AI infrastructure. “Enterprises must invest in an enterprise AI/ML platform from a vendor that at least keeps pace with, and ideally pushes the envelope of, enterprise AI technology,” Gualtieri said. “The technology must also serve a reimagined enterprise operating in a world of abundant intelligence.” This perspective underscores the shift from viewing AI as a peripheral experiment to recognizing it as a core component of future business strategy.The infrastructure revolutionThe AI revolution has been fueled by breakthroughs in AI models and applications, but those innovations have also created new challenges. Today’s AI workloads, especially around training and inference for large language models (LLMs), require unprecedented levels of computing power. This is where custom AI infrastructure comes into play.>>Don’t miss our special issue:Fit for Purpose: Tailoring AI Infrastructure.<<“AI infrastructure is not one-size-fits-all,” says Gualtieri. “There are three key workloads: data preparation, model training and inference.” Each of these tasks has different infrastructure requirements, and getting it wrong can be costly, according to Gualtieri. For example, while data preparation often relies on traditional computing resources, training massive AI models like GPT-4o or LLaMA 3.1 necessitates specialized chips such asNvidia’s GPUs, Amazon’s Trainium or Google’s TPUs.Nvidia, in particular, has taken the lead in AI infrastructure, thanks to its GPU dominance. “Nvidia’s success wasn’t planned, but it was well-earned,” Gualtieri explains. “They were in the right place at the right time, and once they saw the potential of GPUs for AI, they doubled down.” However, Gualtieri believes that competition is on the horizon, with companies like Intel and AMD looking to close the gap.The cost of the cloudCloud computing has been a key enabler of AI, but as workloads scale, the costs associated with cloud services have become a point of concern for enterprises. According to Gualtieri, cloud services are ideal for “bursting workloads” — short-term, high-intensity tasks. However, for enterprises running AI models 24/7, the pay-as-you-go cloud model can become prohibitively expensive.“Some enterprises are realizing they need a hybrid approach,” Gualtieri said. “They might use the cloud for certain tasks but invest in on-premises infrastructure for others. It’s about balancing flexibility and cost-efficiency.”This sentiment was echoed by Ankur Mehrotra, general manager of Amazon SageMaker at AWS. In a recent interview, Mehrotra noted that AWS customers are increasingly looking for solutions that combine the flexibility of the cloud with the control and cost-efficiency of on-premise infrastructure. “What we’re hearing from our customers is that they want purpose-built capabilities for AI at scale,” Mehrotra explains. “Price performance is critical, and you can’t optimize for it with generic solutions.”To meet these demands, AWS has been enhancing its SageMaker service, which offers managed AI infrastructure and integration with popular open-source tools like Kubernetes and PyTorch. “We want to give customers the best of both worlds,” says Mehrotra. “They get the flexibility and scalability of Kubernetes, but with the performance and resilience of our managed infrastructure.”The role of open sourceOpen-source tools like PyTorch and TensorFlow have become foundational to AI development, and their role in building custom AI infrastructure cannot be overlooked. Mehrotra underscores the importance of supporting these frameworks while providing the underlying infrastructure needed to scale. “Open-source tools are table stakes,” he says. “But if you just give customers the framework without managing the infrastructure, it leads to a lot of undifferentiated heavy lifting.”AWS’s strategy is to provide a customizable infrastructure that works seamlessly with open-source frameworks while minimizing the operational burden on customers. “We don’t want our customers spending time on managing infrastructure. We want them focused on building models,” says Mehrotra.Gualtieri agrees, adding that while open-source frameworks are critical, they must be backed by robust infrastructure. “The open-source community has done amazing things for AI, but at the end of the day, you need hardware that can handle the scale and complexity of modern AI workloads,” he says.The future of AI infrastructureAs enterprises continue to navigate the AI landscape, the demand for scalable, efficient and custom AI infrastructure will only grow. This is especially true as artificial general intelligence (AGI) — or agentic AI — becomes a reality. “AGI will fundamentally change the game,” Gualtieri said. “It’s not just about training models and making predictions anymore. Agentic AI will control entire processes, and that will require a lot more infrastructure.”Mehrotra also sees the future of AI infrastructure evolving rapidly. “The pace of innovation in AI is staggering,” he says. “We’re seeing the emergence of industry-specific models, like BloombergGPT for financial services. As these niche models become more common, the need for custom infrastructure will grow.”AWS, Nvidia and other major players are racing to meet this demand by offering more customizable solutions. But as Gualtieri points out, it’s not just about the technology. “It’s also about partnerships,” he says. “Enterprises can’t do this alone. They need to work closely with vendors to ensure their infrastructure is optimized for their specific needs.”Custom AI infrastructure is no longer just a cost center — it’s a strategic investment that can provide a significant competitive edge. As enterprises scale their AI ambitions, they must carefully consider their infrastructure choices to ensure they are not only meeting today’s demands but also preparing for the future. Whether through cloud, on-premises, or hybrid solutions, the right infrastructure can make all the difference in turning AI from an experiment into a business driverVB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Automation,Ensemble raises $3.3M to bring ‘dark matter’ tech to enterprise AI,https://venturebeat.com/ai/ensemble-raises-3-3m-to-bring-dark-matter-tech-to-enterprise-ai/,"September 26, 2024 6:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMachine learning startupEnsemblehas raised$3.3 million in seed fundingto address the growing importance of data quality in artificial intelligence. Salesforce Ventures led the round, with participation from M13, Motivate, and Amplo.FoundersAlex ReneauandZach Albertsonare pioneering a novel approach to data representation that promises to enhance machine learning model performance without requiring vast amounts of additional data or complex model architectures.Unlocking hidden data relationships with ‘dark matter’ technology“We have a new way to essentially approximate hidden relationships in your data or missing information that you wish was originally in your dataset to improve your model,” said Alex Reneau, CEO of Ensemble, in an exclusive interview with VentureBeat. “We’re able to enable customers to maximize their own data that they’re working with, even when it’s limited, sparse, or highly complex, allowing them to train effective models with less comprehensive information.”The company’s proprietary “dark matter” technology slots into the machine learning pipeline between feature engineering and model training. It creates enriched data representations that can uncover latent patterns and relationships, potentially making previously unsolvable problems tractable.Addressing enterprise AI adoption challengesThis approach comes at a critical time forenterprise AI adoption. Despite rapid advances in AI capabilities, many organizations struggle to deploy models in production environments due to data quality issues.Caroline Fiegel, an investor at Salesforce Ventures, explained the rationale behind their investment: “We have maybe watched over the past 12 to 24 months, enterprises move more slowly into AI and into production than we had anticipated,” she told VenutreBeat. “When you peel that back and really start to understand why, it’s because the data is disparate. It’s kind of low quality. It’s riddled with PII.”Ensemble’s technology could have far-reaching implications across industries. The company is already working with customers in biotechnology and advertising technology, with early results showing promise in areas such as predicting virus-host interactions in the gut microbiome.From impossible to possible: Expanding the horizons of machine learning“We actually care a lot more about the cases where ML is able to do what was otherwise impossible before,” Reneau emphasized. “So it’s not just about doing what a human can do, and making it faster, but [it’s about] what a human couldn’t do.”The funding will be used to accelerate product development, expand the team, and ramp up go-to-market efforts. As the AI landscape continues to evolve rapidly, Ensemble sees its role as providing a foundational technology that can adapt to changing needs.“With these models constantly developing, and the data landscape is going to be ever-evolving, I think that we’re definitely more set—on the core research side of it,” Reneau said, hinting at the company’s long-term vision.For Salesforce Ventures, the investment aligns with their thesis on the critical role of data in AI adoption. “Building trust in AI today is really built in outcomes,” Fiegel said, “and so knowing that Alex and Zach kind of share that core north star with us is what keeps us excited.”As enterprises grapple with the challenges of implementing AI at scale, Ensemble’s approach to data quality could prove to be a key enabler. The company’s progress will be closely watched by both the tech industry and the broader business community as a potential solution to one of AI’s most persistent obstacles.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Automation,AI for all: Meta’s ‘Llama Stack’ promises to simplify enterprise adoption,https://venturebeat.com/ai/ai-for-all-meta-llama-stack-promises-to-simplify-enterprise-ai-adoption/,"September 25, 2024 11:52 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreToday at its annualMeta Connectdeveloper conference,MetalaunchedLlama Stackdistributions, a comprehensive suite of tools designed to simplify AI deployment across a wide range of computing environments. This move, announced alongside the release of the newLlama 3.2 models, represents a major step in making advanced AI capabilities more accessible and practical for businesses of all sizes.The Llama Stack introduces astandardized APIfor model customization and deployment, addressing one of the most pressing challenges in enterprise AI adoption: the complexity of integrating AI systems into existing IT infrastructures. By providing a unified interface for tasks such as fine-tuning, synthetic data generation and agentic application building, Meta positions Llama Stack as a turnkey solution for organizations looking to leverage AI without extensive in-house expertise.The Llama Stack API architecture illustrates Meta’s comprehensive approach to enterprise AI development. The multi-layered structure spans from hardware infrastructure to end-user applications, offering a standardized framework for model development, deployment, and management across diverse computing environments. Credit: MetaCloud partnerships expand Llama’s reachCentral to this initiative is Meta’s collaboration with major cloud providers and technology firms. Partnerships withAWS,Databricks,Dell Technologiesand others ensure that Llama Stack distributions will be available across a wide range of platforms, from on-premises data centers to public clouds. This multi-platform approach could prove particularly attractive to enterprises with hybrid or multi-cloud strategies, offering flexibility in how and where AI workloads are run.The introduction of Llama Stack comes at a critical juncture in the AI industry. As businesses increasingly recognize the potential of generative AI to transform operations, many have struggled with the technical complexities and resource requirements of deploying large language models. Meta’s approach, which includes both powerful cloud-based models and lightweight versions suitable for edge devices, addresses the full spectrum of enterprise AI needs.The Llama Stack Distribution architecture illustrates Meta’s comprehensive approach to AI deployment. This layered structure seamlessly connects developers, API interfaces, and diverse distribution channels, enabling flexible implementation across on-premises, cloud, and edge environments. Credit: MetaBreaking down barriers to AI adoptionThe implications for IT decision-makers are substantial. Organizations that have been hesitant to invest in AI due to concerns about vendor lock-in or the need for specialized infrastructure may find Llama Stack’s open and flexible approach compelling. The ability to run modelson-deviceorin the cloudusing the same API could enable more sophisticated AI strategies that balance performance, cost, and data privacy considerations.However, Meta’s initiative faces challenges. The company must convince enterprises of the long-term viability of itsopen-source approachin a market dominated by proprietary solutions. Additionally, concerns about data privacy and model safety need addressing, particularly for industries handling sensitive information.Meta has emphasized its commitment to responsible AI development, including the release ofLlama Guard 3, a safeguard system designed to filter potentially harmful content in both text and image inputs. This focus on safety could be crucial in winning over cautious enterprise adopters.The future of enterprise AI: Flexibility and accessibilityAs enterprises evaluate their AI strategies, Llama Stack’s promise of simplified deployment and cross-platform compatibility is likely to attract significant attention. While it’s too early to declare it the de facto standard for enterprise AI development, Meta’s bold move has undoubtedly disrupted the competitive landscape of AI infrastructure solutions.The real strength of Llama Stack is its ability to make AI development more accessible to businesses of all sizes. By simplifying the technical challenges and reducing the resources needed for AI implementation, Meta is opening the door for widespread innovation across industries. Smaller companies and startups, previously priced out of advanced AI capabilities, might now have the tools to compete with larger, resource-rich corporations.Moreover, the flexibility offered by Llama Stack could lead to more nuanced and efficient AI strategies. Companies might deploy lightweight models on edge devices for real-time processing while leveraging more powerful cloud-based models for complex analytics—all using the same underlying framework.For business and tech leaders, Llama Stack offers a simpler path to using AI across their companies. The question is no longer if they should use AI, but how to best fit it into their current systems. Meta’s new tools could speed up this process for many industries.As companies rush to adopt these new AI capabilities, one thing is clear: the race to harness AI’s potential is no longer just for tech giants. With Llama Stack, even the corner store might soon be powered by AI.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Automation,"Generative AI adoption surpasses early PC and internet usage, study finds",https://venturebeat.com/ai/generative-ai-adoption-surpasses-early-pc-and-internet-usage-study-finds/,"September 24, 2024 11:59 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThe rise of generative artificial intelligence (AI) has been a hot topic in tech circles. But new research from theFederal Reserve Bank of St. Louis,Vanderbilt University, andHarvard Kennedy Schoolreveals the true extent of generative AI’s infiltration into everyday work life—and the results are eye-opening. According to the paper,The Rapid Adoption of Generative AI, the technology has taken hold faster than previous transformative technologies like the personal computer (PC) or the internet.Here are five surprising takeaways from the study, which surveyed thousands of U.S. workers to gauge the adoption of generative AI at work and at home.1.Generative AI is already more widely adopted than PCs were at this stageGenerative AI is spreading faster than anyone could have predicted. Just two years after the public release of ChatGPT, 39.4% of Americans aged 18-64 reported using generative AI, with 28% using it at work. To put that in perspective, it took three years for PCs to hit a 20% adoption rate.“Generative AI has been adopted at a faster pace than PCs or the internet,” the researchers write. “This is driven by faster adoption of generative AI at home compared with the PC, likely because of differences in portability and cost.” The ease of access to tools like ChatGPT and Google Gemini has played a crucial role in this faster uptake.The data from the survey shows generative A.I. reaching nearly 40 percent adoption just two years after introduction, far outpacing the early adoption rates of PCs and the internet. (Credit: Federal Reserve Bank of St. Louis)2.Generative AI is being used by everyone—not just tech workersWhile you might expect generative AI to be used mostly by software developers or data scientists, the research shows that adoption is widespread across industries. In fact, one in five “blue-collar” workers—those in construction, installation, repair, and transportation—regularly use generative AI on the job.“Generative AI adoption is most common in management, business, and computer occupations, with usage rates exceeding 40%,” the paper says. “Still, one in five ‘blue-collar’ workers and one in five workers without a college degree use generative AI regularly on the job as well.”This shows that AI is no longer reserved for high-skilled or specialized roles. From writing reports to generating creative ideas, generative AI is being used in a surprising variety of tasks across the occupational spectrum.Generative A.I. usage across occupations, showing its reach beyond tech fields. While computer and management professionals lead adoption, even blue-collar workers report significant use, highlighting A.I.’s broad impact on diverse workplaces. (Credit: Federal Reserve Bank of St. Louis)3.AI adoption mirrors the trend of rising workplace inequalityJust as the PC revolution led to greater workplace inequality, with computers complementing high-skilled workers while automating routine tasks, the adoption of generative AI could accelerate this trend. The study found that younger, more educated, and higher-income workers are more likely to use AI on the job.Notably, workers with a bachelor’s degree or higher are twice as likely to use AI as those without one (40% vs. 20%). The researchers warn that this could exacerbate existing inequalities in the labor market.“Generative AI usage is more common among younger, more educated, and higher-income workers. This is notable because the PC revolution was followed by rising labor market inequality,” the authors write.Disparities in generative A.I. adoption across demographic groups reveal potential new dimensions of workplace inequality. Men, younger workers, and those with advanced degrees show higher usage rates, while adoption among those without college education lags significantly. (Credit: Federal Reserve Bank of St. Louis)4.AI Is already saving time on a variety of tasksWhen it comes to specific tasks, workers are using generative AI for more than just coding or technical work. The most common uses of AI at work include writing, administrative tasks, and interpreting text or data. In fact, 57% of those using AI at work reported using it to help with writing tasks, and 49% said they used it for searching for information.The researchers note that “usage rates at work exceeded 25% for all ten tasks in our list,” underscoring just how broadly helpful generative AI has become across job functions. Whether it’s summarizing reports or generating new ideas, AI is already saving employees significant time.A breakdown of generative A.I. usage by task in the workplace, revealing its widespread application. Writing tops the list at nearly 57%, while even less expected tasks like generating new ideas see significant adoption. The data underscores A.I.’s broad impact on diverse work activities. (Credit: Federal Reserve Bank of St. Louis)5.AI could boost U.S. labor productivity—but it’s still early daysPerhaps the most exciting finding of the study is that generative AI could provide a notable boost to labor productivity. Based on current usage patterns, the researchers estimate that between 0.5% and 3.5% of all U.S. work hours are currently being assisted by generative AI. They further estimate that this could result in a labor productivity increase of between 0.125% and 0.875%.“If we assume that generative AI increases task productivity by 25%—the median estimate across five randomized studies—this would translate to an increase in labor productivity of between 0.125 and 0.875 percentage points at current levels of usage,” the study explains.However, the authors caution that these estimates are speculative, given the early adoption stage of generative AI. While the technology’s potential is immense, its long-term impact on the economy will depend on how deeply it becomes embedded in everyday workflows.Daily time spent using generative A.I. at work, showing varied adoption intensity. While most users engage with A.I. for 15-59 minutes per day, over a quarter use it for more than an hour daily, suggesting its growing integration into workplace routines. (Credit: Federal Reserve Bank of St. Louis)VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Automation,Microsoft unveils ‘trustworthy AI’ features to fix hallucinations and boost privacy,https://venturebeat.com/ai/microsoft-unveils-trustworthy-ai-features-to-fix-hallucinations-and-boost-privacy/,"September 24, 2024 10:01 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMicrosoft unveiled a suite of new artificial intelligence safety features on Tuesday, aiming to address growing concerns about AI security, privacy, and reliability. The tech giant is branding this initiative as “Trustworthy AI,” signaling a push towards more responsible development and deployment of AI technologies.The announcement comes as businesses and organizations increasingly adopt AI solutions, bringing both opportunities and challenges. Microsoft’s new offerings includeconfidential inferencingfor itsAzure OpenAI Service, enhanced GPU security, and improved tools for evaluating AI outputs.“To make AI trustworthy, there are many, many things that you need to do, from core research innovation to this last mile engineering,” said Sarah Bird, a senior leader in Microsoft’s AI efforts, in an interview with VentureBeat. “We’re still really in the early days of this work.”Combating AI hallucinations: Microsoft’s new correction featureOne of the key features introduced is a “Correction” capability inAzure AI Content Safety. This tool aims to address the problem of AI hallucinations — instances where AI models generate false or misleading information. “When we detect there’s a mismatch between the grounding context and the response… we give that information back to the AI system,” Bird explained. “With that additional information, it’s usually able to do better the second try.”Microsoft is also expanding its efforts in “embedded content safety,” allowing AI safety checks to run directly on devices, even when offline. This feature is particularly relevant for applications like Microsoft’sCopilot for PC, which integrates AI capabilities directly into the operating system.“Bringing safety to where the AI is is something that is just incredibly important to make this actually work in practice,” Bird noted.Balancing innovation and responsibility in AI developmentThe company’s push for trustworthy AI reflects a growing industry awareness of the potential risks associated with advanced AI systems. It also positions Microsoft as a leader in responsible AI development, potentially giving it an edge in the competitive cloud computing and AI services market.However, implementing these safety features isn’t without challenges. When asked about performance impacts, Bird acknowledged the complexity: “There is a lot of work we have to do in integration to make the latency make sense… in streaming applications.”Microsoft’s approach appears to be resonating with some high-profile clients. The company highlighted collaborations with the New York CityDepartment of Educationand the South AustraliaDepartment of Education, which are using Azure AI Content Safety to create appropriate AI-powered educational tools.For businesses and organizations looking to implement AI solutions, Microsoft’s new features offer additional safeguards. However, they also highlight the increasing complexity of deploying AI responsibly, suggesting that the era of easy, plug-and-play AI may be giving way to more nuanced, security-focused implementations.The future of AI safety: Setting new industry standardsAs the AI landscape continues to evolve rapidly, Microsoft’s latest announcements underscore the ongoing tension between innovation and responsible development. “There isn’t just one quick fix,” Bird emphasized. “Everyone has a role to play in it.”Industry analysts suggest that Microsoft’s focus on AI safety could set a new standard for the tech industry. As concerns about AI ethics and security continue to grow, companies that can demonstrate a commitment to responsible AI development may gain a competitive advantage.However, some experts caution that while these new features are a step in the right direction, they are not a panacea for all AI-related concerns. The rapid pace of AI advancement means that new challenges are likely to emerge, requiring ongoing vigilance and innovation in the field of AI safety.As businesses and policymakers grapple with the implications of widespread AI adoption, Microsoft’s “Trustworthy AI” initiative represents a significant effort to address these concerns. Whether it will be enough to allay all fears about AI safety remains to be seen, but it’s clear that major tech players are taking the issue seriously.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Automation,OpenAI tackles global language divide with massive multilingual AI dataset release,https://venturebeat.com/ai/openai-tackles-global-language-divide-with-massive-multilingual-ai-dataset-release/,"September 23, 2024 5:33 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreOpenAI took a major step toward expanding the global reach of artificial intelligence by releasing a multilingual dataset that evaluates the performance of language models across 14 languages, including Arabic, German, Swahili, Bengali and Yoruba.The company shared theMultilingual Massive Multitask Language Understanding (MMMLU) dataseton the open data platform Hugging Face. This new evaluation builds on the popularMassive Multitask Language Understanding (MMLU) benchmark, which tested an AI system’s knowledge across 57 disciplines from mathematics to law and computer science, but only in English.By incorporating a diverse array of languages into the new multilingual evaluation, some of which have limited resources for AI training data, OpenAI set a new benchmark for multilingual AI capabilities. This benchmark could open up more equitable global access to the technology. The AI industry has faced criticism for its inability to develop language models that can understand languages spoken by millions of people worldwide.OpenAI delivers global benchmark for evaluating multilingual AIThe MMMLU dataset challenges AI models to perform in diverse linguistic environments, reflecting the growing need for AI systems that can engage with users across the globe. As businesses and governments increasingly adopt AI-driven solutions, the demand for models that can understand and generate text inmultiple languageshas become more pressing.Until recently, AI research has focusedprimarily on Englishand a few widely spoken languages, leaving many low-resource languages behind. OpenAI’s decision to include languages like Swahili and Yoruba, spoken by millions but often neglected in AI research, signals a shift toward more inclusive AI technology. This move is especially important for enterprises looking to deploy AI solutions in emerging markets, where language barriers have traditionally posed significant challenges.Human translation raises the bar for multilingual AI accuracyOpenAI used professionalhuman translatorsto create the MMMLU dataset, ensuring higher accuracy than comparable datasets that rely on machine translation. Automated translation tools often introduce subtle errors, particularly in languages with fewer resources to train on. By relying on human expertise, OpenAI ensures that the dataset provides a more reliable foundation for evaluating AI models in multiple languages.This decision is crucial for industries where precision is non-negotiable. In sectors like healthcare, law, and finance, even minor translation errors can have serious implications. OpenAI’s focus on translation quality positions the MMMLU dataset as a critical tool for enterprises that require AI systems to perform reliably across linguistic and cultural boundaries.Hugging Face partnership boosts open access to multilingual AI dataBy releasing the MMMLU dataset on Hugging Face, a popular platform for sharing machine learning models and datasets, OpenAI is engaging the broader AI research community. Hugging Face has become a go-to destination for open-source AI tools, and the addition of the MMMLU dataset signals OpenAI’s commitment to advancing open access in AI research.However, this release comes at a time when OpenAI has faced growing scrutiny over its approach to openness.Criticism has mountedin recent months, especially fromco-founder Elon Musk, who has accused the company of straying from its original mission of being an open-source, nonprofit entity.Musk’s lawsuit, filed earlier this year, claims that OpenAI’s shift toward for-profit activities—particularly its partnership with Microsoft—contradicts the company’s founding principles.Despite this, OpenAI has defended its current strategy, arguing that it prioritizes “open access” rather than open source. In this framework, OpenAI aims to provide broad access to its technologies without necessarily sharing the inner workings of its most advanced models. The release of the MMMLU dataset fits within this philosophy, offering the research community a powerful tool while maintaining control over its proprietary models.OpenAI Academy: Expanding access to AI in emerging marketsIn addition to the MMMLU dataset release, OpenAI is furthering its commitment to global AI accessibility through the launch of theOpenAI Academy. Announced on the same day as the MMMLU dataset, the Academy is designed to invest in developers and mission-driven organizations that are leveraging AI to tackle critical problems in their communities, particularly in low- and middle-income countries.The Academy will provide training, technical guidance, and$1 million in API creditsto ensure that local AI talent can access cutting-edge resources. By supporting developers who understand the unique social and economic challenges of their regions, OpenAI hopes to empower communities to build AI applications tailored to local needs.This initiative complements the MMMLU dataset by emphasizing OpenAI’s goal of making advanced AI tools and education available to diverse, global communities. Both the MMMLU dataset and the Academy reflect OpenAI’s long-term strategy of ensuring that AI development benefits all of humanity, especially communities that have traditionally been underserved by the latest AI advancements.Multilingual AI gives businesses a competitive edgeFor enterprises, the MMMLU dataset presents an opportunity to benchmark their own AI systems in aglobal context. As companies expand into international markets, the ability to deploy AI solutions that understand multiple languages becomes critical. Whether it’s customer service, content moderation, or data analysis, AI systems that perform well across languages can offer a competitive advantage by reducing friction in communication and improving user experience.The dataset’s focus on professional and academic subjects adds another layer of value for businesses. Companies in law, education, and research can use the MMMLU dataset to test how well their AI models perform in specialized domains, ensuring that their systems meet the high standards required for these sectors. As AI continues to evolve, the ability to handle complex, domain-specific tasks in multiple languages will become a key differentiator for businesses competing on a global stage.A multilingual future: What the MMMLU dataset means for AIThe release of the MMMLU dataset is likely to have lasting implications for the AI industry. As more companies and researchers begin to test their models against this multilingual benchmark, the demand for AI systems that can operate seamlessly across languages will only grow. This could lead to new innovations in language processing, as well as greater adoption of AI solutions in parts of the world that have traditionally been underserved by technology.For OpenAI, the MMMLU dataset represents both a challenge and an opportunity. On one hand, the company is positioning itself as a leader in multilingual AI, offering tools that address a critical gap in the current AI landscape. On the other hand, OpenAI’s evolving stance on openness will continue to be scrutinized as it navigates the tensions between public good and private interest.As AI becomes increasingly integrated into the global economy, companies and governments alike will need to grapple with the ethical and practical implications of these technologies. OpenAI’s release of the MMMLU dataset is a step in the right direction, but it also raises important questions about how much of the AI revolution will be open to all.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Enterprise Analytics,Ensemble raises $3.3M to bring ‘dark matter’ tech to enterprise AI,https://venturebeat.com/ai/ensemble-raises-3-3m-to-bring-dark-matter-tech-to-enterprise-ai/,"September 26, 2024 6:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMachine learning startupEnsemblehas raised$3.3 million in seed fundingto address the growing importance of data quality in artificial intelligence. Salesforce Ventures led the round, with participation from M13, Motivate, and Amplo.FoundersAlex ReneauandZach Albertsonare pioneering a novel approach to data representation that promises to enhance machine learning model performance without requiring vast amounts of additional data or complex model architectures.Unlocking hidden data relationships with ‘dark matter’ technology“We have a new way to essentially approximate hidden relationships in your data or missing information that you wish was originally in your dataset to improve your model,” said Alex Reneau, CEO of Ensemble, in an exclusive interview with VentureBeat. “We’re able to enable customers to maximize their own data that they’re working with, even when it’s limited, sparse, or highly complex, allowing them to train effective models with less comprehensive information.”The company’s proprietary “dark matter” technology slots into the machine learning pipeline between feature engineering and model training. It creates enriched data representations that can uncover latent patterns and relationships, potentially making previously unsolvable problems tractable.Addressing enterprise AI adoption challengesThis approach comes at a critical time forenterprise AI adoption. Despite rapid advances in AI capabilities, many organizations struggle to deploy models in production environments due to data quality issues.Caroline Fiegel, an investor at Salesforce Ventures, explained the rationale behind their investment: “We have maybe watched over the past 12 to 24 months, enterprises move more slowly into AI and into production than we had anticipated,” she told VenutreBeat. “When you peel that back and really start to understand why, it’s because the data is disparate. It’s kind of low quality. It’s riddled with PII.”Ensemble’s technology could have far-reaching implications across industries. The company is already working with customers in biotechnology and advertising technology, with early results showing promise in areas such as predicting virus-host interactions in the gut microbiome.From impossible to possible: Expanding the horizons of machine learning“We actually care a lot more about the cases where ML is able to do what was otherwise impossible before,” Reneau emphasized. “So it’s not just about doing what a human can do, and making it faster, but [it’s about] what a human couldn’t do.”The funding will be used to accelerate product development, expand the team, and ramp up go-to-market efforts. As the AI landscape continues to evolve rapidly, Ensemble sees its role as providing a foundational technology that can adapt to changing needs.“With these models constantly developing, and the data landscape is going to be ever-evolving, I think that we’re definitely more set—on the core research side of it,” Reneau said, hinting at the company’s long-term vision.For Salesforce Ventures, the investment aligns with their thesis on the critical role of data in AI adoption. “Building trust in AI today is really built in outcomes,” Fiegel said, “and so knowing that Alex and Zach kind of share that core north star with us is what keeps us excited.”As enterprises grapple with the challenges of implementing AI at scale, Ensemble’s approach to data quality could prove to be a key enabler. The company’s progress will be closely watched by both the tech industry and the broader business community as a potential solution to one of AI’s most persistent obstacles.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Enterprise Analytics,"Generative AI adoption surpasses early PC and internet usage, study finds",https://venturebeat.com/ai/generative-ai-adoption-surpasses-early-pc-and-internet-usage-study-finds/,"September 24, 2024 11:59 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThe rise of generative artificial intelligence (AI) has been a hot topic in tech circles. But new research from theFederal Reserve Bank of St. Louis,Vanderbilt University, andHarvard Kennedy Schoolreveals the true extent of generative AI’s infiltration into everyday work life—and the results are eye-opening. According to the paper,The Rapid Adoption of Generative AI, the technology has taken hold faster than previous transformative technologies like the personal computer (PC) or the internet.Here are five surprising takeaways from the study, which surveyed thousands of U.S. workers to gauge the adoption of generative AI at work and at home.1.Generative AI is already more widely adopted than PCs were at this stageGenerative AI is spreading faster than anyone could have predicted. Just two years after the public release of ChatGPT, 39.4% of Americans aged 18-64 reported using generative AI, with 28% using it at work. To put that in perspective, it took three years for PCs to hit a 20% adoption rate.“Generative AI has been adopted at a faster pace than PCs or the internet,” the researchers write. “This is driven by faster adoption of generative AI at home compared with the PC, likely because of differences in portability and cost.” The ease of access to tools like ChatGPT and Google Gemini has played a crucial role in this faster uptake.The data from the survey shows generative A.I. reaching nearly 40 percent adoption just two years after introduction, far outpacing the early adoption rates of PCs and the internet. (Credit: Federal Reserve Bank of St. Louis)2.Generative AI is being used by everyone—not just tech workersWhile you might expect generative AI to be used mostly by software developers or data scientists, the research shows that adoption is widespread across industries. In fact, one in five “blue-collar” workers—those in construction, installation, repair, and transportation—regularly use generative AI on the job.“Generative AI adoption is most common in management, business, and computer occupations, with usage rates exceeding 40%,” the paper says. “Still, one in five ‘blue-collar’ workers and one in five workers without a college degree use generative AI regularly on the job as well.”This shows that AI is no longer reserved for high-skilled or specialized roles. From writing reports to generating creative ideas, generative AI is being used in a surprising variety of tasks across the occupational spectrum.Generative A.I. usage across occupations, showing its reach beyond tech fields. While computer and management professionals lead adoption, even blue-collar workers report significant use, highlighting A.I.’s broad impact on diverse workplaces. (Credit: Federal Reserve Bank of St. Louis)3.AI adoption mirrors the trend of rising workplace inequalityJust as the PC revolution led to greater workplace inequality, with computers complementing high-skilled workers while automating routine tasks, the adoption of generative AI could accelerate this trend. The study found that younger, more educated, and higher-income workers are more likely to use AI on the job.Notably, workers with a bachelor’s degree or higher are twice as likely to use AI as those without one (40% vs. 20%). The researchers warn that this could exacerbate existing inequalities in the labor market.“Generative AI usage is more common among younger, more educated, and higher-income workers. This is notable because the PC revolution was followed by rising labor market inequality,” the authors write.Disparities in generative A.I. adoption across demographic groups reveal potential new dimensions of workplace inequality. Men, younger workers, and those with advanced degrees show higher usage rates, while adoption among those without college education lags significantly. (Credit: Federal Reserve Bank of St. Louis)4.AI Is already saving time on a variety of tasksWhen it comes to specific tasks, workers are using generative AI for more than just coding or technical work. The most common uses of AI at work include writing, administrative tasks, and interpreting text or data. In fact, 57% of those using AI at work reported using it to help with writing tasks, and 49% said they used it for searching for information.The researchers note that “usage rates at work exceeded 25% for all ten tasks in our list,” underscoring just how broadly helpful generative AI has become across job functions. Whether it’s summarizing reports or generating new ideas, AI is already saving employees significant time.A breakdown of generative A.I. usage by task in the workplace, revealing its widespread application. Writing tops the list at nearly 57%, while even less expected tasks like generating new ideas see significant adoption. The data underscores A.I.’s broad impact on diverse work activities. (Credit: Federal Reserve Bank of St. Louis)5.AI could boost U.S. labor productivity—but it’s still early daysPerhaps the most exciting finding of the study is that generative AI could provide a notable boost to labor productivity. Based on current usage patterns, the researchers estimate that between 0.5% and 3.5% of all U.S. work hours are currently being assisted by generative AI. They further estimate that this could result in a labor productivity increase of between 0.125% and 0.875%.“If we assume that generative AI increases task productivity by 25%—the median estimate across five randomized studies—this would translate to an increase in labor productivity of between 0.125 and 0.875 percentage points at current levels of usage,” the study explains.However, the authors caution that these estimates are speculative, given the early adoption stage of generative AI. While the technology’s potential is immense, its long-term impact on the economy will depend on how deeply it becomes embedded in everyday workflows.Daily time spent using generative A.I. at work, showing varied adoption intensity. While most users engage with A.I. for 15-59 minutes per day, over a quarter use it for more than an hour daily, suggesting its growing integration into workplace routines. (Credit: Federal Reserve Bank of St. Louis)VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Enterprise Analytics,Microsoft unveils ‘trustworthy AI’ features to fix hallucinations and boost privacy,https://venturebeat.com/ai/microsoft-unveils-trustworthy-ai-features-to-fix-hallucinations-and-boost-privacy/,"September 24, 2024 10:01 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMicrosoft unveiled a suite of new artificial intelligence safety features on Tuesday, aiming to address growing concerns about AI security, privacy, and reliability. The tech giant is branding this initiative as “Trustworthy AI,” signaling a push towards more responsible development and deployment of AI technologies.The announcement comes as businesses and organizations increasingly adopt AI solutions, bringing both opportunities and challenges. Microsoft’s new offerings includeconfidential inferencingfor itsAzure OpenAI Service, enhanced GPU security, and improved tools for evaluating AI outputs.“To make AI trustworthy, there are many, many things that you need to do, from core research innovation to this last mile engineering,” said Sarah Bird, a senior leader in Microsoft’s AI efforts, in an interview with VentureBeat. “We’re still really in the early days of this work.”Combating AI hallucinations: Microsoft’s new correction featureOne of the key features introduced is a “Correction” capability inAzure AI Content Safety. This tool aims to address the problem of AI hallucinations — instances where AI models generate false or misleading information. “When we detect there’s a mismatch between the grounding context and the response… we give that information back to the AI system,” Bird explained. “With that additional information, it’s usually able to do better the second try.”Microsoft is also expanding its efforts in “embedded content safety,” allowing AI safety checks to run directly on devices, even when offline. This feature is particularly relevant for applications like Microsoft’sCopilot for PC, which integrates AI capabilities directly into the operating system.“Bringing safety to where the AI is is something that is just incredibly important to make this actually work in practice,” Bird noted.Balancing innovation and responsibility in AI developmentThe company’s push for trustworthy AI reflects a growing industry awareness of the potential risks associated with advanced AI systems. It also positions Microsoft as a leader in responsible AI development, potentially giving it an edge in the competitive cloud computing and AI services market.However, implementing these safety features isn’t without challenges. When asked about performance impacts, Bird acknowledged the complexity: “There is a lot of work we have to do in integration to make the latency make sense… in streaming applications.”Microsoft’s approach appears to be resonating with some high-profile clients. The company highlighted collaborations with the New York CityDepartment of Educationand the South AustraliaDepartment of Education, which are using Azure AI Content Safety to create appropriate AI-powered educational tools.For businesses and organizations looking to implement AI solutions, Microsoft’s new features offer additional safeguards. However, they also highlight the increasing complexity of deploying AI responsibly, suggesting that the era of easy, plug-and-play AI may be giving way to more nuanced, security-focused implementations.The future of AI safety: Setting new industry standardsAs the AI landscape continues to evolve rapidly, Microsoft’s latest announcements underscore the ongoing tension between innovation and responsible development. “There isn’t just one quick fix,” Bird emphasized. “Everyone has a role to play in it.”Industry analysts suggest that Microsoft’s focus on AI safety could set a new standard for the tech industry. As concerns about AI ethics and security continue to grow, companies that can demonstrate a commitment to responsible AI development may gain a competitive advantage.However, some experts caution that while these new features are a step in the right direction, they are not a panacea for all AI-related concerns. The rapid pace of AI advancement means that new challenges are likely to emerge, requiring ongoing vigilance and innovation in the field of AI safety.As businesses and policymakers grapple with the implications of widespread AI adoption, Microsoft’s “Trustworthy AI” initiative represents a significant effort to address these concerns. Whether it will be enough to allay all fears about AI safety remains to be seen, but it’s clear that major tech players are taking the issue seriously.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Enterprise Analytics,OpenAI tackles global language divide with massive multilingual AI dataset release,https://venturebeat.com/ai/openai-tackles-global-language-divide-with-massive-multilingual-ai-dataset-release/,"September 23, 2024 5:33 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreOpenAI took a major step toward expanding the global reach of artificial intelligence by releasing a multilingual dataset that evaluates the performance of language models across 14 languages, including Arabic, German, Swahili, Bengali and Yoruba.The company shared theMultilingual Massive Multitask Language Understanding (MMMLU) dataseton the open data platform Hugging Face. This new evaluation builds on the popularMassive Multitask Language Understanding (MMLU) benchmark, which tested an AI system’s knowledge across 57 disciplines from mathematics to law and computer science, but only in English.By incorporating a diverse array of languages into the new multilingual evaluation, some of which have limited resources for AI training data, OpenAI set a new benchmark for multilingual AI capabilities. This benchmark could open up more equitable global access to the technology. The AI industry has faced criticism for its inability to develop language models that can understand languages spoken by millions of people worldwide.OpenAI delivers global benchmark for evaluating multilingual AIThe MMMLU dataset challenges AI models to perform in diverse linguistic environments, reflecting the growing need for AI systems that can engage with users across the globe. As businesses and governments increasingly adopt AI-driven solutions, the demand for models that can understand and generate text inmultiple languageshas become more pressing.Until recently, AI research has focusedprimarily on Englishand a few widely spoken languages, leaving many low-resource languages behind. OpenAI’s decision to include languages like Swahili and Yoruba, spoken by millions but often neglected in AI research, signals a shift toward more inclusive AI technology. This move is especially important for enterprises looking to deploy AI solutions in emerging markets, where language barriers have traditionally posed significant challenges.Human translation raises the bar for multilingual AI accuracyOpenAI used professionalhuman translatorsto create the MMMLU dataset, ensuring higher accuracy than comparable datasets that rely on machine translation. Automated translation tools often introduce subtle errors, particularly in languages with fewer resources to train on. By relying on human expertise, OpenAI ensures that the dataset provides a more reliable foundation for evaluating AI models in multiple languages.This decision is crucial for industries where precision is non-negotiable. In sectors like healthcare, law, and finance, even minor translation errors can have serious implications. OpenAI’s focus on translation quality positions the MMMLU dataset as a critical tool for enterprises that require AI systems to perform reliably across linguistic and cultural boundaries.Hugging Face partnership boosts open access to multilingual AI dataBy releasing the MMMLU dataset on Hugging Face, a popular platform for sharing machine learning models and datasets, OpenAI is engaging the broader AI research community. Hugging Face has become a go-to destination for open-source AI tools, and the addition of the MMMLU dataset signals OpenAI’s commitment to advancing open access in AI research.However, this release comes at a time when OpenAI has faced growing scrutiny over its approach to openness.Criticism has mountedin recent months, especially fromco-founder Elon Musk, who has accused the company of straying from its original mission of being an open-source, nonprofit entity.Musk’s lawsuit, filed earlier this year, claims that OpenAI’s shift toward for-profit activities—particularly its partnership with Microsoft—contradicts the company’s founding principles.Despite this, OpenAI has defended its current strategy, arguing that it prioritizes “open access” rather than open source. In this framework, OpenAI aims to provide broad access to its technologies without necessarily sharing the inner workings of its most advanced models. The release of the MMMLU dataset fits within this philosophy, offering the research community a powerful tool while maintaining control over its proprietary models.OpenAI Academy: Expanding access to AI in emerging marketsIn addition to the MMMLU dataset release, OpenAI is furthering its commitment to global AI accessibility through the launch of theOpenAI Academy. Announced on the same day as the MMMLU dataset, the Academy is designed to invest in developers and mission-driven organizations that are leveraging AI to tackle critical problems in their communities, particularly in low- and middle-income countries.The Academy will provide training, technical guidance, and$1 million in API creditsto ensure that local AI talent can access cutting-edge resources. By supporting developers who understand the unique social and economic challenges of their regions, OpenAI hopes to empower communities to build AI applications tailored to local needs.This initiative complements the MMMLU dataset by emphasizing OpenAI’s goal of making advanced AI tools and education available to diverse, global communities. Both the MMMLU dataset and the Academy reflect OpenAI’s long-term strategy of ensuring that AI development benefits all of humanity, especially communities that have traditionally been underserved by the latest AI advancements.Multilingual AI gives businesses a competitive edgeFor enterprises, the MMMLU dataset presents an opportunity to benchmark their own AI systems in aglobal context. As companies expand into international markets, the ability to deploy AI solutions that understand multiple languages becomes critical. Whether it’s customer service, content moderation, or data analysis, AI systems that perform well across languages can offer a competitive advantage by reducing friction in communication and improving user experience.The dataset’s focus on professional and academic subjects adds another layer of value for businesses. Companies in law, education, and research can use the MMMLU dataset to test how well their AI models perform in specialized domains, ensuring that their systems meet the high standards required for these sectors. As AI continues to evolve, the ability to handle complex, domain-specific tasks in multiple languages will become a key differentiator for businesses competing on a global stage.A multilingual future: What the MMMLU dataset means for AIThe release of the MMMLU dataset is likely to have lasting implications for the AI industry. As more companies and researchers begin to test their models against this multilingual benchmark, the demand for AI systems that can operate seamlessly across languages will only grow. This could lead to new innovations in language processing, as well as greater adoption of AI solutions in parts of the world that have traditionally been underserved by technology.For OpenAI, the MMMLU dataset represents both a challenge and an opportunity. On one hand, the company is positioning itself as a leader in multilingual AI, offering tools that address a critical gap in the current AI landscape. On the other hand, OpenAI’s evolving stance on openness will continue to be scrutinized as it navigates the tensions between public good and private interest.As AI becomes increasingly integrated into the global economy, companies and governments alike will need to grapple with the ethical and practical implications of these technologies. OpenAI’s release of the MMMLU dataset is a step in the right direction, but it also raises important questions about how much of the AI revolution will be open to all.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Enterprise Analytics,"OpenAI expands o1 AI models to enterprise and education, competing directly with Anthropic",https://venturebeat.com/ai/openai-expands-o1-ai-models-to-enterprise-and-education-competing-directly-with-anthropic/,"September 19, 2024 1:07 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreOpenAIhas made its latest AI models,o1-previewando1-mini, available to allChatGPT EnterpriseandChatGPT Educustomers. These models, designed to handle complex reasoning tasks, are poised to change how organizations and academic institutions tackle their most difficult challenges, from advanced coding to scientific research.The o1 models, first announced earlier this month, represent OpenAI’s most advanced attempt yet at creating AI capable of deep, multi-step reasoning. By imitating human thought processes, these models can solve intricate problems that earlier AI iterations struggled with, offering new possibilities for industries reliant on advanced problem-solving.We're releasing a preview of OpenAI o1—a new series of AI models designed to spend more time thinking before they respond.These models can reason through complex tasks and solve harder problems than previous models in science, coding, and math.https://t.co/peKzzKX1bu— OpenAI (@OpenAI)September 12, 2024AI designed to think: What makes o1 models differentThe o1-preview and o1-mini models are built to think more critically and deeply than their predecessors. OpenAI trained these models to spend more time processing information before responding, allowing them to handle complex tasks in areas like mathematics, coding, and scientific discovery.In early tests, o1-preview demonstrated its capabilities bysolving 83% of problemsin a qualifying exam for the International Mathematics Olympiad—a substantial improvement over GPT-4o, which managed only 13%. Similarly, the model excelled in coding competitions, ranking in the89th percentileon Codeforces, a platform where coding skills are rigorously tested.The smaller, more cost-efficient o1-mini model is tailored specifically for coding tasks, offering a more affordable option for companies that need advanced problem-solving without the need for broad world knowledge. This makes o1-mini particularly useful for tasks like generating and debugging complex code, providing an accessible option for smaller businesses and developers.Why o1 models are a game-changer for enterprisesFor enterprise customers, the new o1 models represent a significant leap forward. Businesses across industries—from finance to healthcare—are increasingly turning to AI not just for automation but to solve intricate, high-stakes problems where human expertise is limited. The o1 models’ ability to reason, refine strategies, and recognize mistakes makes them ideal for these use cases.These capabilities are particularly attractive for companies dealing with complex data sets and workflows. The o1-preview model, for example, can assist physicists in generating complex quantum optics formulas or help healthcare researchers annotate large-scale genomic data. This is a stark contrast from earlier AI models that primarily handled repetitive, low-level tasks.I just had o1 write a major cancer treatment project based on a very specific immunological approach. It created the full framework of the project in under a minute, with highly creative aims, approaches, and even considerations for potential pitfalls and alternative strategies…— Derya Unutmaz, MD (@DeryaTR_)September 14, 2024Dr. Derya Unutmaz, an immunologist at The Jackson Laboratory, recently used the o1-preview model to write a cancer treatment proposal. “It created the full framework of the project in under a minute, with highly creative aims and even considerations for potential pitfalls,” he posted on X.com (formerly Twitter). “This would have taken me days, if not longer, to prepare,” he added, noting that the model brought up ideas he might not have considered himself, even with 30 years of experience in the field.This kind of productivity and creativity boost is why so many businesses are eager to integrate these models into their workflows. OpenAI’s decision to prioritize enterprise customers with this release highlights its strategy to capture the high-value, high-complexity segment of the AI market.Educational institutions stand to benefit immenselyThe o1 models are also a powerful tool for educational institutions. Universities and research centers often face resource and time constraints when conducting complex data analysis or research. By making these models available to ChatGPT Edu customers, OpenAI is giving students and researchers access to cutting-edge AI tools that can help them tackle some of the most difficult problems in their respective fields.That feeling when ChatGPT o1 accomplishes in 1 hour what took you about a year in your PhD:https://t.co/jG7UxEUT12— Dr. Kyle Kabasares (@AstronoMisfit)September 15, 2024Initial feedback from the academic community has been overwhelmingly positive. Dr. Kyle Kabasares, an astrophysicist at the Bay Area Environmental Research Institute, posted on X.com that o1-preview “accomplished in 1 hour what took me about a year during my PhD.” In fields like computational fluid dynamics and immunology, where complex calculations and data analysis are routine, the o1 models have already proven their value by speeding up research processes and offering new insights.The o1 models are also poised to change how students learn. By handling more complex tasks, these models allow students to focus on higher-level thinking rather than getting bogged down in rote processes. This shift could lead to more innovation and creativity in academic research, accelerating breakthroughs in fields ranging from physics to biology.Safety and governance: OpenAI’s commitment to responsible AIIn addition to their advanced capabilities, the o1 models come with enhanced safety features. OpenAI has developed anew safety training approachthat allows these models to reason through ethical guidelines and safety rules. This is crucial for enterprises and educational institutions handling sensitive data.OpenAI has stated that it does not use customer data for training, ensuring that proprietary information remains secure. The company has also introduced rigorous safety evaluations, including a test known as “jailbreaking,” where o1-preview scored 84 out of 100, far surpassing GPT-4o’s score of 22. This means the o1 models are better equipped to resist attempts to bypass safety protocols, a critical feature for businesses concerned about compliance and data privacy.In a broader context, OpenAI has formalized partnerships with AI safety institutes in theU.S.andU.K., giving these organizations early access to the models for independent testing. This collaboration aims to ensure that AI advancements are aligned with ethical guidelines and regulatory frameworks, a growing concern as AI systems become more autonomous and integrated into daily operations.The competitive landscape: OpenAI vs. AnthropicThe release of the o1 models positions OpenAI as a leader in the highly competitive AI enterprise space. However, the company faces strong competition. Anthropic, another major player in AI, recently launched its own enterprise-focused model,Claude Enterprise, which offers amassive 500,000-token context window—more than double what OpenAI’s models currently provide. While Anthropic’s models excel in processing large data sets, OpenAI’s strength lies in its focus on deep reasoning and problem-solving.OpenAI’s ability to integrate these advanced models into its existing enterprise and educational offerings gives it a competitive edge. While Anthropic may have the upper hand in data processing capacity, OpenAI’s focus on reasoning tasks could give it a long-term advantage, especially in industries where problem-solving is more valuable than sheer data crunching.The future of AI in business and educationThe introduction of OpenAI’s o1-preview and o1-mini models signals a turning point in the landscape of artificial intelligence. These models go beyond automating routine tasks—they’re designed to think critically, making them true partners in tackling the toughest challenges in industries like healthcare, quantum research, and advanced coding.As businesses and educational institutions increasingly rely on AI for high-stakes decision-making and complex problem-solving, the impact of these models could reshape what we expect from intelligent systems.In a world where innovation often happens at the intersection of technology and human insight, the o1 series offers a bridge to the future. It’s no longer about what AI can do—it’s about what AIshoulddo. And with OpenAI’s latest leap forward, the answer seems clear: it should do a lot more.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Data Decision Makers,Have we reached peak human?,https://venturebeat.com/ai/have-we-reached-peak-human/,"September 18, 2024 10:05 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreTwo weeks ago, OpenAI’s former chief scientist Ilya Sutskeverraised $1 billionto back his newly formed company, Safe Superintelligence (SSI). The startup aims to safely build AI systems that exceed human cognitive capabilities. Just a few months before that, Elon Musk’s startup xAI raised $6 billion to pursue superintelligence, a goal Musk predicts will beachieved within five or six years. These are staggering rounds of funding for newly formed companies, and it only adds to the many billions already poured into OpenAI, Anthropic and other firms racing to build superintelligence.As a longtime researcher in this field, I agree with Musk that superintelligence will be achieved within years, not decades, but I am skeptical that it can be achieved safely. Instead, I believe we must view this milestone as an“evolutionary pressure point”for humanity — one in which our fitness as a species will be challenged by superior intelligences with interests that will eventually conflict with our own.I often compare this milestoneto the arrival of an advanced alien speciesfrom another planet and point out the “Arrival Mind Paradox” — the fact that we would fear a superior alien intelligence far more than we fear the superior intelligences we’re currently building here on earth. This is because most people wrongly believe we are crafting AI systems to “be human.” This is not true. We are building AI systems to be very good at pretending to be human, and to know humans inside and out. But the way their brains work is very different from ours — as different as any alien brain that might show up from afar.And yet, we continue to push for superintelligence. In fact, 2024 may go down as the year we reach “Peak Human.” By this I mean, the moment in time when AI systems can cognitively outperform more than half of human adults.  After we pass that milestone, we will steadily lose our cognitive edge until AI systems can outthink all individual humans — even the most brilliant among us.AI beats one-third of humans on reasoning tasksUntil recently, the average human could easily outperform even the most powerful AI systems when it comes to basic reasoning tasks. There are many ways to measure reasoning, none-of-which are considered the gold standard, but the best known is the classic IQ test. Journalist Maxim Lott has been testing all major large language models (LLMs) on a standardized Mensa IQ test. Last week, for the very first time, an AI model significantly exceeded the median human IQ score of 100. The model that crossed the peak of the bell curve was OpenAI’s new “o1” system — itreportedly scored a 120 IQ. So, does this mean AI has exceeded the reasoning ability of most humans?Not so fast. It is not quite valid to administer standard IQ tests to AI systems because the data they trained on likely included the tests (and answers), which is fundamentally unfair. To address this, Lott had a custom IQ test created that does not appear anywhere online and therefore is not in the training data. He gave that “offline test” to OpenAI’s o1 model and itscored an IQ of 95.This is still an extremely impressive result. That score beats 37% of adults on the reasoning tasks. It also represents a rapid increase, as OpenAI’s previous model GPT-4 (which was just released last year) was outperformed by 98% of adults on the same test. At this rate of progress, it is very likely that an AI model will be able to beat 50% of adult humans on standard IQ tests this year.Does this mean we will reach peak human in 2024?Yes and no.First, I predict yes, at least one foundational AI model will be released in 2024 that can outthink more than 50% of adult humans on pure reasoning tasks. From this perspective, we will exceed my definition for peak human and will be on a downward path towards the rapidly approaching day when an AI is released that can outperform all individual humans, period.Second, I need to point out that we humans have another trick up our sleeves. It’s called collective intelligence, and it relates to the fact that human groups can be smarter than individuals. And we humans have a lot of individuals — more than 8 billion at the moment.I bring this up because my personal focus as an AI researcher over the last decade has been the use of AI to connect groups of humans together into real-time systems that amplify our collective intelligence to superhuman levels. I call this goalcollective superintelligence, and I believe it is a viable pathway for keeping humanity cognitively competitive even after AI systems can outperform the reasoning ability of every individual among us. I like to think of this as “peak humanity,” and I am confident we can push it to intelligence levels that will surprise us all.Back in 2019, my research team atUnanimous AIconducted our first experiments in which we enabled groups of people to take IQ tests together by forming real-time systems mediated by Ai algorithms. This first-generation technology called “Swarm AI” enabled small groups of 6 to 10 randomly selected participants (who averaged 100 IQ) to amplify their collective performance to a collective IQ score of 114 when deliberating as an AI facilitated system (Willcox and Rosenberg). This was a good start, but not within striking distance of Collective Superintelligence.More recently, we unveiled a new technology calledconversational swarm intelligence(CSI). It enables large groups (up to 400 people) to hold real-time conversational deliberations that amplify the group’s collective intelligence. In collaboration with Carnegie Mellon University, we conducted a 2024 study in which groups of 35 randomly selected people were tasked with taking IQ test questions together in real-time as AI-facilitated “conversational swarms.” As published this year, thegroups averaged IQ scores of 128(the 97th percentile). This is a strong result, but I believe we are just scratching the surface of how smart humans can become when we use AI to think together in far larger groups.I am passionate about pursuingcollective superintelligencebecause it has the potential to greatly amplify humanity’s cognitive abilities, and unlike a digital superintelligence it is inherently instilled with human values, morals, sensibilities and interests. Of course, this begs the question — how long can we stay ahead of the purely digital AI systems? That depends on whether AI continues to advance at an accelerating pace or if we hit a plateau. Either way, amplifying our collective intelligence might help us maintain our edge long enough to figure out how to protect ourselves from being outmatched.When I raise the issue of peak human, many people point out that human intelligence is far more than just the logic and reasoning measured by IQ tests. I fully agree, but when we look at the most “human” of all qualities — creativity and artistry — we see evidence that AI systems are catching up with us just as quickly. It was only a few years ago that virtually all artwork was crafted by humans. Arecent analysisestimates that generative AI is producing 15 billion images per year and that rate is accelerating.Even more surprising, astudy published just last weekshowed that AI chatbots can outperform humans on creativity tests. To quote the paper, “the results suggest that AI has reached at least the same level, or even surpassed, the average human’s ability to generate ideas in the most typical test of creative thinking (AUT).”I’m not sure I fully believe this result, but it’s just a matter of time before it holds true.Whether we like it or not, our evolutionary position as the smartest and most creative brains on planet earth is likely to be challenged in the near future. We can debate whether this will be a net positive or a net negative for humanity, but either way, we need to be doing more toprotect ourselvesfrom being outmatched.Louis Rosenberg, is a computer scientist and entrepreneur in the fields of AI  and mixed reality. His new book,Our Next Reality, explores the impact of AI and spatial computing on humanity.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Data Decision Makers,Why data science alone won’t make your product successful,https://venturebeat.com/programming-development/why-data-science-alone-wont-make-your-product-successful/,"September 15, 2024 12:15 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThe last decade has seen the divide between tech and commercial teams thin almost to the point of nonexistence. And I, for one, am in favor of it. Not every tech team works in a tech company, and blurring the lines between the commercial and technological means that we can build and ship product safe in the knowledge that it will be well received, widely adopted (not always a given), and contribute meaningfullyto the bottom line. Name a better way to motivate a high-performance tech team, and I’ll listen.It’s a change that was accelerated — if not caused by — data tech. We’ve spent decades working through big data, business intelligence, andAI hype cycles. Each introduced new skills, problems and collaborators for the CTO and their team to get to grips with, and each moved us just a little further from the rest of the organization; no one else can do what we do, but everyone needs it done.Technical teams are not inherently commercial, and as these roles expanded to include building and delivering tools to support various teams across the organization, this gap became increasingly apparent. We’ve all seen the stats about the number of data science projects, in particular, that never get productionized — and it’s little wonder why. Tools built for commercial teams by people who don’t fully understand their needs, goals or processes will always be of limited use.This waste of technology dollars was immensely justifiable in theearly days of AI— investors wanted to see investment in the technology, not outcomes — but the tech has matured, and the market has shifted. Now, we have to show actual returns on our technology investments, which means delivering innovations that have a measurable impact on the bottom line.Transitioning from support to a core functionThe growing pains of the data tech hype cycles have delivered two incredible boons to the modern CTO and their team (over and above the introduction of tools like machine learning (ML) and AI). The first is a mature, centralized data architecture that removes historical data silos across the business and gives us a clear picture — for the first time — of exactly what’s happening on a commercial level and how one team’s actions affect another. The second is the move from a support function to a core function.This second one is important. As a core function, tech workers now have a seat at the table alongside their commercial colleagues, and these relationships help to foster a greater understanding of processes outside of the technology team, including what these colleagues need to achieve and how that impacts the business.This, in turn, has given rise to new ways of working. For the first time,technical individualsare no longer squirreled away, fielding unconnected requests from across the business to pull this stat or crunch this data. Instead, they can finally see the impact they have on the business in monetary terms. It’s a rewarding viewpoint and one that has given rise to a new way of working; an approach that maximizes this contribution and aims to generate as much value as quickly as possible.Introducing lean valueI hesitate to add another project management methodology to the lexicon, but lean-value warrants some consideration, particularly in an environment where return on tech investment is so heavily scrutinized. The guiding principle is ‘ruthless prioritization to maximize value.’ For my team, that means prioritizing research with the highest likelihood of either delivering value or progressing organizational goals. It also means deprioritizing non-critical tasks.We focus on attaining a minimum viable product (MVP), applying lean principles across engineering and architecture, and — here’s the tricky bit — actively avoiding a perfect build in the initial pass. Each week, we review non-functional requirements and reprioritize them based on our objectives. This approach reduces unnecessary code and prevents teams from getting sidetracked or losing sight of the bigger picture. It’s a way of working we’ve also found to be inclusive of neurodiverse individuals within the team, since there’s a very clear framework to remain anchored to.The result has been accelerated product rollouts. We have a dispersed, international team and operate a modularmicroservice architecture, which lends itself well to the lean-value approach. Weekly reviews keep us focused and prevent unnecessary development — itself a time saver — while allowing us to make changes incrementally and so avoid extensive redesigns.Leveraging LLMs to improve quality and speed up deliveryWe set quality levels we must achieve, but opting for efficiency over perfection means we’re pragmatic about using tools such as AI-generated code. GPT 4o can save us time and money by generating architecture and feature recommendations. Our senior staff then spend their time critically assessing and refining those recommendations instead of writing the code from scratch themselves.There will be plenty who find that particular approach a turn-off or short-sighted, but we’re careful to mitigate risks. Each build increment must be production-ready, refined and approved before we move on to the next. There is never a stage at which humans are out of the loop. All code  — especially generated  — is overseen and approved by experienced team members in line with our own ethical and technical codes of conduct.Data lakehouses: lean value data architectureInevitably, the lean-value framework spilled out into other areas of our process, and embracing large language models (LLMs) as a time-saving tool led us to data lakehousing; a portmanteau of data lake and data warehouse.Standardizing data and structuring unstructured data to deliver an enterprise data warehouse (EDW) is a years-long process, and it comes with downsides. EDWs are rigid, expensive and have limited utility for unstructured data or varied data formats.Whereas a data lakehouse can store both structured and unstructured data, using LLMs to process this reduces the time required to standardize and structure data and automatically transforms it into valuable insight. The lakehouse provides a single platform for data management that can support both analytics and ML workflows and requires fewer resources from the team to set up and manage. Combining LLMs and data lakehouses speeds up time to value, reduces costs, and maximizes ROI.As with the lean-value approach to product development, this lean-value approach to data architecture requires some guardrails. Teams need to have robust and well-considered data governance in place to maintain quality, security and compliance. Balancing the performance of querying large datasets while maintaining cost efficiency is also an ongoing challenge that requires constant performance optimization.A seat at the tableThe lean-value approach is a framework with the potential to change how technology teams integrate AI insight with strategic planning. It allows us to deliver meaningfully for our organizations, motivates high-performing teams and ensures they’re used to maximum efficiency. Critically for the CTO, it ensures that the return on technology investments is clear and measurable, creating a culture in which the technology department drives commercial objectives and contributes as much to revenue as departments such as sales or marketing.Raghu Punnamraju is CTO atVelocity Clinical Research.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Data Decision Makers,What does it cost to build a conversational AI?,https://venturebeat.com/ai/what-does-it-cost-to-build-a-conversational-ai/,"September 14, 2024 12:05 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMore than 40% of marketing, sales and customer service organizationshave adopted generative AI— making it second only to IT and cybersecurity. Of all gen AI technologies,conversational AIwill spread rapidly within these sectors, because of its ability to bridge current communication gaps between businesses and customers.Yet many marketing business leaders I’ve spoken to get stuck at the crossroads of how to begin implementing that technology. They don’t know which of the availablelarge language models(LLMs) to choose, and whether to opt for open source or closed source. They’re worried about spending too much money on a new and uncharted technology.Companies can certainly buy off-the-shelf conversational AI tools, but if they’re going to be a core part of the business, they can build their own in-house.To help lower the fear factor for those opting to build, I wanted to share some of the internal research my team and I have done in our own search for the best LLM to build our conversational AI. We spent some time looking at the different LLM providers, and how much you should expect to fork out for each one depending on inherent costs and the type of usage you’re expecting from your target audience.We chose to compareGPT-4o(OpenAI) and Llama 3 (Meta). These are two of the major LLMs most businesses will be weighing against each other, and we consider them to be the highest quality models out there. They also allow us to compare a closed source (GPT) and an open source (Llama) LLM.How do you calculate LLM costs for a conversational AI?The two primary financial considerations when selecting an LLM are the set up cost and the eventual processing costs.Set up costs cover everything that’s required to get the LLM up and running towards your end goal, including development and operational expenses. The processing cost is the actual cost of each conversation once your tool is live.When it comes to set up, the cost-to-value ratio will depend on what you’re using the LLM for and how much you’ll be using it. If you need to deploy your product ASAP,then you may be happy paying a premium for a model that comes with little to no set up, like GPT-4o. It may take weeks to get Llama 3 set up, during which time you could already have been fine-tuning a GPT product for the market.However, if you’re managing a large number of clients, or want more control over your LLM, you may want to swallow the greater set up costs early to get greater benefits down the line.When it comes to conversation processing costs, we will be looking at token usage, as this allows the most direct comparison. LLMs like GPT-4o and Llama 3 use a basic metric called a “token” — a unit of text that these models can process as input and output. There’s no universal standard for how tokens are defined across different LLMs. Some calculate tokens per word, per sub words, per character or other variations.Because of all these factors, it’s hard to have an apples-to-apples comparison of LLMs, but we approximated this by simplifying the inherent costs of each model as much as possible.We found that while GPT-4o is cheaper in terms of upfront costs, over time Llama 3 turns out to be exponentially more cost effective. Let’s get into why, starting with the setup considerations.What are the foundational costs of each LLM?Before we can dive into the cost per conversation of each LLM, we need to understand how much it will cost us to get there.GPT-4o is a closed source model hosted by OpenAI. Because of this, all you need to do is set your tool up to ping GPT’s infrastructure and data libraries through a simple API call. There is minimal setup.Llama 3, on the other hand, is an open source model that must be hosted on your own private servers or on cloud infrastructure providers. Your business can download the model components at no cost — then it’s up to you to find a host.The hosting cost is a consideration here. Unless you’re purchasing your own servers, which is relatively uncommon to start, you have to pay a cloud provider a fee for using their infrastructure — and each different provider might have a different way of tailoring the pricing structure.Most of the hosting providers will “rent” an instance to you, and charge you for the compute capacity by the hour or second. AWS’s ml.g5.12xlarge instance, for example, charges per server time. Others might bundle usage in different packages and charge you yearly or monthly flat fees based on different factors, such as your storage needs.The provider Amazon Bedrock, however, calculates costs based on the number of tokens processed, which means it could prove to be a cost-effective solution for the business even if your usage volumes are low. Bedrock is a managed, serverless platform by AWS that also simplifies thedeployment of the LLMby handling the underlying infrastructure.Beyond the direct costs, to get your conversational AI operating on Llama 3 you also need to allocate far more time and money towards operations, including the initial selection and setting up a server or serverless option and running maintenance. You also need to spend more on the development of, for example, error logging tools and system alerts for any issues that may arise with the LLM servers.The main factors to consider when calculating the foundational cost-to-value ratio include the time to deployment; the level of product usage (if you’re powering millions of conversations per month, the setup costs will rapidly be outweighed by your ultimate savings); and the level of control you need over your product and data (open source models work best here).What are the costs per conversation for major LLMs?Now we can explore the basic cost of every unit of conversation.For our modeling, we used the heuristic: 1,000 words = 7,515 characters = 1,870 tokens.We assumed the average consumer conversation to total 16 messages between the AI and the human. This was equal to an input of 29,920 tokens, and an output of 470 tokens — so 30,390 tokens in all. (The input is a lot higher due to prompt rules and logic).On GPT-4o, thepriceper 1,000 input tokens is $0.005, and per 1,000 output tokens $0.015, which results in the “benchmark” conversation costing approximately $0.16.GPT-4o input / outputNumber of tokensPrice per 1,000 tokensCostInput tokens29,920$0.00500$0.14960Output tokens470$0.01500$0.00705Total cost per conversation$0.15665For Llama 3-70B on AWS Bedrock, thepriceper 1,000 input tokens is $0.00265, and per 1,000 output tokens $0.00350, which results in the “benchmark” conversation costing approximately $0.08.Llama 3-70B input / outputNumber of tokensPrice per 1,000 tokensCostInput tokens29,920$0.00265$0.07929Output tokens470$0.00350$0.00165Total cost per conversation$0.08093In summary, once the two models have been fully set up, the cost of a conversation run on Llama 3 would cost almost 50% less than an equivalent conversation run on GPT-4o. However, any server costs would have to be added to the Llama 3 calculation.Keep in mind that this is only a snapshot of the full cost of each LLM. Many other variables come into play as you build out the product for your unique needs, such as whether you’re using a multi-prompt approach or single-prompt approach.For companies that plan to leverage conversational AI as a core service, but not a fundamental element of their brand, it may well be that the investment of building the AI in-house simply isn’t worth the time and effort compared to the quality you can get from off-the-shelf products.Whatever path you choose, integrating a conversational AI can be incredibly useful. Just make sure you’re always guided by what makes sense for your company’s context, and the needs of your customers.Sam Oliver is a Scottish tech entrepreneur and serial startup founder.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Data Decision Makers,AI orchestration: Crafting harmony or creating dependency?,https://venturebeat.com/ai/ai-orchestration-crafting-harmony-or-creating-dependency/,"September 8, 2024 12:15 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreAsAI toolsbecome increasingly integrated into our daily lives, we face a critical question: Are we harnessing their power to enhance our abilities, or are we slowly outsourcing our minds — or both?As an early adopter of generative AI tools like DALL-E, ChatGPT, Claude and others, I have experienced firsthand how these technologies can boost productivity and creativity. I have used AI to build slide decks, create marketing content and tackle various professional challenges. When faced with tasks requiring critical or creative thinking, my first instinct now is to turn to myAI partners.I recently completed a task to create new thought leadership ideas, specifically through leveraging surveys or research reports that could enhance the reputation of a company. When starting, a couple of ideas popped into my head, but the assignment needed a few more possibilities and I was stuck. I turned to achatbot colleague, created a descriptive prompt, reminded it that this was a task at which it excelled, engaged in some back-and-forth dialog and had ten more ideas a minute later.Of those ideas, four struck me as good. With a few minor tweaks the assignment was done with a total elapsed time of 30 minutes. More time was needed to put these into a readable format, but the hard part was done, much of it incollaboration with the AI.One way to look at this experience is to marvel at the improved efficiency and productivity. Working with the AI, I was able to produce a high-quality product in minimal time. That is a significant advantage for any employee, and for every business. This outcome worked because I have extensive experience and was able to easily ascertain which of the AI-generated ideas had the most merit, as well as how to improve on them for the final recommendation. The final ideas were presented and well received.Orchestral AIIn the past, I would have labored more on this assignment. It might have required four or more hours to think about the task from the viewpoint of the requester, as well as determine what would be novel about this IP. This would have required me to do an hour or two of online research, leveraging my experience more fully, and — hopefully — would have led to a strong product. WithAI as my copilot, I was able to cut most of this laborious process. Some might say this AI option removed the routine drudge work, and all for the good.Using AI in this way fits well within the concept of human as AI orchestrator, where a person conducts the variousAI toolsmuch as a conductor leads a symphony. For example, Perplexity helps with up-to-date AI-assisted search which, when shared with ChatGPT through a smart prompt can be useful in generating relevant ideas. Those ideas can then be used as input to Claude for further validation and insight, then summarized visually with an image from DALL-E or Designer. Each tool has a specific role to play, with the goal being to produce an output this is accurate, pleasing and harmonious with the assignment.However, the line between conducting a harmonious symphony and falling into discord can be perilously thin. As we conduct this AI symphony it is worth asking — are we truly in control, or are we becoming overly reliant on the very tools we orchestrate?This is good, right?As a regular user of AI tools, I am hardly a luddite or a technology skeptic. Convenience is wonderful. I really appreciate, for example, how my phone through its GPS and map applications can readily route me on the fastest path to my destination. That said, I have noticed that the innate human skills I have (such as my sense of direction and learned ability to navigate) are getting rusty. I am becoming dependent on technology.Theuse of AIis often discussed as a partnership between humans and machines, transforming the relationship between human creativity and machine intelligence. If my experience is any guide, this partnership is indeed transforming our relationship with machines. But is this transformed relationship a form of collaboration when the technology truly augments human capabilities, or is this a form of dependency where we simply outsource our cognitive skills to the machine?Collaboration or dependencyIn a collaborative relationship, both parties have an equal and complementary role. AI excels at processing enormous amounts of data, pattern recognition and certain types of analysis, while people excel at creativity,emotional intelligenceand complex decision-making. In this relationship, the human keeps agency through critically evaluating AI outputs and making final decisions.However, this relationship can easily veer into dependency where we become unable or unwilling to perform tasks without AI help, even for tasks we could previously do independently. As AI outputs have become amazingly human-like and convincing, it is easy to accept them without critical evaluation or understanding, even when knowing the content may be a hallucination — an AI-generated output that appears convincing but is false or misleading. There is a clear risk that human skills could deteriorate due to lack of use as we deploy AI to take over more tasks.It is a fine lineMaking use of AI tools to augment our capabilities provides a tremendous amount of convenience and efficiency that is incredibly useful in the short term. We can now quickly get the information and answers needed.But this capability can come at a price, as there is a thin line between collaboration — also known as augmentation — and dependency. Put simply, in using the latest AI tools, I wonder if I am outsourcing critical cognitive abilities such as problem-solving, critical thinking and memory, and in so doing losing human agency. We must all traverse this boundary carefully. If the relationship becomes one of dependency, this could lead to an inability to think for ourselves and open us to manipulation whether it is intentional or not.Going forward, AI tools are only going to get better and become more engaging and convincing. For example, ChatGPT’s new advanced voice mode sounds remarkably lifelike. According to aCNN report: “It responds in real time, can adjust to being interrupted, makes the kinds of noises that humans make during conversations like laughing or ‘hmms.’ It can also judge a speaker’s emotional state based on their tone of voice.” The company said it is worried that people could become dependent upon this technology.As this example shows, the line between collaboration and dependency just became much thinner. As AI continues to advance and become more indistinguishable from human interaction, the distinction between collaboration and dependency becomes increasingly blurred. Or worse, as leading historian Yuval Noah Harari — who is renowned for his works on the history and future of humankindpoints out— intimacy is a powerful weapon which can then be used to persuade us.While AI offers immense benefits in terms of efficiency and productivity, it is imperative that we stay mindful of the potential risks associated with over-reliance on these technologies. By fostering a balanced approach that prioritizes human agency and critical thinking, we can harness the power of AI while safeguarding our cognitive abilities and ensuring a future where humans and machines work together in a truly symbiotic relationship.The choice is ours: Will we let AI guide us, or will we remain the true orchestrators of our own minds? The implications of this choice extend far beyond mere convenience; they touch on the very essence of human autonomy and our ability to think critically in an increasingly automated world.Gary Grossman is EVP of technology practice atEdelman.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Data Decision Makers,Get ready for a tumultuous era of GPU cost volatility,https://venturebeat.com/ai/get-ready-for-a-tumultuous-era-of-gpu-cost-volitivity/,"September 7, 2024 12:05 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreGraphics chips, or GPUs, are the engines of theAI revolution, powering the large language models (LLMs) that underpin chatbots and other AI applications. With price tags for these chips likely to fluctuate significantly in the years ahead, many businesses will need to learn how to manage variable costs for a critical product for the first time.This is a discipline that some industries are already familiar with. Companies in energy-intensive sectors such as mining are used to managing fluctuating costs for energy, balancing different energy sources to achieve the right combination of availability and price. Logistics companies do this for shipping costs, which are vacillating wildlyright nowthanks to disruption in the Suez and Panama canals.Volatility ahead: The compute cost conundrumCompute cost volatility is different because it will affect industries that have no experience with this type of cost management. Financial services and pharmaceutical companies, for example, don’t usually engage in energy or shipping trading, but they are among the companies that stand tobenefit greatly from AI. They will need to learn fast.Nvidia is the main provider of GPUs, which explains why itsvaluation soared this year. GPUs are prized because they can process many calculations in parallel, making them ideal for training and deploying LLMs. Nvidia’s chips have been so sought after that one company has had them delivered byarmored car.The costs associated with GPUs are likely to continue to fluctuate significantly and will be hard to anticipate, buffeted by the fundamentals of supply and demand.Drivers of GPU cost volatilityDemand is almost certain to increase as companies continue to build AI at a rapid pace. Investment firm Mizuho has said the total market for GPUs couldgrow tenfoldover the next five years to more than $400 billion, as businesses rush to deploy new AI applications.Supply depends on several factors that are hard to predict. They include manufacturing capacity, which is costly to scale, as well as geopolitical considerations — many GPUs aremanufactured in Taiwan, whose continued independence isthreatened by China.Supplies have already been scarce, with some companies reportedly waitingsix monthsto get their hands on Nvidia’s powerful H100 chips. As businesses become more dependent on GPUs to power AI applications, these dynamics mean that they will need to get to grips with managing variable costs.Strategies for GPU cost managementTo lock in costs, more companies may choose to manage their own GPU servers rather than renting them from cloud providers. This creates additional overhead but provides greater control and can lead to lower costs in the longer term. Companies may also buy up GPUs defensively: Even if they don’t know how they’ll use them yet, these defensive contracts can ensure they’ll have access to GPUs for future needs — and that their competitors won’t.Not all GPUs are alike, so companies should optimize costs by securing the right type of GPUs for their intended purpose. The most powerful GPUs are most relevant for the handful of organizations that traingiant foundational models, like OpenAI’s GPT and Meta’s LLama. Most companies will be doing less demanding, higher volume inference work, which involves running data against an existing model, for which a greater number of lower performance GPUs would be the right strategy.Geographic location is another lever organizations can use to manage costs. GPUs are power hungry, and a large part of their unit economics is the cost of the electricity used to power them. Locating GPU servers in a region with access to cheap, abundant power, such asNorway, can significantly reduce costs compared to a region like the eastern U.S., where electricity costs are typically higher.CIOs should also look closely at the trade-offs between the cost and quality of AI applications to strike the most effective balance. They may be able to use lesscomputing powerto run models for applications that demand less accuracy, for example, or that aren’t as strategic to their business.Switching between different cloud service providers and different AI models provides a further way for organizations to optimize costs, much as logistics companies use different transport modes and shipping routes to manage costs today. They can also adopt technologies that optimize the cost of operating LLM models for different use cases, making GPU usage more efficient.The challenge of demand forecastingThe whole field of AI computing continues to advance quickly, making it hard for organizations to forecast their own GPU demand accurately. Vendors are building newer LLMs that have more efficient architectures, like Mistral’s “Mixture-of-Experts” design, which requires only parts of a model to be used for different tasks. Chip makers including Nvidia and TitanML, meanwhile, are working on techniques to make inference more efficient.At the same time, new applications and use cases are emerging that add to the challenge of predicting demand accurately. Even relatively simple use cases today, like RAG chatbots, may see changes in how they’re built, pushing GPU demand up or down. Predicting GPU demand is uncharted territory for most companies and will be hard to get it right.Start planning for volatile GPU costs nowThe surge in AI development shows no signs of abating. Global revenue associated with AI software, hardware, service and sales will grow19% per yearthrough 2026 to hit $900 billion, according to Bank of America Global Research and IDC. This is great news for chip makers like Nvidia, but for many businesses it will require learning a whole new discipline of cost management. They should start planning now.Florian Douetteau is the CEO and co-founder ofDataiku.DataDecisionMakersWelcome to the VentureBeat community!DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.You might even considercontributing an articleof your own!Read More From DataDecisionMakers"
Virtual Communication,Thera scores $4M seed to make it easier to hire and pay international employees,https://venturebeat.com/virtual/thera-scores-4m-seed-to-make-it-easier-to-hire-and-pay-international-employees/,"August 22, 2024 6:02 AM","The number of businesses hiring international employees is on the rise: spurred by rising domestic labor costs and the need for specific talent, 75% of small-to-medium-sized businessesin a 2023 surveyof 500 owners and decision-makers byGustosaid they planned to increase international headcount, and 54% planned to do so in the coming 1-3 years.However, for businesses looking to take advantage of the global talent pool, ensuring they are set up topaytheir international employees and contractors in a timely fashion can be a bear. EnterThera, a payroll and payments startup founded in 2022 that promises to help businesses of all sizes navigate the maze of differing countries’ labor rules and regulations and get employees paid no matter where they work from.“We started with this thesis that more businesses are going to be global from day one,” said Akhil Reddy, Thera’s founder and CEO, in a phone interview with VentureBeat earlier this week.The thesis has legs. Today, Thera announced it has raised $4 million in seed funding from Y Combinator, 10x Founders, Amino Capital, Zillionize and Bayhouse Capital. It also got funding from angel investors Oliver Jung, Chris Bakke, Andrew Yeung, Akash Magoon and Bobby Matson.Video showing Thera’s dashboard. Credit: TheraThera’s origin storyReddy has considerable experience designing systems to ease the flow of payments digitally, having previously built systems for Amazon Prime, the e-commerce giant’s free-shipping and included media subscription service tier.“Two of the big things I learned at Amazon were the importance of selection and transferring affordable credits,” Reddy told VentureBeat. “We’re taking that same ethos and trying to apply it to SaaS [software-as-a-service.]”Inspired by that experience, Thera’s new, built-from-scratch system replaces multiple financial tools, offering an ecosystem of native apps for payroll, treasury and accounts payable/receivable (AP/AR) services.“Payments are a massive challenge for many companies,” Reddy added in a press release provided to VentureBeat. “Through our unique bundled app model and customer-centric approach, we strive to create a streamlined experience to manage all financial operations while increasing our customer’s bottom line. We are committed to providing the rates and transparency the industry so desperately needs.”What Thera offersScreenshot of Thera client-side dashboard. Credit: TheraThera offers a range of services including U.S. payroll for hiring and paying employees in all 50 states, contractor management for more than 150 countries, multiple currencies and employer-of-record services in 150+ countries and payments in five methods. It also features Thera AP/AR for global invoicing and payments, and Bill Pay to manage all payables in one place.Thanks to its integration with major payment providers and global systems, as well as a database of consistently updated information on the different labor laws around the world, combined with its platform of native apps for payroll and tracking, Thera expects to save its customers significant sums of money.On its website, the company boasts of being 80% more affordable than Deel AP/AR for global payroll and 90% more affordable than Stripe Invoicing. Also, it claims a speed improvement over rivals that benefits employees and contracts, with 95% of payrolls arriving the same day compared to 2-10 days for other payroll companies.By saving on these costs and time, Thera also says it can pass the savings onto contractors — who can make up to 3% more than other providers.Thera’s business customers can also expect to receive several direct support and success reps available to them 24/7/365 through Slack.“You get a customer success manager, and you also get a global HR specialist in a Slack channel,” Reddy told VentureBeat. “Everyone gets a shared Slack channel to answer any questions they might have.”Thera’s initial successBy consolidating these services, Thera provides businesses with a seamless experience and some of the most competitive rates on the market.Already, Thera is processing $10M+ in payroll annually for thousands of workers worldwide at some of the fastest-growing companies in the U.S., including Oceans, Landed, 1840 & Company and Zendrop.“We saw a lot of customers complaining about the lack of transparency with their existing [payments] providers,” he elaborated to VentureBeat. “There’s so many hidden fees in the FX [financial experience], and then also one-time fees that they were unaware of.Where Thera goes nextThe $4 million Seed funding will be instrumental in accelerating Thera’s growth.The company plans to use the funds to further develop its platform and expand its team in New York City, where it is currently headquartered.Reddy also told VentureBeat that while Thera’s financial and payments apps for businesses are today available on desktop via the web, the company is working on mobile offerings that should be available soon.As Thera continues to grow, the company remains focused on its mission to streamline financial operations for businesses around the world.With the new funding, Thera is well-positioned to enhance its platform, expand its team, and solidify its position as a leading player in the B2B payments space.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Virtual Communication,How Shift transformed Chromium into a browser for power users,https://venturebeat.com/virtual/how-shift-transformed-chromium-into-a-browser-for-power-users/,"August 19, 2024 7:20 AM","Shift’sinterface looks clean, fresh and simple — but under the hood, it’s a powerful Chromium browser that’s the first of its kind. Shift’s browser merges web apps and search into a single interface, eliminating window-switching and making it easy for users to manage multiple email inboxes. In Shift, users can create custom browser workspaces with separate apps, accounts and tabs, promoting natural organization and reducing tab pile-ups.By developing the browser on top of a Chromium base and tapping into underlying APIs, Shift created a highly custom interface. Shift’s Chromium foundations allow the development team to explore new features and iterate in a more responsive and agile way. It’s how they reimagined the modern browser to align with how people are actually using the internet, says Michael Foucher, VP of Product at Shift. The flexibility of their software base is Shift’s competitive advantage.“We chose Chromium because it’s by far the most active open-source browser in the world, and they’re constantly trying to make it better, more secure and more performant,” Foucher says. “We’ve built a browser that consolidates apps and search into a single interface, and Chromium gives us a rock-solid foundation to keep innovating.”Shift’s approach to development is unique, he adds, and that’s the secret sauce and their competitive advantage.Shift’s browser is pushing the boundaries of ChromiumMost Chromium variants out there, like Opera, Brave and Microsoft Edge, are building their UI using bulky C++ code — resulting in all their products looking very much like Chrome — and it’s not because they love the interface.“It’s because developers don’t want to touch it,” Foucher says. “If you touch C++ code, everything blows up. It’s very hard to make any substantial UI changes because any small change to that code can destroy the whole thing.”Shift’s browser, however, leverages powerful web technologies to craft a unique user experience on top of Chromium, replacing the standard look and feel. Shift makes a number of customizations directly to Chromium, including proprietary APIs to facilitate communication between the parts of the browser that users interact with and the engine that drives it. Through this process, Shift creates a fully customized interface very quickly and easily makes alterations or additions — without the need for an army of developers.“It’s a major productivity boost, and makes our small team mighty,” Foucher says. “We can develop and test and make improvements in a flash. That speed and that ability to leverage the newest and best in web tech is a huge advantage for us.”However, Chromium itself can slow down the development process significantly. At over 20 million lines of code, Chromium is massive. A build can take a whole day locally, or multiple hours in the cloud. To solve the issue, Shift turned to EngFlow for remote execution that distributes, builds and tests across a cluster of machines, and remotely caches the results. At Shift, build times have dropped from two or more hours to under 20 minutes.“It’s not inexpensive. But it is a huge savings because of the productivity enhancement and the cost of developers not sitting around waiting for things to build,” Foucher says. “That’s a huge factor in allowing us to move forward quickly.”Designing a new kind of interfaceFrom the start, Shift has been intentional with the look and feel of its interface, working with multiple designers to create an environment that’s organized, flexible and efficient — and more attractive than a bare-bones browser.While it’s a new kind of browsing experience, Shift users can quickly get comfortable with the interface. Shift draws from the Chromium community for open-source features that have become staples in the browser experience and users are already familiar with.“I look to them as the muscle memory we’re trying to mimic for people. The keyboard shortcuts you love in Chrome, you’ll be able to use them in Shift,” Foucher says. “We try to take advantage of those familiar, heavily used features. Where we do diverge, it’s to provide a net new experience that will make our browser better.”Behind the scenes, Shift is exploring more customization in user settings with the ultimate goal of handing users more control over their browsers. These highly-anticipated features will be released in future product updates. In the meantime, Shift is continually rolling out upgrades to their browser including cosmetic changes that might appear to be minor on the surface, Foucher says, but they’re important.“It shows a sense of pride in the product we’re releasing,” he explains. “It might be subtle, but it just shows that we really care about the user experience.”Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contactsales@venturebeat.com."
Virtual Communication,Exclusive: Resquared nabs $5M to take on leading CRMs with B2B local sales platform,https://venturebeat.com/ai/exclusive-resquared-nabs-5m-to-take-on-leading-crms-with-b2b-local-sales-platform/,"July 23, 2024 2:06 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreSales and software are a match made in heaven: just askSalesforceor evenHubspotorCreatio, all of which have built multi-billion dollar companies atop customer relationship management software suites specifically focused on sales teams.But are all of those firms missing out on an opportunity right under their noses?Resquared(also known as Re²) thinks so.The Y Combinator-backed, fully remote startup revealed to VentureBeat recently that it successfully closed a $5 million seed round for its B2B sales tool focused specifically on reachinglocalbusinesses across the U.S., those that may not have much or any online footprint except for consumer-facing social tools like Facebook pages and Instagram accounts.“We invest a lot of our own team’s time and research towards figuring out what is the most effective way to reach local businesses,” said Griffin Morris, Resquared’s co-founder and CEO, in a teleconference interview with VentureBeat conducted last week. “They’re typically not on LinkedIn, but if you reach out to a business through their Facebook or Instagram page, you can get a 20% reply rate and it’s often from the owner of the business. So that’s how we start all of our product development: with best practices.”Already,Resquared claimsto have collected data on more than 11 million SMBs throughout the U.S. and Canada. It offers a “suite of AI-powered email and social media messaging tools.”“We have thousands of sales reps on the platform and hundreds of sales organizations as clients,” Morris told me. “A lot of our growth comes from investing in content and our clients sharing their positive experiences on social media.”This funding round was spearheaded by SNR’s Kevin Patrick Mahaffey, alongside 1984 Ventures, Merus Capital, Oleg Rogynskyy from People.ai, Don Tepman, famously known as “StripMallGuy” (with 230,000+ followers on X) and Twenty Two Ventures.The infusion of capital will expedite Resquared’s mission to establish a new segment of B2B sales, tailored to meet the unique challenges of reaching small and medium-sized businesses (SMBs).The problem: 400,000 sales reps focused on local biz, but using the wrong tools for the jobMore than 400,000 sales representatives in the U.S. currently target local businesses, according to research obtained by Resquared, selling everything from real estate leases on new locations within cities to restaurant and office supplies to, of course, software tools.However, these local salespeople often resort to outdated methods like door-to-door sales or telemarketing for lead generation.While some use platforms like ZoomInfo or enterprise CRMs, these tools typically rely on LinkedIn data, which only 10% of local business owners use.This disconnect results in inefficient outreach efforts, with local businesses receiving numerous spam messages and having limited vendor choices.How Resquared provides salespeople with a data advantage to reach local SMBsResquared aims to solve this issue by supporting thousands of sales reps with campaigns that boast open rates twice the industry average.The platform provides a higher return on investment through increased revenue and time saved, with clients reporting up to 30% increases in monthly sales.It relies on a combination of AI-driven automated outreach over email and social media, and its underlying local business data, as well as comprehensive analytics to track performance of said messages to prospective customers.Screenshot of Resquared’s local outreach platform. Credit: Resquared“We have always had AI and machine learning features, studying interactions and data to figure out what works best,” Morris explained. “We use proprietary machine learning features and open AI APIs to enhance our platform.”Pricing for Resquared’s platformvaries depending on the number of users and is customized for each client.“If you had a sales team with five reps, starting on the platform would be around $1,000 a month,” Morris ssaid. “Every client gets one-on-one onboarding and training to ensure high-quality and personalized sales processes.”Kevin Patrick Mahaffey of SNR commented on the investment in the release, stating, “Selling software online is like living in the future: AI-enabled tools at every layer of the stack, sophisticated pipelines, and highly orchestrated processes. Selling to real-world businesses, however, looks about the same as it did 30 years ago. Resquared gives superpowers to sales teams engaging with local businesses so main street can benefit from the onward march of progress as well.”Origin storyResquared was born from the shared experiences of its founders, Morris and Tyler Carlson, who have both focused their careers on the intersection of tech and the brick-and-mortar economy.Morris began his career selling door-to-door to local businesses before transitioning to product management.“I had a sales job where my quota was to walk into 40 local businesses a day in St. Louis, Missouri. Twelve years later, that’s still how it’s done for the most part,” noted Morris. “Companies often rely on high volume, low-quality marketing outreach like call centers.”Carlson, on the other hand, worked closely with major brick-and-mortar franchises with thousands of local outposts including Subway and Burger King, helping them modernize their stores with technology.Building on this foundation, Resquared emerged with a vision to not just improve sales processes but to create an entirely new segment of B2B sales from the ground up.The team developed targeted playbooks based on local business needs, evolving into a comprehensive platform powered by a feedback loop of data-driven best practices, AI workflows, and automation. Resquared also offers free courses, playbooks, and a podcast titled “Selling Local.”Initial success storiesAs mentioned previously, Resquared provides a database and instant access to more than 11 million local businesses, using AI to automate high-quality, personalized outreach.Clients include publicly traded retail landlords such as Kimco, marketing agencies specializing in SEO, commercial services companies ranging from pest control to maintenance, commercial insurance, small business lending, and suppliers.During an interview, Morris elaborated on Resquared’s approach. “We also gather data from public government records, social media sites, and information that businesses share on their websites. It’s messy data, but we clean, organize, and match it to ensure quality.”Growth and future outlookResquared is experiencing rapid growth, with thousands of sales reps and hundreds of sales organizations using the platform.The company plans to use the new funding to expand its sales and engineering teams, focusing on an ambitious product roadmap.Currently, Resquared operates primarily in the U.S. and Canada, supporting a wide range of clients from billion-dollar publicly traded companies to regional businesses with significant sales teams.The company is fully remote, with team members spread across various hubs like New York, Southern California, and Florida. Morris himself operates out of Philadelphia, Pennsylvania, while co-founder Carlson works out of Tampa, Florida.This flexibility allows Resquared to hire the best talent from anywhere while maintaining strong team cohesion through regular in-person meetings and conferences.With this fresh injection of capital and a clear mission, Resquared is poised to revolutionize B2B sales for local business vendors, bringing high-quality, personalized sales techniques to a traditionally underserved market segment. To learn more about how Resquared is changing the game for companies targeting local businesses, visitRe2.ai.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Virtual Communication,Legal software company Clio raises massive $900M round to power AI advances,https://venturebeat.com/virtual/legal-software-company-clio-raises-massive-900m-round-to-power-ai-advances/,"July 23, 2024 5:30 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreThe 16-year-old Canadian legal software firmClio, which makes a cloud-based operating system for law firms that handles everything from client intake to court filings to accounting, announced today that it has raised $900 million in a Series F investment round.This massive round was led by New Enterprise Associates (NEA) and values the firm at a total of $3 billion. Clio is touting it as the largest transaction ever in cloud legal technology, and says the money will be used to further advance its tech offerings with AI and other features, as well as expand to new markets and firms. It already counts customers in more than 130 countries, though the majority of its 150,000 individual users are in the U.S.“We’re able to dramatically simplify workflows and unify them into one system,” said Jack Newton, CEO and Founder of Clio, in a phone interview with VentureBeat. “We put law firms of all stripes, all of the data of each firm, in their own centralized, secure system of record that they depend on as their source of truth.”The Series F round saw participation from Goldman Sachs Asset Management, Sixth Street Growth, CapitalG, and Tidemark, joining current stakeholders TCV, JMI Equity, funds and accounts advised by T. Rowe Price Associates, Inc. and T. Rowe Price Investment Management, Inc., and OMERS.Tony Florence, Co-CEO at NEA, who has joined Clio’s Board of Directors, commented in a press release: “Clio embodies everything NEA looks for in a growth-stage investment: an exceptional, purpose-driven team, market and product leadership, and stellar business physics. Clio is mission critical to law firms, and the company’s best-in-class retention and NPS are testaments to the team’s ability to continuously innovate, deliver immense value, and meet the dynamic needs of the legal sector. With the right foundation in place for continued market expansion and advanced AI capabilities, we believe the best is yet to come.”Investment in AI and new featuresClio plans to invest further in its burgeoning AI portfolio and integrated legal payments.Recently, the companyannounced Clio Duo, a proprietary generative AI solution designed to help lawyers complete routine tasks and leverage firm analytics for more efficient practice management.Clio Duo includes audit log functionality for court discovery, enhancing legal practice efficiency.“We’re all in on AI,” said Newton. “We’re weaving it into many different areas of the product…it’s something we’ve been investing in since the earliest days of large language models over a year and a half ago.”He pointed to thelaunch of ChatGPT in November 2022as the key turning point for interest among customers in AI, as it was for many industries. He also noted his own background was in machine learning.At the same time, Newton acknowledged that “there’s a lot of anxiety around AI replacing lawyers among our customer base…but we really perceive the power of AI is to amplify them so that they’re able to dramatically increase their productivity, and position them to better serve the huge amount of unmet legal demand.”Newton added that statistics available to Clio showed that 77% of current legal issues annually arenot servedby lawyers — with people going to other experts or simply going it alone to solve them, also termed as the “justice gap.” He said that AI would help lawyers by giving them more time to find and serve more people who are currently doing without representation.Additionally, Clio offers Clio Accounting to manage firm finances, ensuring compliance, and a module specifically for personal injury lawyers, providing rapid settlement estimates for high-volume case assessments.The platform also includes Clio Draft for intelligent document automation and court form libraries covering more than 50 jurisdictions, as well as electronic court filing services to streamline court interactions.Security is paramountObviously, the legal sector is among those that have a consistent and dire need for security — by the very nature of dealing with confidential material before trials and filings, whether it be pertaining to individuals or organizations and their goings on.Amid the recent outage of security providerCrowdStrikethat left millions of airport travelers stranded and bank customers frustrated, questions have swirled throughout the tech and business worlds about enterprises being overly reliant on cloud software companies.But Netwon says Clio understands these concerns and takes security extremely seriously and has since the start.“We’ve made a very significant investment in security and compliance, and operate a dedicated security team that is larger than some of our competitors and their development team,” the CEO told VentureBeat. “We’re able to provide assurance to our HIPAA regulated customers [Health Insurance Portability and Accountability Act — pertaining to healthcare firms]. That’s something we’ve invested in since day one of our technical investments.”A winning streakClio’s growth since its Series E funding in April 2021, which raised $110 million, has been remarkable. The company has exceeded $200 million in annual recurring revenue (ARR) and expanded its international footprint to the APAC region. Its all-in-one payments business, launched in 2022, now processes billions of dollars annually in legal-specific transactions.Clio’s workforce has grown to over 1,100 employees, with hubs in North America, EMEA, and APAC regions. The company continues to hire across various areas, including product development, R&D, sales, marketing, and customer success, to support its expanding operations.A history of innovationSince its inception in 2008, Clio has made a major impact on the legal technology landscape. After recognizing the challenges faced by solo practitioners and small law firms, founders Jack Newton and Rian Gauvreau aimed to bring the practice of law to the cloud.Clio has hosted 10 Clio Cloud Conferences, presented 32 Reisman Awards, and published seven Legal Trends Reports. It is recommended by over 100 bar associations and law societies and has added more than 200 partners to the Clio App Directory. Clio also offers an Academic Access Program, providing free software to over 55,000 users at more than 200 schools.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Virtual Communication,Shift: The app-integrated power browser that’s reimagining your entire online life,https://venturebeat.com/virtual/shift-the-app-integrated-power-browser-thats-reimagining-your-entire-online-life/,"June 26, 2024 6:50 AM","Presented by ShiftWork or play, modern lives are entirely, irrevocably online, and getting more complicated by the day. The number of online tools developed to contain and tame them, at the office and throughout all the facets of our personal lives, have spiraled into dizzying numbers — and web browsers are the portals we use most frequently to access those responsibilities. Today, getting stuff done looks very much like rabbiting between tabs, from social media and personal communication apps to a variety of email inboxes. We have different apps, accounts and logins for work and family and play, plus a line-up of tabs, like an internet junk drawer. We’re too afraid to close it, fearing it could be vital to our work and life online.But it’s pretty safe to say your emotional support tabs rarely, if ever, do anything but add stress and a vague, undefined and wholly unnecessary urgency. That’s the problem we are solving withShift,says Michael Foucher, VP of Product. Shift is a power browser that integrates all your web apps and email accounts into one seamless, serene online experience. This browser brings together everything you do on the internet into one naturally organized window — so you don’t have to keep jumping between windows or put up with stressful tab clutter forever.“What makes us a power browser is the seamless app integration and workspace organization,” Foucher explains. “And it’s our design philosophy. Workflows and the way we set up our online life and productivity, both inside and outside of the office, is extremely personal. We want to take that chaos away, offer a basic mental scaffolding that lets users easily map every sphere of their online life.”Shift organizes your digital life in just one browser windowBy merging all your apps and windows into one, Shift gives you a bird’s-eye view of everything you do on the internet. Within Shift, you can then create separate browser workspaces for each part of your life. Whether it’s for personal, work or play, a workspace creates a dedicated place for context-specific browsing, so that you can narrow your focus to the facet of your life that’s demanding attention. Rather than switching through windows or multiple accounts and losing tabs somewhere among the clutter, all these connected pieces come together seamlessly in a workspace.Create multiple workspaces to organize your online life. Each workspace can be tied to an email account and you can switch between your inboxes and workspaces easily by clicking the profile icons at the top of the left sidebar. At the bottom left, choose from over 1,500 integrated apps and find each app pinned to a dedicated workspace, all of which open within Shift. Front and center is your search bar and a unique set of bookmarks for each of your workspaces. Notifications are fine-tunable: you can mute alerts from other workspaces if you choose, or toggle them individually for each app and account.“What we’ve heard loud and clear from our users is that by just providing this basic map for each part of their lives, whether it be work or home or a hobby or a nonprofit group, it just makes everything so much easier to organize, and removes a lot of the overwhelm,” Foucher says. “Every workspace changes to reflect the mindset they’re in and goals they have, and everything else goes away.”Shift accesses these accounts through a secure, local API client that never sends any data back to Shift’s databases. Apps are installed once and shared across workspaces, each logged in to the correct account. Plus, smart app linking means if you get email notifications from one of your apps, clicking on a link will take you right to the app within Shift instead of opening a brand new tab. Every workspace is fully adaptable to any kind of workflow–home organization deadlines, work productivity, keeping track of a side hustle and more. No matter where you log in, Shift always remembers every workspace, app and bookmark, and signing in is seamless and secure.“We really shine when you set up Shift to isolate those spaces and create workflows for each area of your life within those spaces,” Foucher says. “When people do that, that gives them the best chance of really unlocking the power of this browser and feeling its value. But everyone does it just slightly differently —they often surprise us with ways they use it.”Unlocking the potential of the Shift browserThe company’s first goal is to make it easy for anyone to use. Shift leverages the basic foundational elements of the Chrome browser for itspowerful features, like bookmarks, but includes web apps and workspaces as each unique, isolated sections of the browser. The familiar interface makes the barrier to entry relatively low, as does thehandy app directory, available right from your workspaces.A new user can import their browsing history from Chrome or their default browser. As they get set up in Shift, they can add any accounts they’re logged into on their browser, whether that’s an email account or web apps that exist in Shift’s directory of apps.“We’re applying familiar concepts in a unique way, offering all the web tools that users know and love and use every day,” Foucher says. “We don’t make people work hard to get to a place where they can become useful. Our goal is that our users never have to turn to the Help page to figure it out.”As a portfolio company of theRedbrickfamily, Shift combines powerful functionality with unique flexibility to create a browser that adapts to how you use the internet. Whether you jump between apps and inboxes all day or simply want to separate your tabs, Shift is the answer to a better online experience. This browser is a game-changer for freelancers, creatives, agencies, and business owners everywhere.Download Shift for free.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contactsales@venturebeat.com."
Programming & Development,Ensemble raises $3.3M to bring ‘dark matter’ tech to enterprise AI,https://venturebeat.com/ai/ensemble-raises-3-3m-to-bring-dark-matter-tech-to-enterprise-ai/,"September 26, 2024 6:00 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMachine learning startupEnsemblehas raised$3.3 million in seed fundingto address the growing importance of data quality in artificial intelligence. Salesforce Ventures led the round, with participation from M13, Motivate, and Amplo.FoundersAlex ReneauandZach Albertsonare pioneering a novel approach to data representation that promises to enhance machine learning model performance without requiring vast amounts of additional data or complex model architectures.Unlocking hidden data relationships with ‘dark matter’ technology“We have a new way to essentially approximate hidden relationships in your data or missing information that you wish was originally in your dataset to improve your model,” said Alex Reneau, CEO of Ensemble, in an exclusive interview with VentureBeat. “We’re able to enable customers to maximize their own data that they’re working with, even when it’s limited, sparse, or highly complex, allowing them to train effective models with less comprehensive information.”The company’s proprietary “dark matter” technology slots into the machine learning pipeline between feature engineering and model training. It creates enriched data representations that can uncover latent patterns and relationships, potentially making previously unsolvable problems tractable.Addressing enterprise AI adoption challengesThis approach comes at a critical time forenterprise AI adoption. Despite rapid advances in AI capabilities, many organizations struggle to deploy models in production environments due to data quality issues.Caroline Fiegel, an investor at Salesforce Ventures, explained the rationale behind their investment: “We have maybe watched over the past 12 to 24 months, enterprises move more slowly into AI and into production than we had anticipated,” she told VenutreBeat. “When you peel that back and really start to understand why, it’s because the data is disparate. It’s kind of low quality. It’s riddled with PII.”Ensemble’s technology could have far-reaching implications across industries. The company is already working with customers in biotechnology and advertising technology, with early results showing promise in areas such as predicting virus-host interactions in the gut microbiome.From impossible to possible: Expanding the horizons of machine learning“We actually care a lot more about the cases where ML is able to do what was otherwise impossible before,” Reneau emphasized. “So it’s not just about doing what a human can do, and making it faster, but [it’s about] what a human couldn’t do.”The funding will be used to accelerate product development, expand the team, and ramp up go-to-market efforts. As the AI landscape continues to evolve rapidly, Ensemble sees its role as providing a foundational technology that can adapt to changing needs.“With these models constantly developing, and the data landscape is going to be ever-evolving, I think that we’re definitely more set—on the core research side of it,” Reneau said, hinting at the company’s long-term vision.For Salesforce Ventures, the investment aligns with their thesis on the critical role of data in AI adoption. “Building trust in AI today is really built in outcomes,” Fiegel said, “and so knowing that Alex and Zach kind of share that core north star with us is what keeps us excited.”As enterprises grapple with the challenges of implementing AI at scale, Ensemble’s approach to data quality could prove to be a key enabler. The company’s progress will be closely watched by both the tech industry and the broader business community as a potential solution to one of AI’s most persistent obstacles.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Programming & Development,The five tech trends driving video games,https://venturebeat.com/ai/the-five-tech-trends-driving-video-games/,"September 25, 2024 9:00 AM","GamesBeat Next is almost here! GB Next is the premier event for product leaders and leadership in the gaming industry. Coming up October 28th and 29th, join fellow leaders and amazing speakers like Matthew Bromberg (CEO Unity), Amy Hennig (Co-President of New Media Skydance Games), Laura Naviaux Sturr (GM Operations Amazon Games), Amir Satvat (Business Development Director Tencent), and so many others. See the full speaker list andregister here.Video games as a medium tend to be driven by changes in technology. A lot of designers want to play with the newest and latest toys that make their jobs easier and expand their options with the worlds they create. Publishers and their investors are also excited by these new possibilities. Lots of money is being invested in whoever can make a big splash with the newest technology, so here’s five tech trends that are currently driving game development to new space.1. Machine Learning and AIMachine learning is not the exact same thing as AI, but both are being used within game development at the moment. While AI can be argued to be a little too broad, machine learning is fairly specific in both its uses and definition. With the recent PlayStation 5 Pro announcement, first-party platform Sony introduced the PlayStation Spectral Super Resolution feature — perhaps unfortunately commonly abbreviated as PSSR. This uses AI to increase the effective resolution of a game without taking a hit to performance. With the Nintendo Switch successor (a follow-up to what will likely end up as the most successful console of all time) rumored to be doing something similar leveraging Nvidia’s DLSS technology for frame generation, it seems this kind of tech is here to stay.Meanwhile, machine learning is improving games using specific algorithms and mathematical models for various uses in game development. Procedural generation is one well-worn example, allowing developers to create worlds without having to hand-create every square inch of them. It is a decidedly young field, but one in which many studios — such as Guerilla Games, creators of Horizon — are already hiring high-level roles for.It was fairly recently that Electronic Arts pledged to be a leader in the AI space through using AI for content generation and for backend tools. It seems likely many other studios will follow suit.Join us for GamesBeat Next!GamesBeat Next is almost here! GB Next is the premier event for product leaders and leadership in the gaming industry. Coming up October 28th and 29th, join fellow leaders and amazing speakers like Matthew Bromberg (CEO Unity), Amy Hennig (Co-President of New Media Skydance Games), Laura Naviaux Sturr (GM Operations Amazon Games), Amir Satvat (Business Development Director Tencent), and so many others. See the full speaker list andregister here.2. VR/AR and the desire to create something newWhile the gold rush on virtual reality may now be over, developers are now looking at what can be done with VR and AR games to make them stand out. Titles like 2023’s Asgard’s Wrath II, the sequel to Sanzaru’s first title in the series, was widely applauded as one of the best VR games to come out in recent memory. A lot of developers are now looking at VR to see how they can plant a flag in the technology for if and when the games become just as mainstream as their flatscreen equivalents.Meta released its Meta Quest III last year, as well as Sony releasing the PlayStation VR 2, so the act of using VR has never been more comfortable. It’s just a question of whether the audiences will respond to the games being put out there.3. Cloud GamingDespite the failure of major cloud gaming initiatives like Google Stadia, the technology has been continued to be used in the background by gaming studios as alternatives to having to download entire games. Xbox continues to count on the Xbox Cloud Gaming program, letting people start a prospective game from the menu and download it later. Hoyoverse recently put games like Genshin Impact on the cloud as an alternative to downloading a hefty 40GB onto your mobile device before giving it a shot. Developers are increasingly finding that removing barriers of entry to games makes people more likely to play them, which is a perfect place to slot in Cloud technology.4. Livestreaming as a means of marketingOnce a game is out, some studios consider the marketing done. But one of the keys in getting a game to sell with a long tail is to hope that it catches on with streamers, essentially getting millions’ worth of free marketing from someone playing and enjoying the game with an audience. Developers are using technology to not just make their games more stream-friendly, but also make them less hostile to streaming. Games that feature prominent music these days, for example, allow full soundtrack switching so as not to run afoul of automatic music-seeking algorithms that could earn a streamer copyright strikes or even more punitive punishments. If it is risky to stream your game, streamers are just likely not to do it.5. User GenerationAnother avenue of technology for game developers these days is to leave some degree of creative decision-making in the users’ hands. An example of this would be Fortnite, which allows users to not only create maps and modes, but lets them leverage Epic’s own markets to spread it around. Not only does this loosen the ropes around Epic’s constant need to develop Fortnite around the clock, it allows the audience to invest some ownership in the hit that could work out to financial payouts for them as other users enjoy the content.These are just a few of the technologies working behind the scenes on video games in recent years. At some point new technologies, techniques, and perspectives will replace these, as is often the case in the medium. For those attending Gamesbeat NEXT in San Francisco in October, I will also be attending to moderate and talk about some of these things for both the people working hands-on with them and decision-makers trying to figure out what is best for their studios.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Programming & Development,Microsoft unveils ‘trustworthy AI’ features to fix hallucinations and boost privacy,https://venturebeat.com/ai/microsoft-unveils-trustworthy-ai-features-to-fix-hallucinations-and-boost-privacy/,"September 24, 2024 10:01 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreMicrosoft unveiled a suite of new artificial intelligence safety features on Tuesday, aiming to address growing concerns about AI security, privacy, and reliability. The tech giant is branding this initiative as “Trustworthy AI,” signaling a push towards more responsible development and deployment of AI technologies.The announcement comes as businesses and organizations increasingly adopt AI solutions, bringing both opportunities and challenges. Microsoft’s new offerings includeconfidential inferencingfor itsAzure OpenAI Service, enhanced GPU security, and improved tools for evaluating AI outputs.“To make AI trustworthy, there are many, many things that you need to do, from core research innovation to this last mile engineering,” said Sarah Bird, a senior leader in Microsoft’s AI efforts, in an interview with VentureBeat. “We’re still really in the early days of this work.”Combating AI hallucinations: Microsoft’s new correction featureOne of the key features introduced is a “Correction” capability inAzure AI Content Safety. This tool aims to address the problem of AI hallucinations — instances where AI models generate false or misleading information. “When we detect there’s a mismatch between the grounding context and the response… we give that information back to the AI system,” Bird explained. “With that additional information, it’s usually able to do better the second try.”Microsoft is also expanding its efforts in “embedded content safety,” allowing AI safety checks to run directly on devices, even when offline. This feature is particularly relevant for applications like Microsoft’sCopilot for PC, which integrates AI capabilities directly into the operating system.“Bringing safety to where the AI is is something that is just incredibly important to make this actually work in practice,” Bird noted.Balancing innovation and responsibility in AI developmentThe company’s push for trustworthy AI reflects a growing industry awareness of the potential risks associated with advanced AI systems. It also positions Microsoft as a leader in responsible AI development, potentially giving it an edge in the competitive cloud computing and AI services market.However, implementing these safety features isn’t without challenges. When asked about performance impacts, Bird acknowledged the complexity: “There is a lot of work we have to do in integration to make the latency make sense… in streaming applications.”Microsoft’s approach appears to be resonating with some high-profile clients. The company highlighted collaborations with the New York CityDepartment of Educationand the South AustraliaDepartment of Education, which are using Azure AI Content Safety to create appropriate AI-powered educational tools.For businesses and organizations looking to implement AI solutions, Microsoft’s new features offer additional safeguards. However, they also highlight the increasing complexity of deploying AI responsibly, suggesting that the era of easy, plug-and-play AI may be giving way to more nuanced, security-focused implementations.The future of AI safety: Setting new industry standardsAs the AI landscape continues to evolve rapidly, Microsoft’s latest announcements underscore the ongoing tension between innovation and responsible development. “There isn’t just one quick fix,” Bird emphasized. “Everyone has a role to play in it.”Industry analysts suggest that Microsoft’s focus on AI safety could set a new standard for the tech industry. As concerns about AI ethics and security continue to grow, companies that can demonstrate a commitment to responsible AI development may gain a competitive advantage.However, some experts caution that while these new features are a step in the right direction, they are not a panacea for all AI-related concerns. The rapid pace of AI advancement means that new challenges are likely to emerge, requiring ongoing vigilance and innovation in the field of AI safety.As businesses and policymakers grapple with the implications of widespread AI adoption, Microsoft’s “Trustworthy AI” initiative represents a significant effort to address these concerns. Whether it will be enough to allay all fears about AI safety remains to be seen, but it’s clear that major tech players are taking the issue seriously.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Programming & Development,OpenAI tackles global language divide with massive multilingual AI dataset release,https://venturebeat.com/ai/openai-tackles-global-language-divide-with-massive-multilingual-ai-dataset-release/,"September 23, 2024 5:33 PM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreOpenAI took a major step toward expanding the global reach of artificial intelligence by releasing a multilingual dataset that evaluates the performance of language models across 14 languages, including Arabic, German, Swahili, Bengali and Yoruba.The company shared theMultilingual Massive Multitask Language Understanding (MMMLU) dataseton the open data platform Hugging Face. This new evaluation builds on the popularMassive Multitask Language Understanding (MMLU) benchmark, which tested an AI system’s knowledge across 57 disciplines from mathematics to law and computer science, but only in English.By incorporating a diverse array of languages into the new multilingual evaluation, some of which have limited resources for AI training data, OpenAI set a new benchmark for multilingual AI capabilities. This benchmark could open up more equitable global access to the technology. The AI industry has faced criticism for its inability to develop language models that can understand languages spoken by millions of people worldwide.OpenAI delivers global benchmark for evaluating multilingual AIThe MMMLU dataset challenges AI models to perform in diverse linguistic environments, reflecting the growing need for AI systems that can engage with users across the globe. As businesses and governments increasingly adopt AI-driven solutions, the demand for models that can understand and generate text inmultiple languageshas become more pressing.Until recently, AI research has focusedprimarily on Englishand a few widely spoken languages, leaving many low-resource languages behind. OpenAI’s decision to include languages like Swahili and Yoruba, spoken by millions but often neglected in AI research, signals a shift toward more inclusive AI technology. This move is especially important for enterprises looking to deploy AI solutions in emerging markets, where language barriers have traditionally posed significant challenges.Human translation raises the bar for multilingual AI accuracyOpenAI used professionalhuman translatorsto create the MMMLU dataset, ensuring higher accuracy than comparable datasets that rely on machine translation. Automated translation tools often introduce subtle errors, particularly in languages with fewer resources to train on. By relying on human expertise, OpenAI ensures that the dataset provides a more reliable foundation for evaluating AI models in multiple languages.This decision is crucial for industries where precision is non-negotiable. In sectors like healthcare, law, and finance, even minor translation errors can have serious implications. OpenAI’s focus on translation quality positions the MMMLU dataset as a critical tool for enterprises that require AI systems to perform reliably across linguistic and cultural boundaries.Hugging Face partnership boosts open access to multilingual AI dataBy releasing the MMMLU dataset on Hugging Face, a popular platform for sharing machine learning models and datasets, OpenAI is engaging the broader AI research community. Hugging Face has become a go-to destination for open-source AI tools, and the addition of the MMMLU dataset signals OpenAI’s commitment to advancing open access in AI research.However, this release comes at a time when OpenAI has faced growing scrutiny over its approach to openness.Criticism has mountedin recent months, especially fromco-founder Elon Musk, who has accused the company of straying from its original mission of being an open-source, nonprofit entity.Musk’s lawsuit, filed earlier this year, claims that OpenAI’s shift toward for-profit activities—particularly its partnership with Microsoft—contradicts the company’s founding principles.Despite this, OpenAI has defended its current strategy, arguing that it prioritizes “open access” rather than open source. In this framework, OpenAI aims to provide broad access to its technologies without necessarily sharing the inner workings of its most advanced models. The release of the MMMLU dataset fits within this philosophy, offering the research community a powerful tool while maintaining control over its proprietary models.OpenAI Academy: Expanding access to AI in emerging marketsIn addition to the MMMLU dataset release, OpenAI is furthering its commitment to global AI accessibility through the launch of theOpenAI Academy. Announced on the same day as the MMMLU dataset, the Academy is designed to invest in developers and mission-driven organizations that are leveraging AI to tackle critical problems in their communities, particularly in low- and middle-income countries.The Academy will provide training, technical guidance, and$1 million in API creditsto ensure that local AI talent can access cutting-edge resources. By supporting developers who understand the unique social and economic challenges of their regions, OpenAI hopes to empower communities to build AI applications tailored to local needs.This initiative complements the MMMLU dataset by emphasizing OpenAI’s goal of making advanced AI tools and education available to diverse, global communities. Both the MMMLU dataset and the Academy reflect OpenAI’s long-term strategy of ensuring that AI development benefits all of humanity, especially communities that have traditionally been underserved by the latest AI advancements.Multilingual AI gives businesses a competitive edgeFor enterprises, the MMMLU dataset presents an opportunity to benchmark their own AI systems in aglobal context. As companies expand into international markets, the ability to deploy AI solutions that understand multiple languages becomes critical. Whether it’s customer service, content moderation, or data analysis, AI systems that perform well across languages can offer a competitive advantage by reducing friction in communication and improving user experience.The dataset’s focus on professional and academic subjects adds another layer of value for businesses. Companies in law, education, and research can use the MMMLU dataset to test how well their AI models perform in specialized domains, ensuring that their systems meet the high standards required for these sectors. As AI continues to evolve, the ability to handle complex, domain-specific tasks in multiple languages will become a key differentiator for businesses competing on a global stage.A multilingual future: What the MMMLU dataset means for AIThe release of the MMMLU dataset is likely to have lasting implications for the AI industry. As more companies and researchers begin to test their models against this multilingual benchmark, the demand for AI systems that can operate seamlessly across languages will only grow. This could lead to new innovations in language processing, as well as greater adoption of AI solutions in parts of the world that have traditionally been underserved by technology.For OpenAI, the MMMLU dataset represents both a challenge and an opportunity. On one hand, the company is positioning itself as a leader in multilingual AI, offering tools that address a critical gap in the current AI landscape. On the other hand, OpenAI’s evolving stance on openness will continue to be scrutinized as it navigates the tensions between public good and private interest.As AI becomes increasingly integrated into the global economy, companies and governments alike will need to grapple with the ethical and practical implications of these technologies. OpenAI’s release of the MMMLU dataset is a step in the right direction, but it also raises important questions about how much of the AI revolution will be open to all.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
Programming & Development,OpenAI Academy launches with $1M in developer credits for devs in low- and middle-income countries,https://venturebeat.com/ai/openai-academy-launches-with-1m-in-developer-credits-for-devs-in-low-and-middle-income-countries/,"September 23, 2024 6:52 AM","Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.Learn MoreIt’s not school, but the academy is designed to boost the skills and careers of local developers.I’m talking, of course, about the OpenAI Academy, a new effort announced today from the AI unicorn that will begin by awarding some unspecified number of developers in low- and middle-income countries $1 million in API credits.The goal? To catalyze economic growth and innovation in sectors such as healthcare, agriculture, education, and finance, as well as “ensure that the transformative potential of artificial intelligence is accessible and beneficial to diverse communities worldwide.”“Many countries have fast-growing technology sectors filled with talented developers and innovative organizations, yet access to advanced training and technical resources is still a significant barrier,” the announcement states. “Investing in the development of local AI talent can have a transformative impact across a range of industries.”Cynics and skeptics will undoubtedly say this is some form of neo technological colonialism — with the U.S.-based OpenAI seeking to spread its influence and increase dependencies on its technology around the globe.Yet for devs who receive the credits, it’s hard to imagine them not celebrating being selected and being excited to use OpenAI models to build their own apps that could become thriving new businesses.Of course, OpenAI will stand to benefit from entrenching itself more among the up-and-coming developers building new startups, but also, there’s nothing that I see in the announcement that says these devs can’t use other AI models simultaneously, nor that they couldn’t one day switch out the underlying API pipelines to other rival providers. It seems like a win-win for devs and OpenAI, to me.Which countries will be eligible to participate?OpenAI’s announcement doesn’t clearly state which countries are included in its list of “low and middle-income countries,” but those are categories that the World Bank uses as well, based on gross national income per capita.In fact, theWorld Bank divides economiesinto four income groups—low, lower-middle, upper-middle, and high income.The U.S. and many European nations are high-income, while many countries in Sub-Saharan Africa and South Asia fall into the low and lower-middle-income categories, where GNI per capita remains a barrier to accessing cutting-edge technologies like AI.Countries such as Afghanistan, Bangladesh, and Angola fall into the low and lower-middle-income categories.In fact, 63% of all countries are considered lower or middle-income (LMIC), some137 different nations.That’s a long list and it’s not likely that OpenAI will be targeting them all at once, so it will be interesting to see where it focuses first and why.Supporting local talent to drive a global impactNaturally, it’s not just API credits OpenAI is dangling as incentive to apply to the program. Indeed, the company pledges to host incubators and contests as well as “experts for developers and mission-driven organizations leveraging AI.”The Academy’s focus on building a global network of developers will help foster collaboration and knowledge sharing across diverse regions.By connecting participants, OpenAI aims to create a robust community that can collectively drive technological advancements and tackle community-specific challenges.The initiative also plans to host contests and incubators in partnership with philanthropists to provide targeted investment in organizations working on the front lines of their communities.For example, KOBI, a recent recipient of the OpenAI prize at The Tools Competition, uses AI to assist students with dyslexia in learning to read. Another beneficiary, I-Stem, employs AI to improve access to content for blind and low-vision communities in India, helping them find meaningful employment.Expanding access to AI resourcesIn addition to direct support for developers, OpenAI has funded the translation of the Massive Multitask Language Understanding (MMLU) benchmark into 14 languages, including Arabic, Bengali, and Swahili.This initiative aims to make AI education more accessible and relevant to non-English speaking communities, facilitating the development of AI solutions that are culturally and linguistically tailored to local needs.The OpenAI Academy represents a significant expansion of OpenAI’s ongoing efforts to empower developers and organizations worldwide.OpenAI also pledges further details on how to access the Academy’s resources.VB DailyStay in the know! Get the latest news in your inbox dailySubscribeBy subscribing, you agree to VentureBeat'sTerms of Service.Thanks for subscribing. Check out moreVB newsletters here.An error occured."
